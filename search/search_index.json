{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Cloud Custodian Documentation Cloud Custodian is a tool that unifies the dozens of tools and scripts most organizations use for managing their public cloud accounts into one open source tool. It uses a stateless rules engine for policy definition and enforcement, with metrics, structured outputs and detailed reporting for clouds infrastructure. It integrates tightly with serverless runtimes to provide real time remediation/response with low operational overhead. Organizations can use Custodian to manage their cloud environments by ensuring compliance to security policies, tag policies, garbage collection of unused resources, and cost management from a single tool. Cloud Custodian can be bound to serverless event streams across multiple cloud providers that maps to security, operations, and governance use cases. Custodian adheres to a compliance as code principle, so you can validate, dry-run, and review changes to your policies. Cloud Custodian policies are expressed in YAML and include the following: The type of resource to run the policy against Filters to narrow down the set of resources Actions to take on the filtered set of resources Navigate below to your cloud provider and get started with Cloud Custodian! ::: {.toctree maxdepth=\"2\" caption=\"Introduction\"} quickstart/index filters actions quickstart/advanced quickstart/policyStructure deployment ::: ::: {.toctree maxdepth=\"1\" caption=\"AWS\"} aws/gettingstarted aws/examples/index aws/usage aws/lambda aws/topics/index aws/contribute aws/resources/index ::: ::: {.toctree maxdepth=\"2\" caption=\"Azure\"} azure/gettingstarted azure/configuration/index azure/examples/index azure/advanced/index azure/resources/index ::: ::: {.toctree maxdepth=\"1\" caption=\"GCP\"} gcp/gettingstarted gcp/examples/index gcp/policy/index gcp/contribute gcp/resources/index ::: ::: {.toctree maxdepth=\"2\" caption=\"Tools\"} tools/c7n-org tools/cask tools/c7n-mailer tools/c7n-logexporter tools/c7n-trailcreator tools/c7n-policystream tools/omnissm tools/c7n-guardian tools/c7n-salactus ::: ::: {.toctree maxdepth=\"2\" caption=\"Contributing\"} contribute developer/index.rst developer/installing.rst developer/tests.rst developer/documentation.rst developer/packaging.rst :::","title":"Home"},{"location":"#cloud-custodian-documentation","text":"Cloud Custodian is a tool that unifies the dozens of tools and scripts most organizations use for managing their public cloud accounts into one open source tool. It uses a stateless rules engine for policy definition and enforcement, with metrics, structured outputs and detailed reporting for clouds infrastructure. It integrates tightly with serverless runtimes to provide real time remediation/response with low operational overhead. Organizations can use Custodian to manage their cloud environments by ensuring compliance to security policies, tag policies, garbage collection of unused resources, and cost management from a single tool. Cloud Custodian can be bound to serverless event streams across multiple cloud providers that maps to security, operations, and governance use cases. Custodian adheres to a compliance as code principle, so you can validate, dry-run, and review changes to your policies. Cloud Custodian policies are expressed in YAML and include the following: The type of resource to run the policy against Filters to narrow down the set of resources Actions to take on the filtered set of resources Navigate below to your cloud provider and get started with Cloud Custodian! ::: {.toctree maxdepth=\"2\" caption=\"Introduction\"} quickstart/index filters actions quickstart/advanced quickstart/policyStructure deployment ::: ::: {.toctree maxdepth=\"1\" caption=\"AWS\"} aws/gettingstarted aws/examples/index aws/usage aws/lambda aws/topics/index aws/contribute aws/resources/index ::: ::: {.toctree maxdepth=\"2\" caption=\"Azure\"} azure/gettingstarted azure/configuration/index azure/examples/index azure/advanced/index azure/resources/index ::: ::: {.toctree maxdepth=\"1\" caption=\"GCP\"} gcp/gettingstarted gcp/examples/index gcp/policy/index gcp/contribute gcp/resources/index ::: ::: {.toctree maxdepth=\"2\" caption=\"Tools\"} tools/c7n-org tools/cask tools/c7n-mailer tools/c7n-logexporter tools/c7n-trailcreator tools/c7n-policystream tools/omnissm tools/c7n-guardian tools/c7n-salactus ::: ::: {.toctree maxdepth=\"2\" caption=\"Contributing\"} contribute developer/index.rst developer/installing.rst developer/tests.rst developer/documentation.rst developer/packaging.rst :::","title":"Cloud Custodian Documentation"},{"location":"actions/","text":"Generic Actions {#actions} The following actions can be applied to all policies for all resources. See the provider specific resource references. Webhook Action The webhook action allows invoking a webhook with information about your resources. You may initiate a call per resource, or a call referencing a batch of resources. Additionally you may define the body and query string using JMESPath references to the resource or resource array. JMESPath queries for query-params, headers and body will have access to the following data: { 'account_id', 'region', 'execution_id', 'execution_start', 'policy', 'resource', \u2500\u25b6 if Batch == false 'resources', \u2500\u25b6 if Batch == true } Examples: actions: - type: webhook url: http://foo.com?hook-id=123 \u2500\u25b6 Call will default to POST query-params: \u2500\u25b6 Additional query string query-params resource_name: resource.name \u2500\u25b6 Value is a JMESPath query into resource dictionary policy_name: policy.name actions: - type: webhook url: http://foo.com batch: true \u2500\u25b6 Single call for full resource array body: 'resources[].name' \u2500\u25b6 JMESPath will reference array of resources query-params: count: 'resources[] | length(@)' \u2500\u25b6 Include resource count in query string static-value: '`foo`' \u2500\u25b6 JMESPath string literal in ticks actions: - type: webhook url: http://foo.com batch: true batch-size: 10 method: POST headers: static-value: '`foo`' \u2500\u25b6 JMESPath string literal in ticks query-params: count: 'resources[] | length(@)'","title":"Actions"},{"location":"actions/#generic-actions-actions","text":"The following actions can be applied to all policies for all resources. See the provider specific resource references.","title":"Generic Actions {#actions}"},{"location":"actions/#webhook-action","text":"The webhook action allows invoking a webhook with information about your resources. You may initiate a call per resource, or a call referencing a batch of resources. Additionally you may define the body and query string using JMESPath references to the resource or resource array. JMESPath queries for query-params, headers and body will have access to the following data: { 'account_id', 'region', 'execution_id', 'execution_start', 'policy', 'resource', \u2500\u25b6 if Batch == false 'resources', \u2500\u25b6 if Batch == true } Examples: actions: - type: webhook url: http://foo.com?hook-id=123 \u2500\u25b6 Call will default to POST query-params: \u2500\u25b6 Additional query string query-params resource_name: resource.name \u2500\u25b6 Value is a JMESPath query into resource dictionary policy_name: policy.name actions: - type: webhook url: http://foo.com batch: true \u2500\u25b6 Single call for full resource array body: 'resources[].name' \u2500\u25b6 JMESPath will reference array of resources query-params: count: 'resources[] | length(@)' \u2500\u25b6 Include resource count in query string static-value: '`foo`' \u2500\u25b6 JMESPath string literal in ticks actions: - type: webhook url: http://foo.com batch: true batch-size: 10 method: POST headers: static-value: '`foo`' \u2500\u25b6 JMESPath string literal in ticks query-params: count: 'resources[] | length(@)'","title":"Webhook Action"},{"location":"contribute/","text":"Contributing to Cloud Custodian {#contribute} If you\\'re interested in contributing to Cloud Custodian development, welcome! Please take a few minutes to familiarize yourself with the project and our expectations. Developer install You\\'ll need to perform a developer install <developer> {.interpreted-text role=\"ref\"} and familiarize yourself with the tests. Issues We use GitHub Issues to report and track work on the project. Discussion is also available on Gitter and a Google Group . Code of Conduct This project adheres to the [CNCF Code of Conduct]( https://github.com/cncf/foundation/blob/master/code-of-conduct.md ) By participating, you are expected to honor this code. Contributor agreement As a CNCF Project, we\\'ve chosen to use the foundation\\'s Contributor License Agreements https://github.com/cncf/cla to ensure that contributions are authorized and in good faith. We do this in preference to the Developer Certificate of Origin (DCO) as a large portion of our contributor community don\\'t have familiarity with it. The contributor agreement signing process is managed through the use of the Linux Foundation\\'s EasyCLA opensource project and is automatically verified on any pull requests.","title":"Contribute"},{"location":"contribute/#contributing-to-cloud-custodian-contribute","text":"If you\\'re interested in contributing to Cloud Custodian development, welcome! Please take a few minutes to familiarize yourself with the project and our expectations.","title":"Contributing to Cloud Custodian {#contribute}"},{"location":"contribute/#developer-install","text":"You\\'ll need to perform a developer install <developer> {.interpreted-text role=\"ref\"} and familiarize yourself with the tests.","title":"Developer install"},{"location":"contribute/#issues","text":"We use GitHub Issues to report and track work on the project. Discussion is also available on Gitter and a Google Group .","title":"Issues"},{"location":"contribute/#code-of-conduct","text":"This project adheres to the [CNCF Code of Conduct]( https://github.com/cncf/foundation/blob/master/code-of-conduct.md ) By participating, you are expected to honor this code.","title":"Code of Conduct"},{"location":"contribute/#contributor-agreement","text":"As a CNCF Project, we\\'ve chosen to use the foundation\\'s Contributor License Agreements https://github.com/cncf/cla to ensure that contributions are authorized and in good faith. We do this in preference to the Developer Certificate of Origin (DCO) as a large portion of our contributor community don\\'t have familiarity with it. The contributor agreement signing process is managed through the use of the Linux Foundation\\'s EasyCLA opensource project and is automatically verified on any pull requests.","title":"Contributor agreement"},{"location":"deployment/","text":"Deployment In this section we will cover a few different deployment options for Cloud Custodian. Compliance as Code {#compliance_as_code} When operating Cloud Custodian, it is highly recommended to treat the policy files as code, similar to that of Terraform or CloudFormation files. Cloud Custodian has a built-in dryrun mode and policy syntax validation which when paired with an automated CI system, can help you release policies with confidence. This tutorial assumes that you have working knowledge of Github, Git, Docker, and a continuous integration tool (Jenkins, Drone, Travis, etc.). To begin, start by checking your policy files into a source control management tool like Github. This allows us to version and enable collaboration through git pull requests and issues. In this example, we will be setting up a new repo in Github. First, set up a new repo in Github and grab the repository url. You don\\'t need to add a README or any other files to it first. mkdir my-policies cd my-policies git init git remote add origin <github repo url> touch policy.yml Next, we\\'ll add a policy to our new policy.yml file. policies: - name: aws-vpcs resource: aws.vpc Once you\\'ve added the policy to your policy file we can stage our changes from our working directory and push it up to our remote: # this should show your policy.yml as an untracked file git status git add policy.yml git commit -m 'init my first policy' git push -u origin master Once you\\'ve pushed your changes you should be able to see your new changes inside of Github. Congratulations, you\\'re now ready to start automatically validating and testing your policies! Continuous Integration of Policies {#continuous_integration_of_policies} Next, enable a CI webhook back to your CI system of choice when pull requests targeting your master branch are opened or updated. This allows us to continuously test and validate the policies that are being modified. In this example, we will be using Microsoft Azure Devops Pipelines. First, navigate to https://azure.microsoft.com/en-us/services/devops/pipelines/ and click the \\\"Start pipelines free with Github\\\" button and follow the flow to connect your Github account with Devops Pipelines. Next click on the Pipelines section in the left hand side of the sidebar and connect with Github. Once the pipeline is setup, we can add the following azure devops configuration to our repo: trigger: - master jobs: - job: 'Validate' pool: vmImage: 'Ubuntu-16.04' steps: - checkout: self - task: UsePythonVersion@0 displayName: \"Set Python Version\" inputs: versionSpec: '3.7' architecture: 'x64' - script: pip install --upgrade pip displayName: Upgrade pip - script: pip install c7n c7n_azure c7n_gcp displayName: Install custodian - script: custodian validate policy.yml displayName: Validate policy file This configuration will install Cloud Custodian and validate the policy.yml file that we created in the previous step. Finally, we can run the new policies against your cloud environment in dryrun mode. This mode will only query the resources and apply the filters on the resources. Doing this allows you to assess the potential blast radius of a given policy change. Setting up the automated dryrun of policies is left as an exercise to the user-- this requires hosting your cloud authentication tokens inside of a CI system or hosting your own CI system and using Managed Service Identities (Azure) or Instance Profiles (AWS). It\\'s important to verify that the results of the dryrun match your expectations. Custodian is a very powerful tool that will do exactly what you tell it to do! In this case, you should always \\\"measure twice, cut once\\\". IAM Setup {#iam_setup} To run Cloud Custodian against your account, you will need an IAM role with appropriate permissions. Depending on the scope of the policy, these permissions may differ from policy to policy. For a baseline, the managed read only policies in each of the respective cloud providers will be enough to dryrun your policies. Actions will require additional IAM permissions which should be added at your discretion. For serverless policies, Custodian will need the corresponding permissions to provision serverless functions. In AWS, you will need ReadOnly access as well as the following permissions: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"CustodianLambdaPermissions\", \"Effect\": \"Allow\", \"Action\": [ \"cloudwatch:PutMetricData\", \"ec2:DescribeNetworkInterfaces\", \"ec2:DeleteNetworkInterface\", \"ec2:CreateNetworkInterface\", \"events:PutRule\", \"events:PutTargets\", \"iam:PassRole\", \"lambda:CreateFunction\", \"lambda:TagResource\", \"lambda:CreateEventSourceMapping\", \"lambda:UntagResource\", \"lambda:PutFunctionConcurrency\", \"lambda:DeleteFunction\", \"lambda:UpdateEventSourceMapping\", \"lambda:InvokeFunction\", \"lambda:UpdateFunctionConfiguration\", \"lambda:UpdateAlias\", \"lambda:UpdateFunctionCode\", \"lambda:AddPermission\", \"lambda:DeleteAlias\", \"lambda:DeleteFunctionConcurrency\", \"lambda:DeleteEventSourceMapping\", \"lambda:RemovePermission\", \"lambda:CreateAlias\", \"logs:CreateLogStream\", \"logs:PutLogEvents\", \"logs:CreateLogGroup\" ], \"Resource\": \"*\" } ] } Note: These are just the permissions to deploy Custodian Lambda functions, these are not the permissions that are required to run Custodian _ in the Lambda function. Those roles are defined in the role attribute in the policy or with the assume role used in the cli. Single Node Deployment {#single_node_usage} Now that your policies are stored and available in source control, you can now fill in the next pieces of the puzzle to deploy. The simplest way to operate Cloud Custodian is to start with running Cloud Custodian against a single account (or subscription or project) on a virtual machine. To start, create a virtual machine on your cloud provider of choice. It\\'s recommended to execute Cloud Custodian in the same cloud provider that you are operating against to prevent a hard dependency on one cloud to another as well being able to utilize your cloud\\'s best practices for credentials (instance profile, service account, etc). Then, log into the instance and set up Custodian, following the instructions in the install-cc {.interpreted-text role=\"ref\"} guide. Once you have Cloud Custodian installed, download your policies that you created in the compliance_as_code {.interpreted-text role=\"ref\"} section. If using git, just simply do a git clone : git clone <repository-url> You now have your policies and custodian available on the instance. Typically, policies that query the extant resources in the account/project/subscription should be run on a regular basis to ensure that resources are constantly compliant. To do this you can simply set up a cron job to run custodian on a set cadence. Monitoring Cloud Custodian {#monitoring_custodian} Cloud Custodian ships with the ability to emit metrics on policy execution and transport logs to cloud provider native logging solutions. When executing Custodian, you can enable metrics simply by adding the -m flag and the cloud provider: # AWS custodian run -s output -m aws policy.yml # Azure custodian run -s output -m azure policy.yml # GCP custodian run -s output -m gcp policy.yml When you enable metrics, a new namespace will be created and the following metrics will be recorded there: ResourceCount ResourceTime ActionTime To enable logging to CloudWatch logs, Stackdriver, or Azure AppInsights, use the -l flag: # AWS CloudWatch Logs custodian run -s output -l /cloud-custodian/policies policy.yml # Azure App Insights Logs custodian run -s output -l azure://cloud-custodian/policies policy.yml # Stackdriver Logs custodian run -s output -l gcp://cloud-custodian/policies policy.yml You can also store the output of your Custodian logs in a cloud provider\\'s blob storage like S3 or Azure Storage accounts: # AWS S3 custodian run -s s3://my-custodian-bucket policy.yml # Azure Storage Accounts custodian run -s azure://my-custodian-storage-account policy.yml Mailer and Notifications Deployment {#mailer_and_notifications_deployment} For instructions on how to deploy the mailer for notifications, see /tools/c7n-mailer {.interpreted-text role=\"doc\"} Multi Account Execution {#multi_account_execution} For more advanced setups, such as executing Custodian against multiple accounts, we distribute the tool c7n-org. c7n-org utilizes a accounts configuration file and assume roles to operate against multiple accounts, projects, or subscriptions in parallel. More information can be found in /tools/c7n-org {.interpreted-text role=\"doc\"}. Advanced Continuous Integration Tips {#advanced_continuous_integration_tips} When policy files reach a sufficiently large size it can cause dryruns to execute for a significantly long period of time. In most cases, the only thing that actually needs to be tested would be the policies that were changed. The following example will download the cloudcustodian/policystream image and generate a policy file containing only the policies that changed between the most recent commit and master. # in your git directory for policies docker pull cloudcustodian/policystream docker run -v $(pwd):/home/custodian/policies cloudcustodian > policystream-diff.yml custodian run -s output -v --dryrun policystream-diff.yml After running your new policy file (policystream-diff.yml), the outputs will be stored in the output directory. Additional Resources {#additional_resources} manheim-c7n-tools - Manheim\\'s Cloud Custodian (c7n) wrapper package, policy generator/interpolator, runner, error scanner, and supporting tools.","title":"Deployment"},{"location":"deployment/#deployment","text":"In this section we will cover a few different deployment options for Cloud Custodian.","title":"Deployment"},{"location":"deployment/#compliance-as-code-compliance_as_code","text":"When operating Cloud Custodian, it is highly recommended to treat the policy files as code, similar to that of Terraform or CloudFormation files. Cloud Custodian has a built-in dryrun mode and policy syntax validation which when paired with an automated CI system, can help you release policies with confidence. This tutorial assumes that you have working knowledge of Github, Git, Docker, and a continuous integration tool (Jenkins, Drone, Travis, etc.). To begin, start by checking your policy files into a source control management tool like Github. This allows us to version and enable collaboration through git pull requests and issues. In this example, we will be setting up a new repo in Github. First, set up a new repo in Github and grab the repository url. You don\\'t need to add a README or any other files to it first. mkdir my-policies cd my-policies git init git remote add origin <github repo url> touch policy.yml Next, we\\'ll add a policy to our new policy.yml file. policies: - name: aws-vpcs resource: aws.vpc Once you\\'ve added the policy to your policy file we can stage our changes from our working directory and push it up to our remote: # this should show your policy.yml as an untracked file git status git add policy.yml git commit -m 'init my first policy' git push -u origin master Once you\\'ve pushed your changes you should be able to see your new changes inside of Github. Congratulations, you\\'re now ready to start automatically validating and testing your policies!","title":"Compliance as Code {#compliance_as_code}"},{"location":"deployment/#continuous-integration-of-policies-continuous_integration_of_policies","text":"Next, enable a CI webhook back to your CI system of choice when pull requests targeting your master branch are opened or updated. This allows us to continuously test and validate the policies that are being modified. In this example, we will be using Microsoft Azure Devops Pipelines. First, navigate to https://azure.microsoft.com/en-us/services/devops/pipelines/ and click the \\\"Start pipelines free with Github\\\" button and follow the flow to connect your Github account with Devops Pipelines. Next click on the Pipelines section in the left hand side of the sidebar and connect with Github. Once the pipeline is setup, we can add the following azure devops configuration to our repo: trigger: - master jobs: - job: 'Validate' pool: vmImage: 'Ubuntu-16.04' steps: - checkout: self - task: UsePythonVersion@0 displayName: \"Set Python Version\" inputs: versionSpec: '3.7' architecture: 'x64' - script: pip install --upgrade pip displayName: Upgrade pip - script: pip install c7n c7n_azure c7n_gcp displayName: Install custodian - script: custodian validate policy.yml displayName: Validate policy file This configuration will install Cloud Custodian and validate the policy.yml file that we created in the previous step. Finally, we can run the new policies against your cloud environment in dryrun mode. This mode will only query the resources and apply the filters on the resources. Doing this allows you to assess the potential blast radius of a given policy change. Setting up the automated dryrun of policies is left as an exercise to the user-- this requires hosting your cloud authentication tokens inside of a CI system or hosting your own CI system and using Managed Service Identities (Azure) or Instance Profiles (AWS). It\\'s important to verify that the results of the dryrun match your expectations. Custodian is a very powerful tool that will do exactly what you tell it to do! In this case, you should always \\\"measure twice, cut once\\\".","title":"Continuous Integration of Policies {#continuous_integration_of_policies}"},{"location":"deployment/#iam-setup-iam_setup","text":"To run Cloud Custodian against your account, you will need an IAM role with appropriate permissions. Depending on the scope of the policy, these permissions may differ from policy to policy. For a baseline, the managed read only policies in each of the respective cloud providers will be enough to dryrun your policies. Actions will require additional IAM permissions which should be added at your discretion. For serverless policies, Custodian will need the corresponding permissions to provision serverless functions. In AWS, you will need ReadOnly access as well as the following permissions: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"CustodianLambdaPermissions\", \"Effect\": \"Allow\", \"Action\": [ \"cloudwatch:PutMetricData\", \"ec2:DescribeNetworkInterfaces\", \"ec2:DeleteNetworkInterface\", \"ec2:CreateNetworkInterface\", \"events:PutRule\", \"events:PutTargets\", \"iam:PassRole\", \"lambda:CreateFunction\", \"lambda:TagResource\", \"lambda:CreateEventSourceMapping\", \"lambda:UntagResource\", \"lambda:PutFunctionConcurrency\", \"lambda:DeleteFunction\", \"lambda:UpdateEventSourceMapping\", \"lambda:InvokeFunction\", \"lambda:UpdateFunctionConfiguration\", \"lambda:UpdateAlias\", \"lambda:UpdateFunctionCode\", \"lambda:AddPermission\", \"lambda:DeleteAlias\", \"lambda:DeleteFunctionConcurrency\", \"lambda:DeleteEventSourceMapping\", \"lambda:RemovePermission\", \"lambda:CreateAlias\", \"logs:CreateLogStream\", \"logs:PutLogEvents\", \"logs:CreateLogGroup\" ], \"Resource\": \"*\" } ] } Note: These are just the permissions to deploy Custodian Lambda functions, these are not the permissions that are required to run Custodian _ in the Lambda function. Those roles are defined in the role attribute in the policy or with the assume role used in the cli.","title":"IAM Setup {#iam_setup}"},{"location":"deployment/#single-node-deployment-single_node_usage","text":"Now that your policies are stored and available in source control, you can now fill in the next pieces of the puzzle to deploy. The simplest way to operate Cloud Custodian is to start with running Cloud Custodian against a single account (or subscription or project) on a virtual machine. To start, create a virtual machine on your cloud provider of choice. It\\'s recommended to execute Cloud Custodian in the same cloud provider that you are operating against to prevent a hard dependency on one cloud to another as well being able to utilize your cloud\\'s best practices for credentials (instance profile, service account, etc). Then, log into the instance and set up Custodian, following the instructions in the install-cc {.interpreted-text role=\"ref\"} guide. Once you have Cloud Custodian installed, download your policies that you created in the compliance_as_code {.interpreted-text role=\"ref\"} section. If using git, just simply do a git clone : git clone <repository-url> You now have your policies and custodian available on the instance. Typically, policies that query the extant resources in the account/project/subscription should be run on a regular basis to ensure that resources are constantly compliant. To do this you can simply set up a cron job to run custodian on a set cadence.","title":"Single Node Deployment {#single_node_usage}"},{"location":"deployment/#monitoring-cloud-custodian-monitoring_custodian","text":"Cloud Custodian ships with the ability to emit metrics on policy execution and transport logs to cloud provider native logging solutions. When executing Custodian, you can enable metrics simply by adding the -m flag and the cloud provider: # AWS custodian run -s output -m aws policy.yml # Azure custodian run -s output -m azure policy.yml # GCP custodian run -s output -m gcp policy.yml When you enable metrics, a new namespace will be created and the following metrics will be recorded there: ResourceCount ResourceTime ActionTime To enable logging to CloudWatch logs, Stackdriver, or Azure AppInsights, use the -l flag: # AWS CloudWatch Logs custodian run -s output -l /cloud-custodian/policies policy.yml # Azure App Insights Logs custodian run -s output -l azure://cloud-custodian/policies policy.yml # Stackdriver Logs custodian run -s output -l gcp://cloud-custodian/policies policy.yml You can also store the output of your Custodian logs in a cloud provider\\'s blob storage like S3 or Azure Storage accounts: # AWS S3 custodian run -s s3://my-custodian-bucket policy.yml # Azure Storage Accounts custodian run -s azure://my-custodian-storage-account policy.yml","title":"Monitoring Cloud Custodian {#monitoring_custodian}"},{"location":"deployment/#mailer-and-notifications-deployment-mailer_and_notifications_deployment","text":"For instructions on how to deploy the mailer for notifications, see /tools/c7n-mailer {.interpreted-text role=\"doc\"}","title":"Mailer and Notifications Deployment {#mailer_and_notifications_deployment}"},{"location":"deployment/#multi-account-execution-multi_account_execution","text":"For more advanced setups, such as executing Custodian against multiple accounts, we distribute the tool c7n-org. c7n-org utilizes a accounts configuration file and assume roles to operate against multiple accounts, projects, or subscriptions in parallel. More information can be found in /tools/c7n-org {.interpreted-text role=\"doc\"}.","title":"Multi Account Execution {#multi_account_execution}"},{"location":"deployment/#advanced-continuous-integration-tips-advanced_continuous_integration_tips","text":"When policy files reach a sufficiently large size it can cause dryruns to execute for a significantly long period of time. In most cases, the only thing that actually needs to be tested would be the policies that were changed. The following example will download the cloudcustodian/policystream image and generate a policy file containing only the policies that changed between the most recent commit and master. # in your git directory for policies docker pull cloudcustodian/policystream docker run -v $(pwd):/home/custodian/policies cloudcustodian > policystream-diff.yml custodian run -s output -v --dryrun policystream-diff.yml After running your new policy file (policystream-diff.yml), the outputs will be stored in the output directory.","title":"Advanced Continuous Integration Tips {#advanced_continuous_integration_tips}"},{"location":"deployment/#additional-resources-additional_resources","text":"manheim-c7n-tools - Manheim\\'s Cloud Custodian (c7n) wrapper package, policy generator/interpolator, runner, error scanner, and supporting tools.","title":"Additional Resources {#additional_resources}"},{"location":"filters/","text":"Generic Filters {#filters} The following filters can be applied to all policies for all resources. See the provider specific resource reference for additional information. Value Filter Cloud Custodian provides for a flexible query language on any resource by allowing for rich queries on JSON objects via JMESPath, and allows for mixing and combining those with boolean conditional operators that are nest-able. (Tutorial here on JMESPath syntax) The base value filter enables the use of jmespath with data returned from a describe call. filters: - type: value key: \"State[0]\" \u2500\u25b6 The value from the describe call value: \"running\" \u2500\u25b6 Value that is being filtered against There are several ways to get a list of possible keys for each resource. Via Custodian CLI Create a new custodian yaml file with just the name and resource fields. Then run custodian run -s OUTPUT_DIR . The valid key fields can be found in the output directory in resources.json {.yaml} policies: - name: my-first-policy resource: aws.ec2 Via Cloud Providers CLI Use the relevant cloud provider cli to run the describe call to view all available keys. For example using aws cli run aws ec2 describe-instances or with azure az vm list . Note: You do not need to include the outermost json field in most cases since custodian removes this field from the results. Via Cloud Provider Documentation Go to the relevant cloud provider sdk documentation and search for the describe api call for the resource you\\'re interested in. The available fields will be listed under the results of that api call. Comparison operators: : The generic value filter allows for comparison operators to be used - `equal` or `eq` - `not-equal` or `ne` - `greater-than` or `gt` - `gte` or `ge` - `less-than` or `lt` - `lte` or `le` {.yaml} filters: - type: value key: CpuOptions.CoreCount \u2500\u25b6 The value from the describe call value: 36 \u2500\u25b6 Value that is being compared op: greater-than \u2500\u25b6 Comparison Operator Other operators: : - absent - present - not-null - empty - contains {.yaml} filters: - type: value key: CpuOptions.CoreCount \u2500\u25b6 The value from the describe call value: present \u2500\u25b6 Checks if key is present Logical Operators: : - or or Or - and or And - not {.yaml} filters: - or: \u2500\u25b6 Logical Operator - type: value key: CpuOptions.CoreCount \u2500\u25b6 The value from the describe call value: 36 \u2500\u25b6 Value that is being compared - type: value key: CpuOptions.CoreCount \u2500\u25b6 The value from the describe call value: 42 \u2500\u25b6 Value that is being compared List Operators: : There is a collection of operators that can be used with user supplied lists. The operators are evaluated as value from key in (the operator) given value . If you would like it evaluated in the opposite way given value in (the operator) value from key then you can include the swap transformation or use the contains operator. - `in` - `not-in` or `ni` - `intersect` - Provides comparison between 2 lists {.yaml} filters: - type: value key: ImageId \u2500\u25b6 The value from the describe call op: in \u2500\u25b6 List operator value: [ID-123, ID-321] \u2500\u25b6 List of Values to be compared against {.yaml} filters: - type: value key: ImageId.List \u2500\u25b6 The value from the describe call op: in \u2500\u25b6 List operator value: ID-321 \u2500\u25b6 Values to be compared against value_type: swap \u2500\u25b6 Switches list comparison order Special operators: : - glob - Provides Glob matching support - regex - Provides Regex matching support but ignores case (1) - regex-case - Provides case sensitive Regex matching support (1) ``` {.yaml} filters: - type: value key: FunctionName \u2500\u25b6 The value from the describe call, or resources.json op: regex \u2500\u25b6 Special operator value: '(custodian|c7n) \\w+' \u2500\u25b6 Regex string: match all values beginning with custodian or c7n_ type: value key: name \u2500\u25b6 The value from the describe call, or resources.json op: regex \u2500\u25b6 Special operator value: '^. c7n. $' \u2500\u25b6 Regex string: match all values containing c7n type: value key: name \u2500\u25b6 The value from the describe call, or resources.json op: regex \u2500\u25b6 Special operator value: '^((?!c7n).)*$' \u2500\u25b6 Regex string: match all values not containing c7n ``` These operators are implemented using re.match . If a filter isn\\'t working as expected take a look at the [re]{.title-ref}__ documentation. __ https://docs.python.org/3/library/re.html#search-vs-match Transformations: Transformations on the value can be done using the value_type keyword. The following value types are supported: age - convert to a datetime (for past date comparisons) cidr - parse an ipaddress cidr_size - the length of the network prefix expiration - convert to a datetime (for future date comparisons) integer - convert the value to an integer normalize - convert the value to lowercase resource_count - compare against the number of matched resources size - the length of an element swap - swap the value and the evaluated key date - parse the filter\\'s value as a date. Note that the [age]{.title-ref} and [expiration]{.title-ref} transformations expect a value given as a number of days. Use a floating point value to match time periods shorter than a day. Examples: ``` {.yaml} Get the size of a group type: value key: SecurityGroups[].GroupId value_type: size value: 2 Membership example using swap type: value key: SecurityGroups[].GroupId value_type: swap op: in value: sg-49b87f44 Convert to integer before comparison type: value key: tag:Count op: greater-than value_type: integer value: 0 Apply only to rds instances created after the given date type: value key: InstanceCreateTime op: greater-than value_type: date value: \"2019/05/01\" Find instances launched within the last 31 days type: value key: LaunchTime op: less-than value_type: age value: 32 Find instances launched within the past 12 hours type: value key: LaunchTime op: less-than value_type: age value: 0.5 Use resource_count to filter resources based on the number that matched Note that no key is used for this value_type since it is matching on the size of the list of resources and not a specific field. type: value value_type: resource_count op: lt value: 2 This policy will use intersect op to compare rds instances subnet group list against a user provided list of public subnets from a s3 txt file. name: find-rds-on-public-subnets-using-s3-list comment: | The txt file needs to be in utf-8 no BOM format and contain one subnet per line in the file no quotes around the subnets either. resource: aws.rds filters: type: value key: \"DBSubnetGroup.Subnets[].SubnetIdentifier\" op: intersect value_from: url: s3://cloud-custodian-bucket/PublicSubnets.txt format: txt This policy will compare rds instances subnet group list against a inline user provided list of public subnets. name: find-rds-on-public-subnets-using-inline-list resource: aws.rds filters: type: value key: \"DBSubnetGroup.Subnets[].SubnetIdentifier\" op: intersect value: - subnet-2a8374658 - subnet-1b8474522 - subnet-2d2736444 ``` Value Regex: When using a Value Filter, a value_regex can be specified. This will mean that the value used for comparison is the output from evaluating a regex on the value found on a resource using [key]{.title-ref}. The filter expects that there will be exactly one capturing group, however non-capturing groups can be specified as well, e.g. (?:newkey|oldkey) . Note that if the value regex does not find a match, it will return a None value. In this example there is an expiration comparison, which needs a datetime, however the tag containing this information also has other data in it. By setting the value_regex to capture just the datetime part of the tag, the filter can be evaluated as normal. ``` {.yaml} Find expiry from tag contents type: value key: \"tag:metadata\" value_type: expiration value_regex: \". delete_after=([0-9]{4}-[0-9]{2}-[0-9]{2}). \" op: less-than value: 0 ``` Value From: value_from allows the use of external values in the Value Filter ::: {.autodoconly} c7n.resolver.ValuesFrom ::: Event Filter Filter against a CloudWatch event JSON associated to a resource type. The list of possible keys are now from the cloudtrail event and not the describe resource call as is the case in the ValueFilter {.yaml} - name: no-ec2-public-ips resource: aws.ec2 mode: type: cloudtrail events: - RunInstances filters: - type: event \u2500\u2510 The key is a JMESPath Query of key: \"detail.requestParameters.networkInterfaceSet.items[].associatePublicIpAddress\" \u251c\u25b6the event JSON from CloudWatch value: true \u2500\u2518 actions: - type: terminate force: true Reduce Filter The reduce filter lets you group, sort, and limit the number of resources to act on. Maybe you want to delete AMIs, but want to do it in small batches where you act on the oldest AMIs first. Or maybe you want to do some chaos engineering and randomly select ec2 instances part of ASGs, but want to make sure no more than one instance per ASG is affected. This filter lets you do that. This works using this process: Group resources Sort each group of resources Selecting a number of resources in each group Combine the resulting resources Grouping resources Resources are grouped based on the value extracted as defined by the group-by attribute. All resources not able to extract a value are placed in a group by themselves. This is also the case when group-by is not specified. Sorting resources Sorting of individual resources within a group is controlled by a combination of the sort-by and order attributes. sort-by determines which value to use to sort and order controls how they are sorted. For any resources with a null value, those are by default sorted last. You can optionally sort those first with the null-order attribute. Note: if neither sort-by or order are specified, no sorting is done. Selecting resources Once groups have been sorted, we can then apply rules to select a specific number of resources in each group. We first discard some resources and then limit the remaining set to a maximum count. When the discard or discard-percent attributes are specified, we take the ordered resources in each group and discard the first discard-percent of them or discard absolute count, whichever is larger. After discarding resources, we then limit the remaining set. limit-percent is applied first to reduce the number of resources to this percentage of the original. limit is then applied to allow for an absolute count. Resources are kept from the beginning of the list. To explain this with an example, suppose you have 50 resources in a group with all of these set: {.yaml} discard: 5 discard-percent: 20 limit: 10 limit-percent: 30 This would first discard the first 10 resources because 20 percent of 50 is 10, which is greater than 5. You now have 40 resources left in the group and the limit settings are applied. 30% of 40 is 12, but limit is set to 10, which is lower, so the first 10 of the remaining are kept. If they were numbered #1-50, you\\'d have discarded 1-10, kept 11-20, and dropped the remaining 21-50. If you had the following settings: {.yaml} discard-percent: 25 limit-percent: 50 We\\'d discard the first 25% of 50 (12), then of the remaining 38 resources, we\\'d keep 50% of those (19). You\\'d end up with resources 13-31. Now, some of these could eliminate all resources from a group. If you have 20 resources in one group and 5 in another and specify limit-percent = 10 , you\\'ll get 2 resources from the first group and 0 resources from the second. Combining resource groups Once the groups have been modified, we now need to combine them back to one set of resources. Since the groups are determined by a JMESPath expression, we sort the groups first based on the order attribute the same way we sort within a group. After the groups are sorted, it\\'s a simple concatenation of resources. Attributes group-by , sort-by These are both defined the same way... Note: For simplicity, you can specify these as just a single string which is treated as the key . key - The JMESPath expression to extract a value value_regex - A regular expression with a single capture group that extracts a portion of the result of the key expression. value_type - parse the value as one of the following: string (default) number date order controls how to sorting is done asc (default) - sort in ascending order based on key desc - sort in descending order based on key reverse - reverse the order of resources (ignores key ) randomize - randomize the order of resources (ignores key ) null-order - when sorting, where to put resources that have a null value last (default) - at the end of the list first - at the start of the list discard - discard the first N resources within each group discard-percent - discard the first N percentage of resources within each group limit - select the first N resources within each group (after discards) limit-percent - select the first N percentage of resources within each group (after discards) Examples This example will select the longest running instance from each ASG, then randomly choose 10% of those, making sure to not affect more than 15 instances total, then terminate them. {.yaml} - name: chaos-engineering resource: aws.ec2 filters: - \"State.Name\": \"running\" - \"tag:aws:autoscaling:groupName\": present - type: reduce group-by: \"tag:aws:autoscaling:groupName\" sort-by: \"LaunchTime\" order: asc limit: 1 - type: reduce order: randomize limit: 15 limit-percent: 10 actions: - terminate This example will delete old AMIs, but make sure to only do the top 10 based on age. {.yaml} - name: limited-ami-expiration resource: aws.ami filters: - type: image-age days: 180 op: ge - type: reduce sort-by: \"CreationDate\" order: asc limit: 10 actions: - deregister This example simply sorts the resources by when they are marked for expiration. We use a date type because the tags might be in different date formats or are not text-sortable. {.yaml} - name: ami-expiration-by-expire-date resource: aws.ami filters: - type: value key: \"tag:expire-after\" value_type: age op: gt value: 0 - type: reduce sort-by: key: \"tag:expire-after\" value_type: date order: asc limit: 10 actions: - deregister","title":"Filters"},{"location":"filters/#generic-filters-filters","text":"The following filters can be applied to all policies for all resources. See the provider specific resource reference for additional information.","title":"Generic Filters {#filters}"},{"location":"filters/#value-filter","text":"Cloud Custodian provides for a flexible query language on any resource by allowing for rich queries on JSON objects via JMESPath, and allows for mixing and combining those with boolean conditional operators that are nest-able. (Tutorial here on JMESPath syntax) The base value filter enables the use of jmespath with data returned from a describe call. filters: - type: value key: \"State[0]\" \u2500\u25b6 The value from the describe call value: \"running\" \u2500\u25b6 Value that is being filtered against There are several ways to get a list of possible keys for each resource. Via Custodian CLI Create a new custodian yaml file with just the name and resource fields. Then run custodian run -s OUTPUT_DIR . The valid key fields can be found in the output directory in resources.json {.yaml} policies: - name: my-first-policy resource: aws.ec2 Via Cloud Providers CLI Use the relevant cloud provider cli to run the describe call to view all available keys. For example using aws cli run aws ec2 describe-instances or with azure az vm list . Note: You do not need to include the outermost json field in most cases since custodian removes this field from the results. Via Cloud Provider Documentation Go to the relevant cloud provider sdk documentation and search for the describe api call for the resource you\\'re interested in. The available fields will be listed under the results of that api call. Comparison operators: : The generic value filter allows for comparison operators to be used - `equal` or `eq` - `not-equal` or `ne` - `greater-than` or `gt` - `gte` or `ge` - `less-than` or `lt` - `lte` or `le` {.yaml} filters: - type: value key: CpuOptions.CoreCount \u2500\u25b6 The value from the describe call value: 36 \u2500\u25b6 Value that is being compared op: greater-than \u2500\u25b6 Comparison Operator Other operators: : - absent - present - not-null - empty - contains {.yaml} filters: - type: value key: CpuOptions.CoreCount \u2500\u25b6 The value from the describe call value: present \u2500\u25b6 Checks if key is present Logical Operators: : - or or Or - and or And - not {.yaml} filters: - or: \u2500\u25b6 Logical Operator - type: value key: CpuOptions.CoreCount \u2500\u25b6 The value from the describe call value: 36 \u2500\u25b6 Value that is being compared - type: value key: CpuOptions.CoreCount \u2500\u25b6 The value from the describe call value: 42 \u2500\u25b6 Value that is being compared List Operators: : There is a collection of operators that can be used with user supplied lists. The operators are evaluated as value from key in (the operator) given value . If you would like it evaluated in the opposite way given value in (the operator) value from key then you can include the swap transformation or use the contains operator. - `in` - `not-in` or `ni` - `intersect` - Provides comparison between 2 lists {.yaml} filters: - type: value key: ImageId \u2500\u25b6 The value from the describe call op: in \u2500\u25b6 List operator value: [ID-123, ID-321] \u2500\u25b6 List of Values to be compared against {.yaml} filters: - type: value key: ImageId.List \u2500\u25b6 The value from the describe call op: in \u2500\u25b6 List operator value: ID-321 \u2500\u25b6 Values to be compared against value_type: swap \u2500\u25b6 Switches list comparison order Special operators: : - glob - Provides Glob matching support - regex - Provides Regex matching support but ignores case (1) - regex-case - Provides case sensitive Regex matching support (1) ``` {.yaml} filters: - type: value key: FunctionName \u2500\u25b6 The value from the describe call, or resources.json op: regex \u2500\u25b6 Special operator value: '(custodian|c7n) \\w+' \u2500\u25b6 Regex string: match all values beginning with custodian or c7n_ type: value key: name \u2500\u25b6 The value from the describe call, or resources.json op: regex \u2500\u25b6 Special operator value: '^. c7n. $' \u2500\u25b6 Regex string: match all values containing c7n type: value key: name \u2500\u25b6 The value from the describe call, or resources.json op: regex \u2500\u25b6 Special operator value: '^((?!c7n).)*$' \u2500\u25b6 Regex string: match all values not containing c7n ``` These operators are implemented using re.match . If a filter isn\\'t working as expected take a look at the [re]{.title-ref}__ documentation. __ https://docs.python.org/3/library/re.html#search-vs-match Transformations: Transformations on the value can be done using the value_type keyword. The following value types are supported: age - convert to a datetime (for past date comparisons) cidr - parse an ipaddress cidr_size - the length of the network prefix expiration - convert to a datetime (for future date comparisons) integer - convert the value to an integer normalize - convert the value to lowercase resource_count - compare against the number of matched resources size - the length of an element swap - swap the value and the evaluated key date - parse the filter\\'s value as a date. Note that the [age]{.title-ref} and [expiration]{.title-ref} transformations expect a value given as a number of days. Use a floating point value to match time periods shorter than a day. Examples: ``` {.yaml}","title":"Value Filter"},{"location":"filters/#get-the-size-of-a-group","text":"type: value key: SecurityGroups[].GroupId value_type: size value: 2","title":"Get the size of a group"},{"location":"filters/#membership-example-using-swap","text":"type: value key: SecurityGroups[].GroupId value_type: swap op: in value: sg-49b87f44","title":"Membership example using swap"},{"location":"filters/#convert-to-integer-before-comparison","text":"type: value key: tag:Count op: greater-than value_type: integer value: 0","title":"Convert to integer before comparison"},{"location":"filters/#apply-only-to-rds-instances-created-after-the-given-date","text":"type: value key: InstanceCreateTime op: greater-than value_type: date value: \"2019/05/01\"","title":"Apply only to rds instances created after the given date"},{"location":"filters/#find-instances-launched-within-the-last-31-days","text":"type: value key: LaunchTime op: less-than value_type: age value: 32","title":"Find instances launched within the last 31 days"},{"location":"filters/#find-instances-launched-within-the-past-12-hours","text":"type: value key: LaunchTime op: less-than value_type: age value: 0.5","title":"Find instances launched within the past 12 hours"},{"location":"filters/#use-resource_count-to-filter-resources-based-on-the-number-that-matched","text":"","title":"Use resource_count to filter resources based on the number that matched"},{"location":"filters/#note-that-no-key-is-used-for-this-value_type-since-it-is-matching-on","text":"","title":"Note that no key is used for this value_type since it is matching on"},{"location":"filters/#the-size-of-the-list-of-resources-and-not-a-specific-field","text":"type: value value_type: resource_count op: lt value: 2","title":"the size of the list of resources and not a specific field."},{"location":"filters/#this-policy-will-use-intersect-op-to-compare-rds-instances-subnet-group-list","text":"","title":"This policy will use intersect op to compare rds instances subnet group list"},{"location":"filters/#against-a-user-provided-list-of-public-subnets-from-a-s3-txt-file","text":"name: find-rds-on-public-subnets-using-s3-list comment: | The txt file needs to be in utf-8 no BOM format and contain one subnet per line in the file no quotes around the subnets either. resource: aws.rds filters: type: value key: \"DBSubnetGroup.Subnets[].SubnetIdentifier\" op: intersect value_from: url: s3://cloud-custodian-bucket/PublicSubnets.txt format: txt","title":"against a user provided list of public subnets from a s3 txt file."},{"location":"filters/#this-policy-will-compare-rds-instances-subnet-group-list-against-a","text":"","title":"This policy will compare rds instances subnet group list against a"},{"location":"filters/#inline-user-provided-list-of-public-subnets","text":"name: find-rds-on-public-subnets-using-inline-list resource: aws.rds filters: type: value key: \"DBSubnetGroup.Subnets[].SubnetIdentifier\" op: intersect value: - subnet-2a8374658 - subnet-1b8474522 - subnet-2d2736444 ``` Value Regex: When using a Value Filter, a value_regex can be specified. This will mean that the value used for comparison is the output from evaluating a regex on the value found on a resource using [key]{.title-ref}. The filter expects that there will be exactly one capturing group, however non-capturing groups can be specified as well, e.g. (?:newkey|oldkey) . Note that if the value regex does not find a match, it will return a None value. In this example there is an expiration comparison, which needs a datetime, however the tag containing this information also has other data in it. By setting the value_regex to capture just the datetime part of the tag, the filter can be evaluated as normal. ``` {.yaml}","title":"inline user provided list of public subnets."},{"location":"filters/#find-expiry-from-tag-contents","text":"type: value key: \"tag:metadata\" value_type: expiration value_regex: \". delete_after=([0-9]{4}-[0-9]{2}-[0-9]{2}). \" op: less-than value: 0 ``` Value From: value_from allows the use of external values in the Value Filter ::: {.autodoconly} c7n.resolver.ValuesFrom :::","title":"Find expiry from tag contents"},{"location":"filters/#event-filter","text":"Filter against a CloudWatch event JSON associated to a resource type. The list of possible keys are now from the cloudtrail event and not the describe resource call as is the case in the ValueFilter {.yaml} - name: no-ec2-public-ips resource: aws.ec2 mode: type: cloudtrail events: - RunInstances filters: - type: event \u2500\u2510 The key is a JMESPath Query of key: \"detail.requestParameters.networkInterfaceSet.items[].associatePublicIpAddress\" \u251c\u25b6the event JSON from CloudWatch value: true \u2500\u2518 actions: - type: terminate force: true","title":"Event Filter"},{"location":"filters/#reduce-filter","text":"The reduce filter lets you group, sort, and limit the number of resources to act on. Maybe you want to delete AMIs, but want to do it in small batches where you act on the oldest AMIs first. Or maybe you want to do some chaos engineering and randomly select ec2 instances part of ASGs, but want to make sure no more than one instance per ASG is affected. This filter lets you do that. This works using this process: Group resources Sort each group of resources Selecting a number of resources in each group Combine the resulting resources","title":"Reduce Filter"},{"location":"filters/#grouping-resources","text":"Resources are grouped based on the value extracted as defined by the group-by attribute. All resources not able to extract a value are placed in a group by themselves. This is also the case when group-by is not specified.","title":"Grouping resources"},{"location":"filters/#sorting-resources","text":"Sorting of individual resources within a group is controlled by a combination of the sort-by and order attributes. sort-by determines which value to use to sort and order controls how they are sorted. For any resources with a null value, those are by default sorted last. You can optionally sort those first with the null-order attribute. Note: if neither sort-by or order are specified, no sorting is done.","title":"Sorting resources"},{"location":"filters/#selecting-resources","text":"Once groups have been sorted, we can then apply rules to select a specific number of resources in each group. We first discard some resources and then limit the remaining set to a maximum count. When the discard or discard-percent attributes are specified, we take the ordered resources in each group and discard the first discard-percent of them or discard absolute count, whichever is larger. After discarding resources, we then limit the remaining set. limit-percent is applied first to reduce the number of resources to this percentage of the original. limit is then applied to allow for an absolute count. Resources are kept from the beginning of the list. To explain this with an example, suppose you have 50 resources in a group with all of these set: {.yaml} discard: 5 discard-percent: 20 limit: 10 limit-percent: 30 This would first discard the first 10 resources because 20 percent of 50 is 10, which is greater than 5. You now have 40 resources left in the group and the limit settings are applied. 30% of 40 is 12, but limit is set to 10, which is lower, so the first 10 of the remaining are kept. If they were numbered #1-50, you\\'d have discarded 1-10, kept 11-20, and dropped the remaining 21-50. If you had the following settings: {.yaml} discard-percent: 25 limit-percent: 50 We\\'d discard the first 25% of 50 (12), then of the remaining 38 resources, we\\'d keep 50% of those (19). You\\'d end up with resources 13-31. Now, some of these could eliminate all resources from a group. If you have 20 resources in one group and 5 in another and specify limit-percent = 10 , you\\'ll get 2 resources from the first group and 0 resources from the second.","title":"Selecting resources"},{"location":"filters/#combining-resource-groups","text":"Once the groups have been modified, we now need to combine them back to one set of resources. Since the groups are determined by a JMESPath expression, we sort the groups first based on the order attribute the same way we sort within a group. After the groups are sorted, it\\'s a simple concatenation of resources.","title":"Combining resource groups"},{"location":"filters/#attributes","text":"group-by , sort-by These are both defined the same way... Note: For simplicity, you can specify these as just a single string which is treated as the key . key - The JMESPath expression to extract a value value_regex - A regular expression with a single capture group that extracts a portion of the result of the key expression. value_type - parse the value as one of the following: string (default) number date order controls how to sorting is done asc (default) - sort in ascending order based on key desc - sort in descending order based on key reverse - reverse the order of resources (ignores key ) randomize - randomize the order of resources (ignores key ) null-order - when sorting, where to put resources that have a null value last (default) - at the end of the list first - at the start of the list discard - discard the first N resources within each group discard-percent - discard the first N percentage of resources within each group limit - select the first N resources within each group (after discards) limit-percent - select the first N percentage of resources within each group (after discards)","title":"Attributes"},{"location":"filters/#examples","text":"This example will select the longest running instance from each ASG, then randomly choose 10% of those, making sure to not affect more than 15 instances total, then terminate them. {.yaml} - name: chaos-engineering resource: aws.ec2 filters: - \"State.Name\": \"running\" - \"tag:aws:autoscaling:groupName\": present - type: reduce group-by: \"tag:aws:autoscaling:groupName\" sort-by: \"LaunchTime\" order: asc limit: 1 - type: reduce order: randomize limit: 15 limit-percent: 10 actions: - terminate This example will delete old AMIs, but make sure to only do the top 10 based on age. {.yaml} - name: limited-ami-expiration resource: aws.ami filters: - type: image-age days: 180 op: ge - type: reduce sort-by: \"CreationDate\" order: asc limit: 10 actions: - deregister This example simply sorts the resources by when they are marked for expiration. We use a date type because the tags might be in different date formats or are not text-sortable. {.yaml} - name: ami-expiration-by-expire-date resource: aws.ami filters: - type: value key: \"tag:expire-after\" value_type: age op: gt value: 0 - type: reduce sort-by: key: \"tag:expire-after\" value_type: date order: asc limit: 10 actions: - deregister","title":"Examples"},{"location":"aws/contribute/","text":"Developer Guide {#aws_contribute} Cloud Custodian is a Python application and supports Python 3 on MacOS, Linux, and Windows. It is recommended using Python 3.7 or higher. Run the following commands in the root directory after cloning Cloud Custodian: make install source bin/activate This creates a virtual env in your enlistment and installs all packages as editable. Now you may run custodian with any flags in order to directly test changes to the source files. For example, custodian schema aws.<resource_type> will return schema for resource type. Adding New AWS Resources Create New AWS Resource Each class definition will use the @resources.register('<resource_name>') decorator to register that class as a Custodian resource substituting [\\<resource_name>]{.title-ref} with the new resource name. The name specified in the decorator is how the resource will be referenced within policies. Register the new resource: @resources.register(\u2018<resource_name>\u2019) An outer class defining the reference in resource mapping: class <resource_type>(query.QueryResourceManager) Interior class that defines the aws metadata for resource class resource_type(query.TypeInfo) : ::: {.autoclass} c7n.query.TypeInfo ::: An example that adds a new resource: @resources.register('scaling-policies') class ScalingPolicies(query.QueryResourceManager): # interior class that defines the aws metadata for resource class resource_type(query.TypeInfo): service = 'autoscaling' arn_type = \"scalingPolicy\" id = name = 'PolicyName' date = 'CreatedTime' # this defines the boto3 call for the resource as well as JMESPATH # for accessing TL resources enum_spec = ( 'describe_policies', 'ScalingPolicies', None ) filter_name = 'PolicyNames' filter_type = 'list' cfn_type = config_type = 'AWS::AutoScaling::ScalingPolicy' Load New AWS Resource If you created a new module for an AWS service (i.e. this was the first resource implemented for this service in Custodian), then import the new service module in resource_map.py : \"aws.<name of resource>\": \"c7n.resources.<name of file>.<name of resource class>\" Add New Filter A filter can be added with a decorator and class: @<New-resource-class>.filter_registry.register('<filter-name>') class <NewFilterName>(ValueFilter) An example that adds a new filter for scaling policies to the ASG resource: @ASG.filter_registry.register('scaling-policies') class ScalingPoliciesFilter(ValueFilter): schema = type_schema( 'scaling-policies', rinherit=ValueFilter.schema ) schema_alias = False permissions = (\"autoscaling:DescribePolicies\",) def process(self, asgs, event=None): self.policy_info = PolicyInfo(self.manager).initialize(asgs) return super(ScalingPoliciesFilter, self).process(asgs, event) def __call__(self, asg): asg_policies = self.policy_info.get(asg) matched = False if asg_policies is not None: for policy in asg_policies: matched = self.match(policy) or matched return matched Add New Action An action can be added with a decorator and class: @<New-resource-class>.action_registry.register('<action-name>') class <NewActionName>(Action) An example that adds a new action for deleting to the ASG resource: @ASG.action_registry.register('delete') class Delete(Action): schema = type_schema('delete', force={'type': 'boolean'}) permissions = (\"autoscaling:DeleteAutoScalingGroup\",) def process(self, asgs): client = local_session( self.manager.session_factory).client('autoscaling') for asg in asgs: self.process_asg(client, asg) def process_asg(self, client, asg): force_delete = self.data.get('force', False) try: self.manager.retry( client.delete_auto_scaling_group, AutoScalingGroupName=asg['AutoScalingGroupName'], ForceDelete=force_delete) except ClientError as e: if e.response['Error']['Code'] == 'ValidationError': return raise Testing For information regarding testing see testing for developers<developer-tests> {.interpreted-text role=\"ref\"}.","title":"Contribute"},{"location":"aws/contribute/#developer-guide-aws_contribute","text":"Cloud Custodian is a Python application and supports Python 3 on MacOS, Linux, and Windows. It is recommended using Python 3.7 or higher. Run the following commands in the root directory after cloning Cloud Custodian: make install source bin/activate This creates a virtual env in your enlistment and installs all packages as editable. Now you may run custodian with any flags in order to directly test changes to the source files. For example, custodian schema aws.<resource_type> will return schema for resource type.","title":"Developer Guide {#aws_contribute}"},{"location":"aws/contribute/#adding-new-aws-resources","text":"","title":"Adding New AWS Resources"},{"location":"aws/contribute/#create-new-aws-resource","text":"Each class definition will use the @resources.register('<resource_name>') decorator to register that class as a Custodian resource substituting [\\<resource_name>]{.title-ref} with the new resource name. The name specified in the decorator is how the resource will be referenced within policies. Register the new resource: @resources.register(\u2018<resource_name>\u2019) An outer class defining the reference in resource mapping: class <resource_type>(query.QueryResourceManager) Interior class that defines the aws metadata for resource class resource_type(query.TypeInfo) : ::: {.autoclass} c7n.query.TypeInfo ::: An example that adds a new resource: @resources.register('scaling-policies') class ScalingPolicies(query.QueryResourceManager): # interior class that defines the aws metadata for resource class resource_type(query.TypeInfo): service = 'autoscaling' arn_type = \"scalingPolicy\" id = name = 'PolicyName' date = 'CreatedTime' # this defines the boto3 call for the resource as well as JMESPATH # for accessing TL resources enum_spec = ( 'describe_policies', 'ScalingPolicies', None ) filter_name = 'PolicyNames' filter_type = 'list' cfn_type = config_type = 'AWS::AutoScaling::ScalingPolicy'","title":"Create New AWS Resource"},{"location":"aws/contribute/#load-new-aws-resource","text":"If you created a new module for an AWS service (i.e. this was the first resource implemented for this service in Custodian), then import the new service module in resource_map.py : \"aws.<name of resource>\": \"c7n.resources.<name of file>.<name of resource class>\"","title":"Load New AWS Resource"},{"location":"aws/contribute/#add-new-filter","text":"A filter can be added with a decorator and class: @<New-resource-class>.filter_registry.register('<filter-name>') class <NewFilterName>(ValueFilter) An example that adds a new filter for scaling policies to the ASG resource: @ASG.filter_registry.register('scaling-policies') class ScalingPoliciesFilter(ValueFilter): schema = type_schema( 'scaling-policies', rinherit=ValueFilter.schema ) schema_alias = False permissions = (\"autoscaling:DescribePolicies\",) def process(self, asgs, event=None): self.policy_info = PolicyInfo(self.manager).initialize(asgs) return super(ScalingPoliciesFilter, self).process(asgs, event) def __call__(self, asg): asg_policies = self.policy_info.get(asg) matched = False if asg_policies is not None: for policy in asg_policies: matched = self.match(policy) or matched return matched","title":"Add New Filter"},{"location":"aws/contribute/#add-new-action","text":"An action can be added with a decorator and class: @<New-resource-class>.action_registry.register('<action-name>') class <NewActionName>(Action) An example that adds a new action for deleting to the ASG resource: @ASG.action_registry.register('delete') class Delete(Action): schema = type_schema('delete', force={'type': 'boolean'}) permissions = (\"autoscaling:DeleteAutoScalingGroup\",) def process(self, asgs): client = local_session( self.manager.session_factory).client('autoscaling') for asg in asgs: self.process_asg(client, asg) def process_asg(self, client, asg): force_delete = self.data.get('force', False) try: self.manager.retry( client.delete_auto_scaling_group, AutoScalingGroupName=asg['AutoScalingGroupName'], ForceDelete=force_delete) except ClientError as e: if e.response['Error']['Code'] == 'ValidationError': return raise","title":"Add New Action"},{"location":"aws/contribute/#testing","text":"For information regarding testing see testing for developers<developer-tests> {.interpreted-text role=\"ref\"}.","title":"Testing"},{"location":"aws/contribute.rst/","text":"Developer Guide {#aws_contribute} Cloud Custodian is a Python application and supports Python 3 on MacOS, Linux, and Windows. It is recommended using Python 3.7 or higher. Run the following commands in the root directory after cloning Cloud Custodian: make install source bin/activate This creates a virtual env in your enlistment and installs all packages as editable. Now you may run custodian with any flags in order to directly test changes to the source files. For example, custodian schema aws.<resource_type> will return schema for resource type. Adding New AWS Resources Create New AWS Resource Each class definition will use the @resources.register('<resource_name>') decorator to register that class as a Custodian resource substituting [\\<resource_name>]{.title-ref} with the new resource name. The name specified in the decorator is how the resource will be referenced within policies. Register the new resource: @resources.register(\u2018<resource_name>\u2019) An outer class defining the reference in resource mapping: class <resource_type>(query.QueryResourceManager) Interior class that defines the aws metadata for resource class resource_type(query.TypeInfo) : ::: {.autoclass} c7n.query.TypeInfo ::: An example that adds a new resource: @resources.register('scaling-policies') class ScalingPolicies(query.QueryResourceManager): # interior class that defines the aws metadata for resource class resource_type(query.TypeInfo): service = 'autoscaling' arn_type = \"scalingPolicy\" id = name = 'PolicyName' date = 'CreatedTime' # this defines the boto3 call for the resource as well as JMESPATH # for accessing TL resources enum_spec = ( 'describe_policies', 'ScalingPolicies', None ) filter_name = 'PolicyNames' filter_type = 'list' cfn_type = config_type = 'AWS::AutoScaling::ScalingPolicy' Load New AWS Resource If you created a new module for an AWS service (i.e. this was the first resource implemented for this service in Custodian), then import the new service module in resource_map.py : \"aws.<name of resource>\": \"c7n.resources.<name of file>.<name of resource class>\" Add New Filter A filter can be added with a decorator and class: @<New-resource-class>.filter_registry.register('<filter-name>') class <NewFilterName>(ValueFilter) An example that adds a new filter for scaling policies to the ASG resource: @ASG.filter_registry.register('scaling-policies') class ScalingPoliciesFilter(ValueFilter): schema = type_schema( 'scaling-policies', rinherit=ValueFilter.schema ) schema_alias = False permissions = (\"autoscaling:DescribePolicies\",) def process(self, asgs, event=None): self.policy_info = PolicyInfo(self.manager).initialize(asgs) return super(ScalingPoliciesFilter, self).process(asgs, event) def __call__(self, asg): asg_policies = self.policy_info.get(asg) matched = False if asg_policies is not None: for policy in asg_policies: matched = self.match(policy) or matched return matched Add New Action An action can be added with a decorator and class: @<New-resource-class>.action_registry.register('<action-name>') class <NewActionName>(Action) An example that adds a new action for deleting to the ASG resource: @ASG.action_registry.register('delete') class Delete(Action): schema = type_schema('delete', force={'type': 'boolean'}) permissions = (\"autoscaling:DeleteAutoScalingGroup\",) def process(self, asgs): client = local_session( self.manager.session_factory).client('autoscaling') for asg in asgs: self.process_asg(client, asg) def process_asg(self, client, asg): force_delete = self.data.get('force', False) try: self.manager.retry( client.delete_auto_scaling_group, AutoScalingGroupName=asg['AutoScalingGroupName'], ForceDelete=force_delete) except ClientError as e: if e.response['Error']['Code'] == 'ValidationError': return raise Testing For information regarding testing see testing for developers<developer-tests> {.interpreted-text role=\"ref\"}.","title":"Contribute.rst"},{"location":"aws/contribute.rst/#developer-guide-aws_contribute","text":"Cloud Custodian is a Python application and supports Python 3 on MacOS, Linux, and Windows. It is recommended using Python 3.7 or higher. Run the following commands in the root directory after cloning Cloud Custodian: make install source bin/activate This creates a virtual env in your enlistment and installs all packages as editable. Now you may run custodian with any flags in order to directly test changes to the source files. For example, custodian schema aws.<resource_type> will return schema for resource type.","title":"Developer Guide {#aws_contribute}"},{"location":"aws/contribute.rst/#adding-new-aws-resources","text":"","title":"Adding New AWS Resources"},{"location":"aws/contribute.rst/#create-new-aws-resource","text":"Each class definition will use the @resources.register('<resource_name>') decorator to register that class as a Custodian resource substituting [\\<resource_name>]{.title-ref} with the new resource name. The name specified in the decorator is how the resource will be referenced within policies. Register the new resource: @resources.register(\u2018<resource_name>\u2019) An outer class defining the reference in resource mapping: class <resource_type>(query.QueryResourceManager) Interior class that defines the aws metadata for resource class resource_type(query.TypeInfo) : ::: {.autoclass} c7n.query.TypeInfo ::: An example that adds a new resource: @resources.register('scaling-policies') class ScalingPolicies(query.QueryResourceManager): # interior class that defines the aws metadata for resource class resource_type(query.TypeInfo): service = 'autoscaling' arn_type = \"scalingPolicy\" id = name = 'PolicyName' date = 'CreatedTime' # this defines the boto3 call for the resource as well as JMESPATH # for accessing TL resources enum_spec = ( 'describe_policies', 'ScalingPolicies', None ) filter_name = 'PolicyNames' filter_type = 'list' cfn_type = config_type = 'AWS::AutoScaling::ScalingPolicy'","title":"Create New AWS Resource"},{"location":"aws/contribute.rst/#load-new-aws-resource","text":"If you created a new module for an AWS service (i.e. this was the first resource implemented for this service in Custodian), then import the new service module in resource_map.py : \"aws.<name of resource>\": \"c7n.resources.<name of file>.<name of resource class>\"","title":"Load New AWS Resource"},{"location":"aws/contribute.rst/#add-new-filter","text":"A filter can be added with a decorator and class: @<New-resource-class>.filter_registry.register('<filter-name>') class <NewFilterName>(ValueFilter) An example that adds a new filter for scaling policies to the ASG resource: @ASG.filter_registry.register('scaling-policies') class ScalingPoliciesFilter(ValueFilter): schema = type_schema( 'scaling-policies', rinherit=ValueFilter.schema ) schema_alias = False permissions = (\"autoscaling:DescribePolicies\",) def process(self, asgs, event=None): self.policy_info = PolicyInfo(self.manager).initialize(asgs) return super(ScalingPoliciesFilter, self).process(asgs, event) def __call__(self, asg): asg_policies = self.policy_info.get(asg) matched = False if asg_policies is not None: for policy in asg_policies: matched = self.match(policy) or matched return matched","title":"Add New Filter"},{"location":"aws/contribute.rst/#add-new-action","text":"An action can be added with a decorator and class: @<New-resource-class>.action_registry.register('<action-name>') class <NewActionName>(Action) An example that adds a new action for deleting to the ASG resource: @ASG.action_registry.register('delete') class Delete(Action): schema = type_schema('delete', force={'type': 'boolean'}) permissions = (\"autoscaling:DeleteAutoScalingGroup\",) def process(self, asgs): client = local_session( self.manager.session_factory).client('autoscaling') for asg in asgs: self.process_asg(client, asg) def process_asg(self, client, asg): force_delete = self.data.get('force', False) try: self.manager.retry( client.delete_auto_scaling_group, AutoScalingGroupName=asg['AutoScalingGroupName'], ForceDelete=force_delete) except ClientError as e: if e.response['Error']['Code'] == 'ValidationError': return raise","title":"Add New Action"},{"location":"aws/contribute.rst/#testing","text":"For information regarding testing see testing for developers<developer-tests> {.interpreted-text role=\"ref\"}.","title":"Testing"},{"location":"aws/gettingstarted/","text":"Getting Started {#aws-gettingstarted} Write your first policy {#aws-write-policy} A policy specifies the following items: The type of resource to run the policy against Filters to narrow down the set of resources Actions to take on the filtered set of resources For this tutorial, let\\'s stop all EC2 instances that are tagged with Custodian . To get started, go make an EC2 instance in your AWS console , and tag it with the key Custodian (any value). Also, make sure you have an access key handy. Then, create a file named custodian.yml with this content: policies: - name: my-first-policy resource: aws.ec2 filters: - \"tag:Custodian\": present At this point, we have specified the following things: The name of the policy The resource type to query against, in this case (aws.ec2) The filters list The Custodian tag filter Running this policy will not execute any actions as the actions list does not exist. We can extend this example to stop the instances that are actually filtered in by the Custodian tag filter by simply specifying the stop action: policies: - name: my-first-policy resource: aws.ec2 filters: - \"tag:Custodian\": present actions: - stop Run your policy Now, run Custodian: AWS_ACCESS_KEY_ID=\"foo\" AWS_SECRET_ACCESS_KEY=\"bar\" custodian run --output-dir=. custodian.yml Note: If you already have AWS credentials configured for AWS CLI or SDK access, then you may omit providing them on the command line. If successful, you should see output similar to the following on the command line: 2016-12-20 08:35:06,133: custodian.policy:INFO Running policy my-first-policy resource: ec2 region:us-east-1 c7n:0.8.21.2 2016-12-20 08:35:07,514: custodian.resources.ec2:INFO Filtered from 3 to 1 ec2 2016-12-20 08:35:07,514: custodian.policy:INFO policy: my-first-policy resource:ec2 has count:1 time:1.38 2016-12-20 08:35:07,515: custodian.actions:INFO Stop 1 of 1 instances 2016-12-20 08:35:08,188: custodian.policy:INFO policy: my-first-policy action: stop resources: 1 execution_time: 0.67 You should also find a new my-first-policy directory with a log and other files (subsequent runs will append to the log by default rather than overwriting it). Lastly, you should find the instance stopping or stopped in your AWS console. Congratulations, and welcome to Custodian! See our extended example of a policy\\'s structure tag compliance policy <policyStructure> {.interpreted-text role=\"ref\"}, or browse all of our use case recipes <usecases> {.interpreted-text role=\"ref\"}. A 2nd Example Policy First a role must be created with the appropriate permissions for custodian to act on the resources described in the policies yaml given as an example below. For convenience, an example policy is provided for this quick start guide. Customized AWS IAM policies will be necessary for your own custodian policies To implement the policy: Open the AWS console Navigate to IAM -> Policies Use the json option to copy the example policy as a new AWS IAM Policy Name the IAM policy as something recognizable and save it. Navigate to IAM -> Roles and create a role called CloudCustodian-QuickStart Assign the role the IAM policy created above. Now with the pre-requisite completed; you are ready continue and run custodian. A custodian policy file needs to be created in YAML format, as an example policies: - name: s3-cross-account description: | Checks S3 for buckets with cross-account access and removes the cross-account access. resource: s3 region: us-east-1 filters: - type: cross-account actions: - type: remove-statements statement_ids: matched - name: ec2-require-non-public-and-encrypted-volumes resource: aws.ec2 description: | Provision a lambda and cloud watch event target that looks at all new instances and terminates those with unencrypted volumes. mode: type: cloudtrail role: CloudCustodian-QuickStart events: - RunInstances filters: - type: ebs key: Encrypted value: false actions: - terminate - name: tag-compliance resource: aws.ec2 description: | Schedule a resource that does not meet tag compliance policies to be stopped in four days. filters: - State.Name: running - \"tag:Environment\": absent - \"tag:AppId\": absent - or: - \"tag:OwnerContact\": absent - \"tag:DeptID\": absent actions: - type: mark-for-op op: stop days: 4 Given that, you can run Cloud Custodian with # Validate the configuration (note this happens by default on run) custodian validate policy.yml # Dryrun on the policies (no actions executed) to see what resources # match each policy. custodian run --dryrun -s out policy.yml # Run the policy custodian run -s out policy.yml Monitor AWS {#monitor-aws-cc} You can generate CloudWatch metrics by specifying the --metrics flag and specifying aws : custodian run -s <output_directory> --metrics aws <policyfile>.yml You can also upload Cloud Custodian logs to CloudWatch logs: custodian run --log-group=/cloud-custodian/<dev-account>/<region> -s <output_directory> <policyfile>.yml And you can output logs and resource records to S3: custodian run -s s3://<my-bucket><my-prefix> <policyfile>.yml If Custodian is being run without Assume Roles, all output will be put into the same account. Custodian is built with the ability to be run from different accounts and leverage STS Role Assumption for cross-account access. Users can leverage the metrics that are being generated after each run by creating Custodian Dashboards in CloudWatch. Troubleshooting & Tinkering If you are not using the us-east-1 region, then you\\'ll need to specify that as well, either on the command line or in an environment variable: --region=us-west-1 AWS_DEFAULT_REGION=us-west-1","title":"Gettingstarted"},{"location":"aws/gettingstarted/#getting-started-aws-gettingstarted","text":"","title":"Getting Started {#aws-gettingstarted}"},{"location":"aws/gettingstarted/#write-your-first-policy-aws-write-policy","text":"A policy specifies the following items: The type of resource to run the policy against Filters to narrow down the set of resources Actions to take on the filtered set of resources For this tutorial, let\\'s stop all EC2 instances that are tagged with Custodian . To get started, go make an EC2 instance in your AWS console , and tag it with the key Custodian (any value). Also, make sure you have an access key handy. Then, create a file named custodian.yml with this content: policies: - name: my-first-policy resource: aws.ec2 filters: - \"tag:Custodian\": present At this point, we have specified the following things: The name of the policy The resource type to query against, in this case (aws.ec2) The filters list The Custodian tag filter Running this policy will not execute any actions as the actions list does not exist. We can extend this example to stop the instances that are actually filtered in by the Custodian tag filter by simply specifying the stop action: policies: - name: my-first-policy resource: aws.ec2 filters: - \"tag:Custodian\": present actions: - stop","title":"Write your first policy {#aws-write-policy}"},{"location":"aws/gettingstarted/#run-your-policy","text":"Now, run Custodian: AWS_ACCESS_KEY_ID=\"foo\" AWS_SECRET_ACCESS_KEY=\"bar\" custodian run --output-dir=. custodian.yml Note: If you already have AWS credentials configured for AWS CLI or SDK access, then you may omit providing them on the command line. If successful, you should see output similar to the following on the command line: 2016-12-20 08:35:06,133: custodian.policy:INFO Running policy my-first-policy resource: ec2 region:us-east-1 c7n:0.8.21.2 2016-12-20 08:35:07,514: custodian.resources.ec2:INFO Filtered from 3 to 1 ec2 2016-12-20 08:35:07,514: custodian.policy:INFO policy: my-first-policy resource:ec2 has count:1 time:1.38 2016-12-20 08:35:07,515: custodian.actions:INFO Stop 1 of 1 instances 2016-12-20 08:35:08,188: custodian.policy:INFO policy: my-first-policy action: stop resources: 1 execution_time: 0.67 You should also find a new my-first-policy directory with a log and other files (subsequent runs will append to the log by default rather than overwriting it). Lastly, you should find the instance stopping or stopped in your AWS console. Congratulations, and welcome to Custodian! See our extended example of a policy\\'s structure tag compliance policy <policyStructure> {.interpreted-text role=\"ref\"}, or browse all of our use case recipes <usecases> {.interpreted-text role=\"ref\"}.","title":"Run your policy"},{"location":"aws/gettingstarted/#a-2nd-example-policy","text":"First a role must be created with the appropriate permissions for custodian to act on the resources described in the policies yaml given as an example below. For convenience, an example policy is provided for this quick start guide. Customized AWS IAM policies will be necessary for your own custodian policies To implement the policy: Open the AWS console Navigate to IAM -> Policies Use the json option to copy the example policy as a new AWS IAM Policy Name the IAM policy as something recognizable and save it. Navigate to IAM -> Roles and create a role called CloudCustodian-QuickStart Assign the role the IAM policy created above. Now with the pre-requisite completed; you are ready continue and run custodian. A custodian policy file needs to be created in YAML format, as an example policies: - name: s3-cross-account description: | Checks S3 for buckets with cross-account access and removes the cross-account access. resource: s3 region: us-east-1 filters: - type: cross-account actions: - type: remove-statements statement_ids: matched - name: ec2-require-non-public-and-encrypted-volumes resource: aws.ec2 description: | Provision a lambda and cloud watch event target that looks at all new instances and terminates those with unencrypted volumes. mode: type: cloudtrail role: CloudCustodian-QuickStart events: - RunInstances filters: - type: ebs key: Encrypted value: false actions: - terminate - name: tag-compliance resource: aws.ec2 description: | Schedule a resource that does not meet tag compliance policies to be stopped in four days. filters: - State.Name: running - \"tag:Environment\": absent - \"tag:AppId\": absent - or: - \"tag:OwnerContact\": absent - \"tag:DeptID\": absent actions: - type: mark-for-op op: stop days: 4 Given that, you can run Cloud Custodian with # Validate the configuration (note this happens by default on run) custodian validate policy.yml # Dryrun on the policies (no actions executed) to see what resources # match each policy. custodian run --dryrun -s out policy.yml # Run the policy custodian run -s out policy.yml","title":"A 2nd Example Policy"},{"location":"aws/gettingstarted/#monitor-aws-monitor-aws-cc","text":"You can generate CloudWatch metrics by specifying the --metrics flag and specifying aws : custodian run -s <output_directory> --metrics aws <policyfile>.yml You can also upload Cloud Custodian logs to CloudWatch logs: custodian run --log-group=/cloud-custodian/<dev-account>/<region> -s <output_directory> <policyfile>.yml And you can output logs and resource records to S3: custodian run -s s3://<my-bucket><my-prefix> <policyfile>.yml If Custodian is being run without Assume Roles, all output will be put into the same account. Custodian is built with the ability to be run from different accounts and leverage STS Role Assumption for cross-account access. Users can leverage the metrics that are being generated after each run by creating Custodian Dashboards in CloudWatch.","title":"Monitor AWS {#monitor-aws-cc}"},{"location":"aws/gettingstarted/#troubleshooting-tinkering","text":"If you are not using the us-east-1 region, then you\\'ll need to specify that as well, either on the command line or in an environment variable: --region=us-west-1 AWS_DEFAULT_REGION=us-west-1","title":"Troubleshooting &amp; Tinkering"},{"location":"aws/gettingstarted.rst/","text":"Getting Started {#aws-gettingstarted} Write your first policy {#aws-write-policy} A policy specifies the following items: The type of resource to run the policy against Filters to narrow down the set of resources Actions to take on the filtered set of resources For this tutorial, let\\'s stop all EC2 instances that are tagged with Custodian . To get started, go make an EC2 instance in your AWS console , and tag it with the key Custodian (any value). Also, make sure you have an access key handy. Then, create a file named custodian.yml with this content: policies: - name: my-first-policy resource: aws.ec2 filters: - \"tag:Custodian\": present At this point, we have specified the following things: The name of the policy The resource type to query against, in this case (aws.ec2) The filters list The Custodian tag filter Running this policy will not execute any actions as the actions list does not exist. We can extend this example to stop the instances that are actually filtered in by the Custodian tag filter by simply specifying the stop action: policies: - name: my-first-policy resource: aws.ec2 filters: - \"tag:Custodian\": present actions: - stop Run your policy Now, run Custodian: AWS_ACCESS_KEY_ID=\"foo\" AWS_SECRET_ACCESS_KEY=\"bar\" custodian run --output-dir=. custodian.yml Note: If you already have AWS credentials configured for AWS CLI or SDK access, then you may omit providing them on the command line. If successful, you should see output similar to the following on the command line: 2016-12-20 08:35:06,133: custodian.policy:INFO Running policy my-first-policy resource: ec2 region:us-east-1 c7n:0.8.21.2 2016-12-20 08:35:07,514: custodian.resources.ec2:INFO Filtered from 3 to 1 ec2 2016-12-20 08:35:07,514: custodian.policy:INFO policy: my-first-policy resource:ec2 has count:1 time:1.38 2016-12-20 08:35:07,515: custodian.actions:INFO Stop 1 of 1 instances 2016-12-20 08:35:08,188: custodian.policy:INFO policy: my-first-policy action: stop resources: 1 execution_time: 0.67 You should also find a new my-first-policy directory with a log and other files (subsequent runs will append to the log by default rather than overwriting it). Lastly, you should find the instance stopping or stopped in your AWS console. Congratulations, and welcome to Custodian! See our extended example of a policy\\'s structure tag compliance policy <policyStructure> {.interpreted-text role=\"ref\"}, or browse all of our use case recipes <usecases> {.interpreted-text role=\"ref\"}. A 2nd Example Policy First a role must be created with the appropriate permissions for custodian to act on the resources described in the policies yaml given as an example below. For convenience, an example policy is provided for this quick start guide. Customized AWS IAM policies will be necessary for your own custodian policies To implement the policy: Open the AWS console Navigate to IAM -> Policies Use the json option to copy the example policy as a new AWS IAM Policy Name the IAM policy as something recognizable and save it. Navigate to IAM -> Roles and create a role called CloudCustodian-QuickStart Assign the role the IAM policy created above. Now with the pre-requisite completed; you are ready continue and run custodian. A custodian policy file needs to be created in YAML format, as an example policies: - name: s3-cross-account description: | Checks S3 for buckets with cross-account access and removes the cross-account access. resource: s3 region: us-east-1 filters: - type: cross-account actions: - type: remove-statements statement_ids: matched - name: ec2-require-non-public-and-encrypted-volumes resource: aws.ec2 description: | Provision a lambda and cloud watch event target that looks at all new instances and terminates those with unencrypted volumes. mode: type: cloudtrail role: CloudCustodian-QuickStart events: - RunInstances filters: - type: ebs key: Encrypted value: false actions: - terminate - name: tag-compliance resource: aws.ec2 description: | Schedule a resource that does not meet tag compliance policies to be stopped in four days. filters: - State.Name: running - \"tag:Environment\": absent - \"tag:AppId\": absent - or: - \"tag:OwnerContact\": absent - \"tag:DeptID\": absent actions: - type: mark-for-op op: stop days: 4 Given that, you can run Cloud Custodian with # Validate the configuration (note this happens by default on run) custodian validate policy.yml # Dryrun on the policies (no actions executed) to see what resources # match each policy. custodian run --dryrun -s out policy.yml # Run the policy custodian run -s out policy.yml Monitor AWS {#monitor-aws-cc} You can generate CloudWatch metrics by specifying the --metrics flag and specifying aws : custodian run -s <output_directory> --metrics aws <policyfile>.yml You can also upload Cloud Custodian logs to CloudWatch logs: custodian run --log-group=/cloud-custodian/<dev-account>/<region> -s <output_directory> <policyfile>.yml And you can output logs and resource records to S3: custodian run -s s3://<my-bucket><my-prefix> <policyfile>.yml If Custodian is being run without Assume Roles, all output will be put into the same account. Custodian is built with the ability to be run from different accounts and leverage STS Role Assumption for cross-account access. Users can leverage the metrics that are being generated after each run by creating Custodian Dashboards in CloudWatch. Troubleshooting & Tinkering If you are not using the us-east-1 region, then you\\'ll need to specify that as well, either on the command line or in an environment variable: --region=us-west-1 AWS_DEFAULT_REGION=us-west-1","title":"Gettingstarted.rst"},{"location":"aws/gettingstarted.rst/#getting-started-aws-gettingstarted","text":"","title":"Getting Started {#aws-gettingstarted}"},{"location":"aws/gettingstarted.rst/#write-your-first-policy-aws-write-policy","text":"A policy specifies the following items: The type of resource to run the policy against Filters to narrow down the set of resources Actions to take on the filtered set of resources For this tutorial, let\\'s stop all EC2 instances that are tagged with Custodian . To get started, go make an EC2 instance in your AWS console , and tag it with the key Custodian (any value). Also, make sure you have an access key handy. Then, create a file named custodian.yml with this content: policies: - name: my-first-policy resource: aws.ec2 filters: - \"tag:Custodian\": present At this point, we have specified the following things: The name of the policy The resource type to query against, in this case (aws.ec2) The filters list The Custodian tag filter Running this policy will not execute any actions as the actions list does not exist. We can extend this example to stop the instances that are actually filtered in by the Custodian tag filter by simply specifying the stop action: policies: - name: my-first-policy resource: aws.ec2 filters: - \"tag:Custodian\": present actions: - stop","title":"Write your first policy {#aws-write-policy}"},{"location":"aws/gettingstarted.rst/#run-your-policy","text":"Now, run Custodian: AWS_ACCESS_KEY_ID=\"foo\" AWS_SECRET_ACCESS_KEY=\"bar\" custodian run --output-dir=. custodian.yml Note: If you already have AWS credentials configured for AWS CLI or SDK access, then you may omit providing them on the command line. If successful, you should see output similar to the following on the command line: 2016-12-20 08:35:06,133: custodian.policy:INFO Running policy my-first-policy resource: ec2 region:us-east-1 c7n:0.8.21.2 2016-12-20 08:35:07,514: custodian.resources.ec2:INFO Filtered from 3 to 1 ec2 2016-12-20 08:35:07,514: custodian.policy:INFO policy: my-first-policy resource:ec2 has count:1 time:1.38 2016-12-20 08:35:07,515: custodian.actions:INFO Stop 1 of 1 instances 2016-12-20 08:35:08,188: custodian.policy:INFO policy: my-first-policy action: stop resources: 1 execution_time: 0.67 You should also find a new my-first-policy directory with a log and other files (subsequent runs will append to the log by default rather than overwriting it). Lastly, you should find the instance stopping or stopped in your AWS console. Congratulations, and welcome to Custodian! See our extended example of a policy\\'s structure tag compliance policy <policyStructure> {.interpreted-text role=\"ref\"}, or browse all of our use case recipes <usecases> {.interpreted-text role=\"ref\"}.","title":"Run your policy"},{"location":"aws/gettingstarted.rst/#a-2nd-example-policy","text":"First a role must be created with the appropriate permissions for custodian to act on the resources described in the policies yaml given as an example below. For convenience, an example policy is provided for this quick start guide. Customized AWS IAM policies will be necessary for your own custodian policies To implement the policy: Open the AWS console Navigate to IAM -> Policies Use the json option to copy the example policy as a new AWS IAM Policy Name the IAM policy as something recognizable and save it. Navigate to IAM -> Roles and create a role called CloudCustodian-QuickStart Assign the role the IAM policy created above. Now with the pre-requisite completed; you are ready continue and run custodian. A custodian policy file needs to be created in YAML format, as an example policies: - name: s3-cross-account description: | Checks S3 for buckets with cross-account access and removes the cross-account access. resource: s3 region: us-east-1 filters: - type: cross-account actions: - type: remove-statements statement_ids: matched - name: ec2-require-non-public-and-encrypted-volumes resource: aws.ec2 description: | Provision a lambda and cloud watch event target that looks at all new instances and terminates those with unencrypted volumes. mode: type: cloudtrail role: CloudCustodian-QuickStart events: - RunInstances filters: - type: ebs key: Encrypted value: false actions: - terminate - name: tag-compliance resource: aws.ec2 description: | Schedule a resource that does not meet tag compliance policies to be stopped in four days. filters: - State.Name: running - \"tag:Environment\": absent - \"tag:AppId\": absent - or: - \"tag:OwnerContact\": absent - \"tag:DeptID\": absent actions: - type: mark-for-op op: stop days: 4 Given that, you can run Cloud Custodian with # Validate the configuration (note this happens by default on run) custodian validate policy.yml # Dryrun on the policies (no actions executed) to see what resources # match each policy. custodian run --dryrun -s out policy.yml # Run the policy custodian run -s out policy.yml","title":"A 2nd Example Policy"},{"location":"aws/gettingstarted.rst/#monitor-aws-monitor-aws-cc","text":"You can generate CloudWatch metrics by specifying the --metrics flag and specifying aws : custodian run -s <output_directory> --metrics aws <policyfile>.yml You can also upload Cloud Custodian logs to CloudWatch logs: custodian run --log-group=/cloud-custodian/<dev-account>/<region> -s <output_directory> <policyfile>.yml And you can output logs and resource records to S3: custodian run -s s3://<my-bucket><my-prefix> <policyfile>.yml If Custodian is being run without Assume Roles, all output will be put into the same account. Custodian is built with the ability to be run from different accounts and leverage STS Role Assumption for cross-account access. Users can leverage the metrics that are being generated after each run by creating Custodian Dashboards in CloudWatch.","title":"Monitor AWS {#monitor-aws-cc}"},{"location":"aws/gettingstarted.rst/#troubleshooting-tinkering","text":"If you are not using the us-east-1 region, then you\\'ll need to specify that as well, either on the command line or in an environment variable: --region=us-west-1 AWS_DEFAULT_REGION=us-west-1","title":"Troubleshooting &amp; Tinkering"},{"location":"aws/lambda/","text":"Lambda Support {#lambda} Lambda provides for powerful realtime event based code execution in response to infrastructure and application behavior. A number of different Amazon services can be used as event sources. CloudWatch Events CloudWatch Events (CWE) is a general event bus for AWS infrastructure. Currently, it covers several major sources of information: CloudTrail API calls over a poll period on CloudTrail delivery, real-time instance status events, autoscale group notifications, and scheduled/periodic events. CloudTrail provides a very rich data source over the entire range of AWS services exposed via the audit trail that allows Custodian to define effective realtime policies against any AWS product. Additionally, for EC2 instances we can provide mandatory policy compliance - this means the non-compliant resources never became available. Cloud Custodian Integration Custodian provides for policy level execution against any CWE event stream. Each Custodian policy can be deployed as an independent Lambda function. The only difference between a Custodian policy that runs in Lambda and one that runs directly from the CLI in poll mode is the specification of the subscription of the events in the mode config block of the policy. Internally Custodian will reconstitute current state for all the resources in the event, execute the policy against them, match against the policy filters, and apply the policy actions to matching resources. CloudTrail API Calls Lambdas can receive CWE over CloudTrail API calls with delay of 90s at P99. policies: - name: ec2-tag-running resource: ec2 mode: type: cloudtrail events: - RunInstances actions: - type: mark tag: foo msg: bar Because the total AWS API surface area is so large most CloudTrail API event subscriptions need two additional fields: For CloudTrail events we need to reference the source API call. To work transparently with existing resource policies, we also need to specify how to extract the resource IDs from the event via JMESPath so that the resources can be queried. For very common API calls for policies, some shortcuts have been defined to allow for easier policy writing as for the RunInstances API call above, which expands to: events: - source: ec2.amazonaws.com event: RunInstances ids: \"responseElements.instancesSet.items[].instanceId\" EC2 Instance State Events Lambdas can receive EC2 instance state events in real time (seconds delay). policies: - name: ec2-require-encrypted-volumes resource: ec2 mode: type: ec2-instance-state events: - pending filters: - type: ebs key: Encrypted value: False actions: - mark - terminate Periodic Function We support both rate per unit time and cron expressions, per scheduler syntax . When using --assume on the custodian run cli command, the specified role is also considered as the execution role to be attached to lambda function that gets deployed. In such scenario it is not required to specify the role attribute in the config block for mode. However, if you are not using the --assume option, then it is required to add role in the config-block of mode. When specifying role {account_id} is runtime substituted so a policy can be used across accounts. policies: - name: s3-bucket-check resource: s3 mode: type: periodic schedule: \"rate(1 day)\" role: arn:aws:iam::{account_id}:role/some-role Event Pattern Filtering Cloud Watch Events also support content/pattern filtering, see https://docs.aws.amazon.com/eventbridge/latest/userguide/content-filtering-with-event-patterns.html https://aws.amazon.com/blogs/compute/reducing-custom-code-by-using-advanced-rules-in-amazon-eventbridge/ In the context of a custodian policy you can define a \\'pattern\\' key under mode, the pattern will be merged with the custodian generated default event pattern. If the pattern filtering does not match the event, the custodian policy lambda will not be invoked/executed. In the following example policy, an additional event pattern is supplied that ignores any create subnet call by the iam user named [deputy]{.title-ref}. policies: - name: subnet-detect resource: aws.subnet mode: type: cloudtrail role: CustodianDemoRole events: - source: ec2.amazonaws.com event: CreateSubnet ids: responseElements.subnet.subnetId pattern: detail: userIdentity: userName: [{'anything-but': 'deputy'}] Config Rules AWS Config rules allow you to invoke logic in response to configuration changes in your AWS environment, and Cloud Custodian is the easiest way to write and provision Config rules. Delay here is typically 1-15m (though the SLA on tag-only changes is a bit higher). In this section we\\'ll look at how we would deploy the quickstart <quickstart> {.interpreted-text role=\"ref\"} example using Config. Before you proceed, make sure you\\'ve removed the Custodian tag from any EC2 instance left over from the quickstart. First, modify custodian.yml to specify a mode type of config-rule . You\\'ll also need the ARN of an IAM role to assume when running the Lambda that Custodian is going to install for you. Sensible policies to add to that role would be AWSLambdaBasicExecutionRole and AWSConfigRulesExecutionRole , on top of any permissions your lambda is going to need to perform the actions you want it to perform. policies: - name: my-first-policy mode: type: config-rule role: arn:aws:iam::123456789012:role/some-role resource: ec2 filters: - \"tag:Custodian\": present actions: - stop Then make sure that you\\'ve set up AWS Config. If you go to the AWS Config console and see the welcome screen instead of the dashboard, go through the setup procedure first . Now deploy the policy: custodian run -s . custodian.yml That should give you log output like this: 2017-01-25 05:43:01,539: custodian.policy:INFO Provisioning policy lambda my-first-policy 2017-01-25 05:43:04,683: custodian.lambda:INFO Publishing custodian policy lambda function custodian-my-first-policy Go check the AWS console to see the Lambda as well as the Config rule that Custodian created. The Config rule should be listed as \\\"Compliant\\\" or \\\"No results reported\\\" (if not, be sure you removed the Custodian tag from any instance left over from the quickstart). Now for the fun part! With your new policy installed, go ahead and create an EC2 instance with a Custodian tag (any non-empty value), and wait (events from Config are effectively delayed 15m up to 6hrs on tag changes). If all goes well, you should eventually see that your new custom Config rule notices the EC2 instance with the Custodian tag, and stops it according to your policy. Congratulations! You have now installed your policy to run under Config rather than from your command line. Lambda Configuration Custodian lambdas support configuring all lambda options via keys on the lambda mode in the YAML. See AWS\\' AWS Lambda Function Configuration page for the full list of configuration options available on a Lambda. Refer to aws_modes {.interpreted-text role=\"ref\"} for detailed explanation of the different type values and the corresponding additional configuration options each requires. Here is an example YAML fragment that shows the options you are most likely to want or need to configure on a lambda: mode: type: cloudtrail events: - CreateBucket ##### ROLE ##### # Specify the ARN role as either name or full ARN. This shows # us running the lambda with the IAM role named Custodian. # Specifying role by name: role: Custodian # Or specifying using a full ARN # role: arn:aws:iam::123456789012:role/Custodian ##### TAGS ##### # Specify the tags to assign to this Lambda. We are setting a # tag named \"Application\" with a value of \"Custodian\", and a # \"CreatedBy\" tag with a value of \"CloudCustodian\". tags: Application: Custodian CreatedBy: CloudCustodian Execution Options When running in Lambda you may want policy execution to run using particular options corresponding to those passed to the custodian CLI. Execution in lambda comes with a default set of configuration which is different from the defaults you might set when running through the command line: Metrics are enabled Output dir is set to a random /tmp/ directory Caching of AWS resource state is disabled Account ID is automatically set with info from sts Region is automatically set to the region of the lambda (using the AWS_DEFAULT_REGION environment variable in lambda) When you want to override these settings, you must set \\'execution-options\\' with one of the following keys: region cache profile account_id assume_role log_group metrics output_dir cache_period dryrun One useful thing we can do with these options is to make a policy execute in a different account using assume_role. A policy definition for this looks like: policies: - name: my-first-policy-cross-account mode: type: periodic schedule: \"rate(1 day)\" role: arn:aws:iam::123456789012:role/lambda-role execution-options: assume_role: arn:aws:iam::210987654321:role/target-role metrics: aws resource: ec2 filters: - \"tag:Custodian\": present actions: - stop A couple of things to note here: Metrics are pushed using the assumed role which may or may not be desired The mode must be periodic as there are restrictions on where policy executions can run according to the mode: Config : May run in a different region but not cross-account Event : Only run in the same region and account Periodic : May run in a different region and different account","title":"Lambda"},{"location":"aws/lambda/#lambda-support-lambda","text":"Lambda provides for powerful realtime event based code execution in response to infrastructure and application behavior. A number of different Amazon services can be used as event sources.","title":"Lambda Support {#lambda}"},{"location":"aws/lambda/#cloudwatch-events","text":"CloudWatch Events (CWE) is a general event bus for AWS infrastructure. Currently, it covers several major sources of information: CloudTrail API calls over a poll period on CloudTrail delivery, real-time instance status events, autoscale group notifications, and scheduled/periodic events. CloudTrail provides a very rich data source over the entire range of AWS services exposed via the audit trail that allows Custodian to define effective realtime policies against any AWS product. Additionally, for EC2 instances we can provide mandatory policy compliance - this means the non-compliant resources never became available.","title":"CloudWatch Events"},{"location":"aws/lambda/#cloud-custodian-integration","text":"Custodian provides for policy level execution against any CWE event stream. Each Custodian policy can be deployed as an independent Lambda function. The only difference between a Custodian policy that runs in Lambda and one that runs directly from the CLI in poll mode is the specification of the subscription of the events in the mode config block of the policy. Internally Custodian will reconstitute current state for all the resources in the event, execute the policy against them, match against the policy filters, and apply the policy actions to matching resources.","title":"Cloud Custodian Integration"},{"location":"aws/lambda/#cloudtrail-api-calls","text":"Lambdas can receive CWE over CloudTrail API calls with delay of 90s at P99. policies: - name: ec2-tag-running resource: ec2 mode: type: cloudtrail events: - RunInstances actions: - type: mark tag: foo msg: bar Because the total AWS API surface area is so large most CloudTrail API event subscriptions need two additional fields: For CloudTrail events we need to reference the source API call. To work transparently with existing resource policies, we also need to specify how to extract the resource IDs from the event via JMESPath so that the resources can be queried. For very common API calls for policies, some shortcuts have been defined to allow for easier policy writing as for the RunInstances API call above, which expands to: events: - source: ec2.amazonaws.com event: RunInstances ids: \"responseElements.instancesSet.items[].instanceId\"","title":"CloudTrail API Calls"},{"location":"aws/lambda/#ec2-instance-state-events","text":"Lambdas can receive EC2 instance state events in real time (seconds delay). policies: - name: ec2-require-encrypted-volumes resource: ec2 mode: type: ec2-instance-state events: - pending filters: - type: ebs key: Encrypted value: False actions: - mark - terminate","title":"EC2 Instance State Events"},{"location":"aws/lambda/#periodic-function","text":"We support both rate per unit time and cron expressions, per scheduler syntax . When using --assume on the custodian run cli command, the specified role is also considered as the execution role to be attached to lambda function that gets deployed. In such scenario it is not required to specify the role attribute in the config block for mode. However, if you are not using the --assume option, then it is required to add role in the config-block of mode. When specifying role {account_id} is runtime substituted so a policy can be used across accounts. policies: - name: s3-bucket-check resource: s3 mode: type: periodic schedule: \"rate(1 day)\" role: arn:aws:iam::{account_id}:role/some-role","title":"Periodic Function"},{"location":"aws/lambda/#event-pattern-filtering","text":"Cloud Watch Events also support content/pattern filtering, see https://docs.aws.amazon.com/eventbridge/latest/userguide/content-filtering-with-event-patterns.html https://aws.amazon.com/blogs/compute/reducing-custom-code-by-using-advanced-rules-in-amazon-eventbridge/ In the context of a custodian policy you can define a \\'pattern\\' key under mode, the pattern will be merged with the custodian generated default event pattern. If the pattern filtering does not match the event, the custodian policy lambda will not be invoked/executed. In the following example policy, an additional event pattern is supplied that ignores any create subnet call by the iam user named [deputy]{.title-ref}. policies: - name: subnet-detect resource: aws.subnet mode: type: cloudtrail role: CustodianDemoRole events: - source: ec2.amazonaws.com event: CreateSubnet ids: responseElements.subnet.subnetId pattern: detail: userIdentity: userName: [{'anything-but': 'deputy'}]","title":"Event Pattern Filtering"},{"location":"aws/lambda/#config-rules","text":"AWS Config rules allow you to invoke logic in response to configuration changes in your AWS environment, and Cloud Custodian is the easiest way to write and provision Config rules. Delay here is typically 1-15m (though the SLA on tag-only changes is a bit higher). In this section we\\'ll look at how we would deploy the quickstart <quickstart> {.interpreted-text role=\"ref\"} example using Config. Before you proceed, make sure you\\'ve removed the Custodian tag from any EC2 instance left over from the quickstart. First, modify custodian.yml to specify a mode type of config-rule . You\\'ll also need the ARN of an IAM role to assume when running the Lambda that Custodian is going to install for you. Sensible policies to add to that role would be AWSLambdaBasicExecutionRole and AWSConfigRulesExecutionRole , on top of any permissions your lambda is going to need to perform the actions you want it to perform. policies: - name: my-first-policy mode: type: config-rule role: arn:aws:iam::123456789012:role/some-role resource: ec2 filters: - \"tag:Custodian\": present actions: - stop Then make sure that you\\'ve set up AWS Config. If you go to the AWS Config console and see the welcome screen instead of the dashboard, go through the setup procedure first . Now deploy the policy: custodian run -s . custodian.yml That should give you log output like this: 2017-01-25 05:43:01,539: custodian.policy:INFO Provisioning policy lambda my-first-policy 2017-01-25 05:43:04,683: custodian.lambda:INFO Publishing custodian policy lambda function custodian-my-first-policy Go check the AWS console to see the Lambda as well as the Config rule that Custodian created. The Config rule should be listed as \\\"Compliant\\\" or \\\"No results reported\\\" (if not, be sure you removed the Custodian tag from any instance left over from the quickstart). Now for the fun part! With your new policy installed, go ahead and create an EC2 instance with a Custodian tag (any non-empty value), and wait (events from Config are effectively delayed 15m up to 6hrs on tag changes). If all goes well, you should eventually see that your new custom Config rule notices the EC2 instance with the Custodian tag, and stops it according to your policy. Congratulations! You have now installed your policy to run under Config rather than from your command line.","title":"Config Rules"},{"location":"aws/lambda/#lambda-configuration","text":"Custodian lambdas support configuring all lambda options via keys on the lambda mode in the YAML. See AWS\\' AWS Lambda Function Configuration page for the full list of configuration options available on a Lambda. Refer to aws_modes {.interpreted-text role=\"ref\"} for detailed explanation of the different type values and the corresponding additional configuration options each requires. Here is an example YAML fragment that shows the options you are most likely to want or need to configure on a lambda: mode: type: cloudtrail events: - CreateBucket ##### ROLE ##### # Specify the ARN role as either name or full ARN. This shows # us running the lambda with the IAM role named Custodian. # Specifying role by name: role: Custodian # Or specifying using a full ARN # role: arn:aws:iam::123456789012:role/Custodian ##### TAGS ##### # Specify the tags to assign to this Lambda. We are setting a # tag named \"Application\" with a value of \"Custodian\", and a # \"CreatedBy\" tag with a value of \"CloudCustodian\". tags: Application: Custodian CreatedBy: CloudCustodian","title":"Lambda Configuration"},{"location":"aws/lambda/#execution-options","text":"When running in Lambda you may want policy execution to run using particular options corresponding to those passed to the custodian CLI. Execution in lambda comes with a default set of configuration which is different from the defaults you might set when running through the command line: Metrics are enabled Output dir is set to a random /tmp/ directory Caching of AWS resource state is disabled Account ID is automatically set with info from sts Region is automatically set to the region of the lambda (using the AWS_DEFAULT_REGION environment variable in lambda) When you want to override these settings, you must set \\'execution-options\\' with one of the following keys: region cache profile account_id assume_role log_group metrics output_dir cache_period dryrun One useful thing we can do with these options is to make a policy execute in a different account using assume_role. A policy definition for this looks like: policies: - name: my-first-policy-cross-account mode: type: periodic schedule: \"rate(1 day)\" role: arn:aws:iam::123456789012:role/lambda-role execution-options: assume_role: arn:aws:iam::210987654321:role/target-role metrics: aws resource: ec2 filters: - \"tag:Custodian\": present actions: - stop A couple of things to note here: Metrics are pushed using the assumed role which may or may not be desired The mode must be periodic as there are restrictions on where policy executions can run according to the mode: Config : May run in a different region but not cross-account Event : Only run in the same region and account Periodic : May run in a different region and different account","title":"Execution Options"},{"location":"aws/lambda.rst/","text":"Lambda Support {#lambda} Lambda provides for powerful realtime event based code execution in response to infrastructure and application behavior. A number of different Amazon services can be used as event sources. CloudWatch Events CloudWatch Events (CWE) is a general event bus for AWS infrastructure. Currently, it covers several major sources of information: CloudTrail API calls over a poll period on CloudTrail delivery, real-time instance status events, autoscale group notifications, and scheduled/periodic events. CloudTrail provides a very rich data source over the entire range of AWS services exposed via the audit trail that allows Custodian to define effective realtime policies against any AWS product. Additionally, for EC2 instances we can provide mandatory policy compliance - this means the non-compliant resources never became available. Cloud Custodian Integration Custodian provides for policy level execution against any CWE event stream. Each Custodian policy can be deployed as an independent Lambda function. The only difference between a Custodian policy that runs in Lambda and one that runs directly from the CLI in poll mode is the specification of the subscription of the events in the mode config block of the policy. Internally Custodian will reconstitute current state for all the resources in the event, execute the policy against them, match against the policy filters, and apply the policy actions to matching resources. CloudTrail API Calls Lambdas can receive CWE over CloudTrail API calls with delay of 90s at P99. policies: - name: ec2-tag-running resource: ec2 mode: type: cloudtrail events: - RunInstances actions: - type: mark tag: foo msg: bar Because the total AWS API surface area is so large most CloudTrail API event subscriptions need two additional fields: For CloudTrail events we need to reference the source API call. To work transparently with existing resource policies, we also need to specify how to extract the resource IDs from the event via JMESPath so that the resources can be queried. For very common API calls for policies, some shortcuts have been defined to allow for easier policy writing as for the RunInstances API call above, which expands to: events: - source: ec2.amazonaws.com event: RunInstances ids: \"responseElements.instancesSet.items[].instanceId\" EC2 Instance State Events Lambdas can receive EC2 instance state events in real time (seconds delay). policies: - name: ec2-require-encrypted-volumes resource: ec2 mode: type: ec2-instance-state events: - pending filters: - type: ebs key: Encrypted value: False actions: - mark - terminate Periodic Function We support both rate per unit time and cron expressions, per scheduler syntax . When using --assume on the custodian run cli command, the specified role is also considered as the execution role to be attached to lambda function that gets deployed. In such scenario it is not required to specify the role attribute in the config block for mode. However, if you are not using the --assume option, then it is required to add role in the config-block of mode. When specifying role {account_id} is runtime substituted so a policy can be used across accounts. policies: - name: s3-bucket-check resource: s3 mode: type: periodic schedule: \"rate(1 day)\" role: arn:aws:iam::{account_id}:role/some-role Event Pattern Filtering Cloud Watch Events also support content/pattern filtering, see https://docs.aws.amazon.com/eventbridge/latest/userguide/content-filtering-with-event-patterns.html https://aws.amazon.com/blogs/compute/reducing-custom-code-by-using-advanced-rules-in-amazon-eventbridge/ In the context of a custodian policy you can define a \\'pattern\\' key under mode, the pattern will be merged with the custodian generated default event pattern. If the pattern filtering does not match the event, the custodian policy lambda will not be invoked/executed. In the following example policy, an additional event pattern is supplied that ignores any create subnet call by the iam user named [deputy]{.title-ref}. policies: - name: subnet-detect resource: aws.subnet mode: type: cloudtrail role: CustodianDemoRole events: - source: ec2.amazonaws.com event: CreateSubnet ids: responseElements.subnet.subnetId pattern: detail: userIdentity: userName: [{'anything-but': 'deputy'}] Config Rules AWS Config rules allow you to invoke logic in response to configuration changes in your AWS environment, and Cloud Custodian is the easiest way to write and provision Config rules. Delay here is typically 1-15m (though the SLA on tag-only changes is a bit higher). In this section we\\'ll look at how we would deploy the quickstart <quickstart> {.interpreted-text role=\"ref\"} example using Config. Before you proceed, make sure you\\'ve removed the Custodian tag from any EC2 instance left over from the quickstart. First, modify custodian.yml to specify a mode type of config-rule . You\\'ll also need the ARN of an IAM role to assume when running the Lambda that Custodian is going to install for you. Sensible policies to add to that role would be AWSLambdaBasicExecutionRole and AWSConfigRulesExecutionRole , on top of any permissions your lambda is going to need to perform the actions you want it to perform. policies: - name: my-first-policy mode: type: config-rule role: arn:aws:iam::123456789012:role/some-role resource: ec2 filters: - \"tag:Custodian\": present actions: - stop Then make sure that you\\'ve set up AWS Config. If you go to the AWS Config console and see the welcome screen instead of the dashboard, go through the setup procedure first . Now deploy the policy: custodian run -s . custodian.yml That should give you log output like this: 2017-01-25 05:43:01,539: custodian.policy:INFO Provisioning policy lambda my-first-policy 2017-01-25 05:43:04,683: custodian.lambda:INFO Publishing custodian policy lambda function custodian-my-first-policy Go check the AWS console to see the Lambda as well as the Config rule that Custodian created. The Config rule should be listed as \\\"Compliant\\\" or \\\"No results reported\\\" (if not, be sure you removed the Custodian tag from any instance left over from the quickstart). Now for the fun part! With your new policy installed, go ahead and create an EC2 instance with a Custodian tag (any non-empty value), and wait (events from Config are effectively delayed 15m up to 6hrs on tag changes). If all goes well, you should eventually see that your new custom Config rule notices the EC2 instance with the Custodian tag, and stops it according to your policy. Congratulations! You have now installed your policy to run under Config rather than from your command line. Lambda Configuration Custodian lambdas support configuring all lambda options via keys on the lambda mode in the YAML. See AWS\\' AWS Lambda Function Configuration page for the full list of configuration options available on a Lambda. Refer to aws_modes {.interpreted-text role=\"ref\"} for detailed explanation of the different type values and the corresponding additional configuration options each requires. Here is an example YAML fragment that shows the options you are most likely to want or need to configure on a lambda: mode: type: cloudtrail events: - CreateBucket ##### ROLE ##### # Specify the ARN role as either name or full ARN. This shows # us running the lambda with the IAM role named Custodian. # Specifying role by name: role: Custodian # Or specifying using a full ARN # role: arn:aws:iam::123456789012:role/Custodian ##### TAGS ##### # Specify the tags to assign to this Lambda. We are setting a # tag named \"Application\" with a value of \"Custodian\", and a # \"CreatedBy\" tag with a value of \"CloudCustodian\". tags: Application: Custodian CreatedBy: CloudCustodian Execution Options When running in Lambda you may want policy execution to run using particular options corresponding to those passed to the custodian CLI. Execution in lambda comes with a default set of configuration which is different from the defaults you might set when running through the command line: Metrics are enabled Output dir is set to a random /tmp/ directory Caching of AWS resource state is disabled Account ID is automatically set with info from sts Region is automatically set to the region of the lambda (using the AWS_DEFAULT_REGION environment variable in lambda) When you want to override these settings, you must set \\'execution-options\\' with one of the following keys: region cache profile account_id assume_role log_group metrics output_dir cache_period dryrun One useful thing we can do with these options is to make a policy execute in a different account using assume_role. A policy definition for this looks like: policies: - name: my-first-policy-cross-account mode: type: periodic schedule: \"rate(1 day)\" role: arn:aws:iam::123456789012:role/lambda-role execution-options: assume_role: arn:aws:iam::210987654321:role/target-role metrics: aws resource: ec2 filters: - \"tag:Custodian\": present actions: - stop A couple of things to note here: Metrics are pushed using the assumed role which may or may not be desired The mode must be periodic as there are restrictions on where policy executions can run according to the mode: Config : May run in a different region but not cross-account Event : Only run in the same region and account Periodic : May run in a different region and different account","title":"Lambda.rst"},{"location":"aws/lambda.rst/#lambda-support-lambda","text":"Lambda provides for powerful realtime event based code execution in response to infrastructure and application behavior. A number of different Amazon services can be used as event sources.","title":"Lambda Support {#lambda}"},{"location":"aws/lambda.rst/#cloudwatch-events","text":"CloudWatch Events (CWE) is a general event bus for AWS infrastructure. Currently, it covers several major sources of information: CloudTrail API calls over a poll period on CloudTrail delivery, real-time instance status events, autoscale group notifications, and scheduled/periodic events. CloudTrail provides a very rich data source over the entire range of AWS services exposed via the audit trail that allows Custodian to define effective realtime policies against any AWS product. Additionally, for EC2 instances we can provide mandatory policy compliance - this means the non-compliant resources never became available.","title":"CloudWatch Events"},{"location":"aws/lambda.rst/#cloud-custodian-integration","text":"Custodian provides for policy level execution against any CWE event stream. Each Custodian policy can be deployed as an independent Lambda function. The only difference between a Custodian policy that runs in Lambda and one that runs directly from the CLI in poll mode is the specification of the subscription of the events in the mode config block of the policy. Internally Custodian will reconstitute current state for all the resources in the event, execute the policy against them, match against the policy filters, and apply the policy actions to matching resources.","title":"Cloud Custodian Integration"},{"location":"aws/lambda.rst/#cloudtrail-api-calls","text":"Lambdas can receive CWE over CloudTrail API calls with delay of 90s at P99. policies: - name: ec2-tag-running resource: ec2 mode: type: cloudtrail events: - RunInstances actions: - type: mark tag: foo msg: bar Because the total AWS API surface area is so large most CloudTrail API event subscriptions need two additional fields: For CloudTrail events we need to reference the source API call. To work transparently with existing resource policies, we also need to specify how to extract the resource IDs from the event via JMESPath so that the resources can be queried. For very common API calls for policies, some shortcuts have been defined to allow for easier policy writing as for the RunInstances API call above, which expands to: events: - source: ec2.amazonaws.com event: RunInstances ids: \"responseElements.instancesSet.items[].instanceId\"","title":"CloudTrail API Calls"},{"location":"aws/lambda.rst/#ec2-instance-state-events","text":"Lambdas can receive EC2 instance state events in real time (seconds delay). policies: - name: ec2-require-encrypted-volumes resource: ec2 mode: type: ec2-instance-state events: - pending filters: - type: ebs key: Encrypted value: False actions: - mark - terminate","title":"EC2 Instance State Events"},{"location":"aws/lambda.rst/#periodic-function","text":"We support both rate per unit time and cron expressions, per scheduler syntax . When using --assume on the custodian run cli command, the specified role is also considered as the execution role to be attached to lambda function that gets deployed. In such scenario it is not required to specify the role attribute in the config block for mode. However, if you are not using the --assume option, then it is required to add role in the config-block of mode. When specifying role {account_id} is runtime substituted so a policy can be used across accounts. policies: - name: s3-bucket-check resource: s3 mode: type: periodic schedule: \"rate(1 day)\" role: arn:aws:iam::{account_id}:role/some-role","title":"Periodic Function"},{"location":"aws/lambda.rst/#event-pattern-filtering","text":"Cloud Watch Events also support content/pattern filtering, see https://docs.aws.amazon.com/eventbridge/latest/userguide/content-filtering-with-event-patterns.html https://aws.amazon.com/blogs/compute/reducing-custom-code-by-using-advanced-rules-in-amazon-eventbridge/ In the context of a custodian policy you can define a \\'pattern\\' key under mode, the pattern will be merged with the custodian generated default event pattern. If the pattern filtering does not match the event, the custodian policy lambda will not be invoked/executed. In the following example policy, an additional event pattern is supplied that ignores any create subnet call by the iam user named [deputy]{.title-ref}. policies: - name: subnet-detect resource: aws.subnet mode: type: cloudtrail role: CustodianDemoRole events: - source: ec2.amazonaws.com event: CreateSubnet ids: responseElements.subnet.subnetId pattern: detail: userIdentity: userName: [{'anything-but': 'deputy'}]","title":"Event Pattern Filtering"},{"location":"aws/lambda.rst/#config-rules","text":"AWS Config rules allow you to invoke logic in response to configuration changes in your AWS environment, and Cloud Custodian is the easiest way to write and provision Config rules. Delay here is typically 1-15m (though the SLA on tag-only changes is a bit higher). In this section we\\'ll look at how we would deploy the quickstart <quickstart> {.interpreted-text role=\"ref\"} example using Config. Before you proceed, make sure you\\'ve removed the Custodian tag from any EC2 instance left over from the quickstart. First, modify custodian.yml to specify a mode type of config-rule . You\\'ll also need the ARN of an IAM role to assume when running the Lambda that Custodian is going to install for you. Sensible policies to add to that role would be AWSLambdaBasicExecutionRole and AWSConfigRulesExecutionRole , on top of any permissions your lambda is going to need to perform the actions you want it to perform. policies: - name: my-first-policy mode: type: config-rule role: arn:aws:iam::123456789012:role/some-role resource: ec2 filters: - \"tag:Custodian\": present actions: - stop Then make sure that you\\'ve set up AWS Config. If you go to the AWS Config console and see the welcome screen instead of the dashboard, go through the setup procedure first . Now deploy the policy: custodian run -s . custodian.yml That should give you log output like this: 2017-01-25 05:43:01,539: custodian.policy:INFO Provisioning policy lambda my-first-policy 2017-01-25 05:43:04,683: custodian.lambda:INFO Publishing custodian policy lambda function custodian-my-first-policy Go check the AWS console to see the Lambda as well as the Config rule that Custodian created. The Config rule should be listed as \\\"Compliant\\\" or \\\"No results reported\\\" (if not, be sure you removed the Custodian tag from any instance left over from the quickstart). Now for the fun part! With your new policy installed, go ahead and create an EC2 instance with a Custodian tag (any non-empty value), and wait (events from Config are effectively delayed 15m up to 6hrs on tag changes). If all goes well, you should eventually see that your new custom Config rule notices the EC2 instance with the Custodian tag, and stops it according to your policy. Congratulations! You have now installed your policy to run under Config rather than from your command line.","title":"Config Rules"},{"location":"aws/lambda.rst/#lambda-configuration","text":"Custodian lambdas support configuring all lambda options via keys on the lambda mode in the YAML. See AWS\\' AWS Lambda Function Configuration page for the full list of configuration options available on a Lambda. Refer to aws_modes {.interpreted-text role=\"ref\"} for detailed explanation of the different type values and the corresponding additional configuration options each requires. Here is an example YAML fragment that shows the options you are most likely to want or need to configure on a lambda: mode: type: cloudtrail events: - CreateBucket ##### ROLE ##### # Specify the ARN role as either name or full ARN. This shows # us running the lambda with the IAM role named Custodian. # Specifying role by name: role: Custodian # Or specifying using a full ARN # role: arn:aws:iam::123456789012:role/Custodian ##### TAGS ##### # Specify the tags to assign to this Lambda. We are setting a # tag named \"Application\" with a value of \"Custodian\", and a # \"CreatedBy\" tag with a value of \"CloudCustodian\". tags: Application: Custodian CreatedBy: CloudCustodian","title":"Lambda Configuration"},{"location":"aws/lambda.rst/#execution-options","text":"When running in Lambda you may want policy execution to run using particular options corresponding to those passed to the custodian CLI. Execution in lambda comes with a default set of configuration which is different from the defaults you might set when running through the command line: Metrics are enabled Output dir is set to a random /tmp/ directory Caching of AWS resource state is disabled Account ID is automatically set with info from sts Region is automatically set to the region of the lambda (using the AWS_DEFAULT_REGION environment variable in lambda) When you want to override these settings, you must set \\'execution-options\\' with one of the following keys: region cache profile account_id assume_role log_group metrics output_dir cache_period dryrun One useful thing we can do with these options is to make a policy execute in a different account using assume_role. A policy definition for this looks like: policies: - name: my-first-policy-cross-account mode: type: periodic schedule: \"rate(1 day)\" role: arn:aws:iam::123456789012:role/lambda-role execution-options: assume_role: arn:aws:iam::210987654321:role/target-role metrics: aws resource: ec2 filters: - \"tag:Custodian\": present actions: - stop A couple of things to note here: Metrics are pushed using the assumed role which may or may not be desired The mode must be periodic as there are restrictions on where policy executions can run according to the mode: Config : May run in a different region but not cross-account Event : Only run in the same region and account Periodic : May run in a different region and different account","title":"Execution Options"},{"location":"aws/usage/","text":"Monitoring your environment {#usage} Cloud Custodian generates a consistent set of outputs for any given policy. Custodian automatically generates per policy outputs with resources metrics and archives serialization for all resources that match against a policy\\'s filters. TODO: figure out where \\\"Custodian Dashboards in CloudWatch\\\" page goes -- here? its own page? part of Getting Started? Metrics By default Cloud Custodian generates CloudWatch metrics on each policy for the number of resources that matched the set of filters, the time to retrieve and filter the resources, and the time to execute actions. In practice this number of matching resources allows for generating enough metrics to put together useful dashboards over policies in CloudWatch custom dashboards. Additionally some filters and actions may generate their own metrics. In order to enable metrics output, the boolean metrics flag needs to be specified when running Cloud Custodian: custodian run -s <output_directory> --metrics aws <policyfile>.yml You can also consolidate metrics into a single account by specifying the master location in the cli. Note that this is only applicable when using the --assume option in the cli or when using c7n-org. By default, metrics will be sent to the same account that is being executed against: custodian run -s <output_directory> --metrics aws://master Additionally, to use a different namespace other than the default CloudMaid , you can add the following query parameter to the metrics flag: custodian run -s <output_directory> --metrics aws://?namespace=foo This will create a new namespace, foo in CloudWatch Metrics. You can also combine these two options to emit metrics into a custom namespace in a central account: custodian run -s <output_directory> --metrics aws://master?namespace=foo Finally, to send metrics to a specific region, use the region query parameter to specify a region: custodian run -s <output_directory> --metrics aws://?region=us-west-2 When running the metrics in a centralized account or when centralizing to a specific region, additional account and region dimensions will be included. CloudWatch Logs Custodian can optionally upload its logs in realtime to CloudWatch logs, if a log group is specified. Each policy\\'s log output is generated as a separate stream. Usage example: custodian run --log-group=/cloud-custodian/<dev-account>/<region> <policyfile>.yml If enabled, it is recommended to set a log subscription on the group to be informed of an operations issue. If S3 output is also enabled, then it is also recommended to set a log group archival policy and to use the S3 logs as permanent/audit archive. You can also aggregate your logs within a single region or account using the same url formatting as is used for metrics. To send your logs to a region in the master account use: custodian run --log-group=aws://master/<log-group-name>?region=<region> <policyfile>.yml This will set up a stream for every region/account you run custodian against within the specified log group. The default log stream format looks like this: account_id/region/policy_name If you want to override this then you can pass the the log stream parameter like this: custodian run --log-group=\\\"aws://master/\\<log-group-name>?region=\\<region>&stream=custodian_{region}_{account}_{policy} \\<policyfile>.yml\\\" it currently accepts these variables: : {account}: the account where the check was executed. {region}: the region where the check was executed. {policy}: the name of the policy that was executed. S3 Logs & Records Custodian will output its logs and structured resource records in JSON format to S3, along with its log files for archival purposes. The S3 bucket and prefix can be specified via parameters: custodian run --output-dir s3://<my-bucket>/<my-prefix> <policyfile>.yml Reports CSV or text-based reports can be generated with the report subcommand. Reporting is used to list information gathered during previous calls to the run subcommand. If your goal is to find out what resources match on a policy use run along with the --dryrun option.","title":"Usage"},{"location":"aws/usage/#monitoring-your-environment-usage","text":"Cloud Custodian generates a consistent set of outputs for any given policy. Custodian automatically generates per policy outputs with resources metrics and archives serialization for all resources that match against a policy\\'s filters. TODO: figure out where \\\"Custodian Dashboards in CloudWatch\\\" page goes -- here? its own page? part of Getting Started?","title":"Monitoring your environment {#usage}"},{"location":"aws/usage/#metrics","text":"By default Cloud Custodian generates CloudWatch metrics on each policy for the number of resources that matched the set of filters, the time to retrieve and filter the resources, and the time to execute actions. In practice this number of matching resources allows for generating enough metrics to put together useful dashboards over policies in CloudWatch custom dashboards. Additionally some filters and actions may generate their own metrics. In order to enable metrics output, the boolean metrics flag needs to be specified when running Cloud Custodian: custodian run -s <output_directory> --metrics aws <policyfile>.yml You can also consolidate metrics into a single account by specifying the master location in the cli. Note that this is only applicable when using the --assume option in the cli or when using c7n-org. By default, metrics will be sent to the same account that is being executed against: custodian run -s <output_directory> --metrics aws://master Additionally, to use a different namespace other than the default CloudMaid , you can add the following query parameter to the metrics flag: custodian run -s <output_directory> --metrics aws://?namespace=foo This will create a new namespace, foo in CloudWatch Metrics. You can also combine these two options to emit metrics into a custom namespace in a central account: custodian run -s <output_directory> --metrics aws://master?namespace=foo Finally, to send metrics to a specific region, use the region query parameter to specify a region: custodian run -s <output_directory> --metrics aws://?region=us-west-2 When running the metrics in a centralized account or when centralizing to a specific region, additional account and region dimensions will be included.","title":"Metrics"},{"location":"aws/usage/#cloudwatch-logs","text":"Custodian can optionally upload its logs in realtime to CloudWatch logs, if a log group is specified. Each policy\\'s log output is generated as a separate stream. Usage example: custodian run --log-group=/cloud-custodian/<dev-account>/<region> <policyfile>.yml If enabled, it is recommended to set a log subscription on the group to be informed of an operations issue. If S3 output is also enabled, then it is also recommended to set a log group archival policy and to use the S3 logs as permanent/audit archive. You can also aggregate your logs within a single region or account using the same url formatting as is used for metrics. To send your logs to a region in the master account use: custodian run --log-group=aws://master/<log-group-name>?region=<region> <policyfile>.yml This will set up a stream for every region/account you run custodian against within the specified log group. The default log stream format looks like this: account_id/region/policy_name If you want to override this then you can pass the the log stream parameter like this: custodian run --log-group=\\\"aws://master/\\<log-group-name>?region=\\<region>&stream=custodian_{region}_{account}_{policy} \\<policyfile>.yml\\\" it currently accepts these variables: : {account}: the account where the check was executed. {region}: the region where the check was executed. {policy}: the name of the policy that was executed.","title":"CloudWatch Logs"},{"location":"aws/usage/#s3-logs-records","text":"Custodian will output its logs and structured resource records in JSON format to S3, along with its log files for archival purposes. The S3 bucket and prefix can be specified via parameters: custodian run --output-dir s3://<my-bucket>/<my-prefix> <policyfile>.yml","title":"S3 Logs &amp; Records"},{"location":"aws/usage/#reports","text":"CSV or text-based reports can be generated with the report subcommand. Reporting is used to list information gathered during previous calls to the run subcommand. If your goal is to find out what resources match on a policy use run along with the --dryrun option.","title":"Reports"},{"location":"aws/usage.rst/","text":"Monitoring your environment {#usage} Cloud Custodian generates a consistent set of outputs for any given policy. Custodian automatically generates per policy outputs with resources metrics and archives serialization for all resources that match against a policy\\'s filters. TODO: figure out where \\\"Custodian Dashboards in CloudWatch\\\" page goes -- here? its own page? part of Getting Started? Metrics By default Cloud Custodian generates CloudWatch metrics on each policy for the number of resources that matched the set of filters, the time to retrieve and filter the resources, and the time to execute actions. In practice this number of matching resources allows for generating enough metrics to put together useful dashboards over policies in CloudWatch custom dashboards. Additionally some filters and actions may generate their own metrics. In order to enable metrics output, the boolean metrics flag needs to be specified when running Cloud Custodian: custodian run -s <output_directory> --metrics aws <policyfile>.yml You can also consolidate metrics into a single account by specifying the master location in the cli. Note that this is only applicable when using the --assume option in the cli or when using c7n-org. By default, metrics will be sent to the same account that is being executed against: custodian run -s <output_directory> --metrics aws://master Additionally, to use a different namespace other than the default CloudMaid , you can add the following query parameter to the metrics flag: custodian run -s <output_directory> --metrics aws://?namespace=foo This will create a new namespace, foo in CloudWatch Metrics. You can also combine these two options to emit metrics into a custom namespace in a central account: custodian run -s <output_directory> --metrics aws://master?namespace=foo Finally, to send metrics to a specific region, use the region query parameter to specify a region: custodian run -s <output_directory> --metrics aws://?region=us-west-2 When running the metrics in a centralized account or when centralizing to a specific region, additional account and region dimensions will be included. CloudWatch Logs Custodian can optionally upload its logs in realtime to CloudWatch logs, if a log group is specified. Each policy\\'s log output is generated as a separate stream. Usage example: custodian run --log-group=/cloud-custodian/<dev-account>/<region> <policyfile>.yml If enabled, it is recommended to set a log subscription on the group to be informed of an operations issue. If S3 output is also enabled, then it is also recommended to set a log group archival policy and to use the S3 logs as permanent/audit archive. You can also aggregate your logs within a single region or account using the same url formatting as is used for metrics. To send your logs to a region in the master account use: custodian run --log-group=aws://master/<log-group-name>?region=<region> <policyfile>.yml This will set up a stream for every region/account you run custodian against within the specified log group. The default log stream format looks like this: account_id/region/policy_name If you want to override this then you can pass the the log stream parameter like this: custodian run --log-group=\\\"aws://master/\\<log-group-name>?region=\\<region>&stream=custodian_{region}_{account}_{policy} \\<policyfile>.yml\\\" it currently accepts these variables: : {account}: the account where the check was executed. {region}: the region where the check was executed. {policy}: the name of the policy that was executed. S3 Logs & Records Custodian will output its logs and structured resource records in JSON format to S3, along with its log files for archival purposes. The S3 bucket and prefix can be specified via parameters: custodian run --output-dir s3://<my-bucket>/<my-prefix> <policyfile>.yml Reports CSV or text-based reports can be generated with the report subcommand. Reporting is used to list information gathered during previous calls to the run subcommand. If your goal is to find out what resources match on a policy use run along with the --dryrun option.","title":"Usage.rst"},{"location":"aws/usage.rst/#monitoring-your-environment-usage","text":"Cloud Custodian generates a consistent set of outputs for any given policy. Custodian automatically generates per policy outputs with resources metrics and archives serialization for all resources that match against a policy\\'s filters. TODO: figure out where \\\"Custodian Dashboards in CloudWatch\\\" page goes -- here? its own page? part of Getting Started?","title":"Monitoring your environment {#usage}"},{"location":"aws/usage.rst/#metrics","text":"By default Cloud Custodian generates CloudWatch metrics on each policy for the number of resources that matched the set of filters, the time to retrieve and filter the resources, and the time to execute actions. In practice this number of matching resources allows for generating enough metrics to put together useful dashboards over policies in CloudWatch custom dashboards. Additionally some filters and actions may generate their own metrics. In order to enable metrics output, the boolean metrics flag needs to be specified when running Cloud Custodian: custodian run -s <output_directory> --metrics aws <policyfile>.yml You can also consolidate metrics into a single account by specifying the master location in the cli. Note that this is only applicable when using the --assume option in the cli or when using c7n-org. By default, metrics will be sent to the same account that is being executed against: custodian run -s <output_directory> --metrics aws://master Additionally, to use a different namespace other than the default CloudMaid , you can add the following query parameter to the metrics flag: custodian run -s <output_directory> --metrics aws://?namespace=foo This will create a new namespace, foo in CloudWatch Metrics. You can also combine these two options to emit metrics into a custom namespace in a central account: custodian run -s <output_directory> --metrics aws://master?namespace=foo Finally, to send metrics to a specific region, use the region query parameter to specify a region: custodian run -s <output_directory> --metrics aws://?region=us-west-2 When running the metrics in a centralized account or when centralizing to a specific region, additional account and region dimensions will be included.","title":"Metrics"},{"location":"aws/usage.rst/#cloudwatch-logs","text":"Custodian can optionally upload its logs in realtime to CloudWatch logs, if a log group is specified. Each policy\\'s log output is generated as a separate stream. Usage example: custodian run --log-group=/cloud-custodian/<dev-account>/<region> <policyfile>.yml If enabled, it is recommended to set a log subscription on the group to be informed of an operations issue. If S3 output is also enabled, then it is also recommended to set a log group archival policy and to use the S3 logs as permanent/audit archive. You can also aggregate your logs within a single region or account using the same url formatting as is used for metrics. To send your logs to a region in the master account use: custodian run --log-group=aws://master/<log-group-name>?region=<region> <policyfile>.yml This will set up a stream for every region/account you run custodian against within the specified log group. The default log stream format looks like this: account_id/region/policy_name If you want to override this then you can pass the the log stream parameter like this: custodian run --log-group=\\\"aws://master/\\<log-group-name>?region=\\<region>&stream=custodian_{region}_{account}_{policy} \\<policyfile>.yml\\\" it currently accepts these variables: : {account}: the account where the check was executed. {region}: the region where the check was executed. {policy}: the name of the policy that was executed.","title":"CloudWatch Logs"},{"location":"aws/usage.rst/#s3-logs-records","text":"Custodian will output its logs and structured resource records in JSON format to S3, along with its log files for archival purposes. The S3 bucket and prefix can be specified via parameters: custodian run --output-dir s3://<my-bucket>/<my-prefix> <policyfile>.yml","title":"S3 Logs &amp; Records"},{"location":"aws/usage.rst/#reports","text":"CSV or text-based reports can be generated with the report subcommand. Reporting is used to list information gathered during previous calls to the run subcommand. If your goal is to find out what resources match on a policy use run along with the --dryrun option.","title":"Reports"},{"location":"aws/examples/","text":"Example Policies {#usecases} These use cases provide examples of specific policies for individual AWS modules. ::: {.toctree titlesonly=\"\" glob=\"\"} ./* :::","title":"Index"},{"location":"aws/examples/#example-policies-usecases","text":"These use cases provide examples of specific policies for individual AWS modules. ::: {.toctree titlesonly=\"\" glob=\"\"} ./* :::","title":"Example Policies {#usecases}"},{"location":"aws/examples/accountinvalidiplogin/","text":"Account - Login From Invalid IP Address {#accountinvalidiplogin} The following example policy will automatically create a CloudWatch Event Rule triggered Lambda function in your account and region which will be triggered anytime a user logs in from an invalid IP address. If the source IP address of the event is outside of the provided ranges in the policy then notify the admins security team for further investigation. Using the cloudtrail mode provides near real-time auto-remediation (typically within 1-2 mins) of the event occurring. Having such a quick auto-remediation action greatly reduces an attack window! By notifying the cloud admins or security team they can validate the login and revoke the login session if it\\'s not valid followed by changing the password for or disabling the compromised user etc. In the below example the filter being applied is regex and reads as follows: -Notify if the source IP address of the event is not from one of the valid IP CIDRs - 158.103.0.0/16 - 142.179.0.0/16 - 187.39.0.0/16 - 12.0.0.0/8 You can generate the Regex for IP ranges on a site like: http://www.analyticsmarket.com/freetools/ipregex policies: - name: invalid-ip-address-login-detected resource: account description: | Notifies on invalid external IP console logins mode: type: cloudtrail events: - ConsoleLogin filters: - not: - type: event key: 'detail.sourceIPAddress' value: | '^((158\\.103\\.|142\\.179\\.|187\\.39\\.)([01]?[0-9]?[0-9]|2[0-4][0-9]|25[0-5]) \\.([01]?[0-9]?[0-9]|2[0-4][0-9]|25[0-5]))|(12\\.([01]?[0-9]?[0-9]|2[0-4][0-9]|25[0-5]) \\.([01]?[0-9]?[0-9]|2[0-4][0-9]|25[0-5])\\.([01]?[0-9]?[0-9]|2[0-4][0-9]|25[0-5]))$' op: regex actions: - type: notify template: default.html priority_header: 1 subject: \"Login From Invalid IP Detected - [custodian {{ account }} - {{ region }}]\" violation_desc: \"A User Has Logged In Externally From A Invalid IP Address Outside The Company's Range:\" action_desc: | \"Please investigate and revoke the invalid session along with any other restrictive actions if appropriate\" to: - CloudAdmins@Company.com - SecurityTeam@Company.com transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/12345678900/cloud-custodian-mailer region: us-east-1 Note that the notify action requires the cloud custodian mailer tool to be installed.","title":"Accountinvalidiplogin"},{"location":"aws/examples/accountinvalidiplogin/#account-login-from-invalid-ip-address-accountinvalidiplogin","text":"The following example policy will automatically create a CloudWatch Event Rule triggered Lambda function in your account and region which will be triggered anytime a user logs in from an invalid IP address. If the source IP address of the event is outside of the provided ranges in the policy then notify the admins security team for further investigation. Using the cloudtrail mode provides near real-time auto-remediation (typically within 1-2 mins) of the event occurring. Having such a quick auto-remediation action greatly reduces an attack window! By notifying the cloud admins or security team they can validate the login and revoke the login session if it\\'s not valid followed by changing the password for or disabling the compromised user etc. In the below example the filter being applied is regex and reads as follows: -Notify if the source IP address of the event is not from one of the valid IP CIDRs - 158.103.0.0/16 - 142.179.0.0/16 - 187.39.0.0/16 - 12.0.0.0/8 You can generate the Regex for IP ranges on a site like: http://www.analyticsmarket.com/freetools/ipregex policies: - name: invalid-ip-address-login-detected resource: account description: | Notifies on invalid external IP console logins mode: type: cloudtrail events: - ConsoleLogin filters: - not: - type: event key: 'detail.sourceIPAddress' value: | '^((158\\.103\\.|142\\.179\\.|187\\.39\\.)([01]?[0-9]?[0-9]|2[0-4][0-9]|25[0-5]) \\.([01]?[0-9]?[0-9]|2[0-4][0-9]|25[0-5]))|(12\\.([01]?[0-9]?[0-9]|2[0-4][0-9]|25[0-5]) \\.([01]?[0-9]?[0-9]|2[0-4][0-9]|25[0-5])\\.([01]?[0-9]?[0-9]|2[0-4][0-9]|25[0-5]))$' op: regex actions: - type: notify template: default.html priority_header: 1 subject: \"Login From Invalid IP Detected - [custodian {{ account }} - {{ region }}]\" violation_desc: \"A User Has Logged In Externally From A Invalid IP Address Outside The Company's Range:\" action_desc: | \"Please investigate and revoke the invalid session along with any other restrictive actions if appropriate\" to: - CloudAdmins@Company.com - SecurityTeam@Company.com transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/12345678900/cloud-custodian-mailer region: us-east-1 Note that the notify action requires the cloud custodian mailer tool to be installed.","title":"Account - Login From Invalid IP Address {#accountinvalidiplogin}"},{"location":"aws/examples/accountrootlogin/","text":"Account - Detect Root Logins {#accountrootlogin} The following example policy will automatically create a CloudWatch Event Rule triggered Lambda function in your account and region which will be triggered anytime the root user of the account logs in. Typically the root user of an AWS account should never need to login after the initial account setup and root user access should be very tightly controlled with hardware MFA and other controls as root has full control of everything in the account. Having this visibility to see if and when someone logs in as root is very important. policies: - name: root-user-login-detected resource: account description: | Notifies Security and Cloud Admins teams on any AWS root user console logins mode: type: cloudtrail events: - ConsoleLogin filters: - type: event key: \"detail.userIdentity.type\" value_type: swap op: in value: Root actions: - type: notify template: default.html priority_header: 1 subject: \"Root User Login Detected! - [custodian {{ account }} - {{ region }}]\" violation_desc: \"A User Has Logged Into the AWS Console With The Root User:\" action_desc: | \"Please investigate and if needed revoke the root users session along with any other restrictive actions if it's an unapproved root login\" to: - CloudAdmins@Company.com - SecurityTeam@Company.com transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/12345678900/cloud-custodian-mailer region: us-east-1 Note that the notify action requires the cloud custodian mailer tool to be installed.","title":"Accountrootlogin"},{"location":"aws/examples/accountrootlogin/#account-detect-root-logins-accountrootlogin","text":"The following example policy will automatically create a CloudWatch Event Rule triggered Lambda function in your account and region which will be triggered anytime the root user of the account logs in. Typically the root user of an AWS account should never need to login after the initial account setup and root user access should be very tightly controlled with hardware MFA and other controls as root has full control of everything in the account. Having this visibility to see if and when someone logs in as root is very important. policies: - name: root-user-login-detected resource: account description: | Notifies Security and Cloud Admins teams on any AWS root user console logins mode: type: cloudtrail events: - ConsoleLogin filters: - type: event key: \"detail.userIdentity.type\" value_type: swap op: in value: Root actions: - type: notify template: default.html priority_header: 1 subject: \"Root User Login Detected! - [custodian {{ account }} - {{ region }}]\" violation_desc: \"A User Has Logged Into the AWS Console With The Root User:\" action_desc: | \"Please investigate and if needed revoke the root users session along with any other restrictive actions if it's an unapproved root login\" to: - CloudAdmins@Company.com - SecurityTeam@Company.com transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/12345678900/cloud-custodian-mailer region: us-east-1 Note that the notify action requires the cloud custodian mailer tool to be installed.","title":"Account - Detect Root Logins {#accountrootlogin}"},{"location":"aws/examples/accountservicelimit/","text":"Account - Service Limit {#accountservicelimit} The following example policy will find any service in your region that is using more than 50% of the limit and raise the limit for 25%. policies: - name: account-service-limits resource: account filters: - type: service-limit threshold: 50 actions: - type: request-limit-increase percent-increase: 25 Noted that the threshold in service-limit filter is an optional field. If not mentioned on the policy, the default value is 80. Global Services : Services like IAM are not region-based. Custodian will put the limit information only in us-east-1 . When running the policy above in multiple regions, the limit of global services will ONLY be raised in us-east-1. Additionally, if you want to target any the global services on the policy, you will need to target the region as us-east-1 on the policy. Here is an example. ``` {.yaml} policies: - name: account-service-limits resource: account region: us-east-1 filters: - type: service-limit services: - IAM threshold: 50 ```","title":"Accountservicelimit"},{"location":"aws/examples/accountservicelimit/#account-service-limit-accountservicelimit","text":"The following example policy will find any service in your region that is using more than 50% of the limit and raise the limit for 25%. policies: - name: account-service-limits resource: account filters: - type: service-limit threshold: 50 actions: - type: request-limit-increase percent-increase: 25 Noted that the threshold in service-limit filter is an optional field. If not mentioned on the policy, the default value is 80. Global Services : Services like IAM are not region-based. Custodian will put the limit information only in us-east-1 . When running the policy above in multiple regions, the limit of global services will ONLY be raised in us-east-1. Additionally, if you want to target any the global services on the policy, you will need to target the region as us-east-1 on the policy. Here is an example. ``` {.yaml} policies: - name: account-service-limits resource: account region: us-east-1 filters: - type: service-limit services: - IAM threshold: 50 ```","title":"Account - Service Limit {#accountservicelimit}"},{"location":"aws/examples/amicomp/","text":"AMI - Stop EC2 using Unapproved AMIs - name: ec2-invalid-ami resource: ec2 comment: | Find all running EC2 instances that are using invalid AMIs and stop them filters: - \"State.Name\": running - type: value key: ImageId op: in value: - ami-12324567 # Invalid - ami-12324567 # Invalid - ami-12324567 # Invalid - ami-12324567 # Invalid - ami-12324567 # Invalid actions: - stop","title":"Amicomp"},{"location":"aws/examples/amicomp/#ami-stop-ec2-using-unapproved-amis","text":"- name: ec2-invalid-ami resource: ec2 comment: | Find all running EC2 instances that are using invalid AMIs and stop them filters: - \"State.Name\": running - type: value key: ImageId op: in value: - ami-12324567 # Invalid - ami-12324567 # Invalid - ami-12324567 # Invalid - ami-12324567 # Invalid - ami-12324567 # Invalid actions: - stop","title":"AMI - Stop EC2 using Unapproved AMIs"},{"location":"aws/examples/asginvalidconfig/","text":"AutoScaling Group - Verify ASGs have valid configurations {#asginvalidconfig} The following example policy will check all AutoScaling Groups in the current account and region for configuration issues which could prevent the ASG from functioning properly or launching an instance. Then the ASG resource owner and a cloud admins group get an email showing the affected ASG(s). The following ASG items are checked when using the - invalid filter: : - invalid subnets - invalid security groups - invalid key pair name - invalid launch config volume snapshots - invalid AMIs - invalid ELB health check policies: - name: asg-invalid-configuration resource: asg filters: - invalid actions: - type: notify template: default.html priority_header: 1 subject: \"ASG-Invalid Config-[custodian {{ account }} - {{ region }}]\" violation_desc: | \"New ASG instances may fail to launch or scale! The following Autoscaling Groups have invalid AMIs, SGs, KeyPairs, Launch Configs, or Health Checks\" action_desc: | \"Actions Taken: Notification Only. Please investigate and fix your ASGs configuration to prevent you from having any outages or issues\" to: - CloudAdmins@Company.com - resource-owner transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/12345678900/cloud-custodian-mailer region: us-east-1 Note that the notify action requires the cloud custodian mailer tool to be installed.","title":"Asginvalidconfig"},{"location":"aws/examples/asginvalidconfig/#autoscaling-group-verify-asgs-have-valid-configurations-asginvalidconfig","text":"The following example policy will check all AutoScaling Groups in the current account and region for configuration issues which could prevent the ASG from functioning properly or launching an instance. Then the ASG resource owner and a cloud admins group get an email showing the affected ASG(s). The following ASG items are checked when using the - invalid filter: : - invalid subnets - invalid security groups - invalid key pair name - invalid launch config volume snapshots - invalid AMIs - invalid ELB health check policies: - name: asg-invalid-configuration resource: asg filters: - invalid actions: - type: notify template: default.html priority_header: 1 subject: \"ASG-Invalid Config-[custodian {{ account }} - {{ region }}]\" violation_desc: | \"New ASG instances may fail to launch or scale! The following Autoscaling Groups have invalid AMIs, SGs, KeyPairs, Launch Configs, or Health Checks\" action_desc: | \"Actions Taken: Notification Only. Please investigate and fix your ASGs configuration to prevent you from having any outages or issues\" to: - CloudAdmins@Company.com - resource-owner transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/12345678900/cloud-custodian-mailer region: us-east-1 Note that the notify action requires the cloud custodian mailer tool to be installed.","title":"AutoScaling Group - Verify ASGs have valid configurations {#asginvalidconfig}"},{"location":"aws/examples/asgnotused/","text":"AMI - ASG Garbage Collector ASG garbage collector which mean that: Check if an ASG has minSize = 0 and DesiredCapacity = 0 Mark the ASG as ops to alert. If value won\\'t change cloudCustodian will send an alert with ASGs. - name: asg-mark-as-unused resource: asg comments: | Mark any unused ASG checking it every day. filters: - type: value key: MinSize value: 0 op: eq - type: value key: DesiredCapacity value: 0 op: eq actions: - type: mark-for-op op: notify days: 30 - name: asg-unmark-as-unused resource: asg comments: | Unmark any ASG that has a value greater than 0. filters: - type: value key: DesiredCapacity op: greater-than value: 0 - \"tag:maid_status\": not-null actions: - unmark - name: asg-slack-alert resource: asg comments: | Alert for ASG which have MinSize < 0 and DesiredCapacity < 0 filters: - \"tag:maid_status\": not-null - type: marked-for-op op: notify actions: - type: notify slack_template: slack violation_desc: Having ASG with both (DesiredCapacity and MinSize) = 0. action_desc: Please investigate if you can delete this ASG. to: - https://hooks.slack.com/services/TXXXXX/XXXXXX/XXXxxXXX transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/12345678900/cloud-custodian-mailer region: us-east-1","title":"Asgnotused"},{"location":"aws/examples/asgnotused/#ami-asg-garbage-collector","text":"ASG garbage collector which mean that: Check if an ASG has minSize = 0 and DesiredCapacity = 0 Mark the ASG as ops to alert. If value won\\'t change cloudCustodian will send an alert with ASGs. - name: asg-mark-as-unused resource: asg comments: | Mark any unused ASG checking it every day. filters: - type: value key: MinSize value: 0 op: eq - type: value key: DesiredCapacity value: 0 op: eq actions: - type: mark-for-op op: notify days: 30 - name: asg-unmark-as-unused resource: asg comments: | Unmark any ASG that has a value greater than 0. filters: - type: value key: DesiredCapacity op: greater-than value: 0 - \"tag:maid_status\": not-null actions: - unmark - name: asg-slack-alert resource: asg comments: | Alert for ASG which have MinSize < 0 and DesiredCapacity < 0 filters: - \"tag:maid_status\": not-null - type: marked-for-op op: notify actions: - type: notify slack_template: slack violation_desc: Having ASG with both (DesiredCapacity and MinSize) = 0. action_desc: Please investigate if you can delete this ASG. to: - https://hooks.slack.com/services/TXXXXX/XXXXXX/XXXxxXXX transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/12345678900/cloud-custodian-mailer region: us-east-1","title":"AMI - ASG Garbage Collector"},{"location":"aws/examples/asgoffhours/","text":"ASG - Offhours Support {#asgoffhours} The following example policy will stop all ASGs with the custodian_downtime tag at 10pm daily and start them back up at 10am daily, leaving them off during weekends. policies: - name: offhour-stop-22 resource: asg comments: | Daily stoppage at 10pm filters: - type: offhour tag: custodian_downtime offhour: 22 actions: - suspend - name: onhour-start-10 resource: asg comments: | Daily start at 10am filters: - type: onhour tag: custodian_downtime onhour: 10 actions: - resume For detailed information on offhours/onhours support and configuration, see offhours {.interpreted-text role=\"ref\"}.","title":"Asgoffhours"},{"location":"aws/examples/asgoffhours/#asg-offhours-support-asgoffhours","text":"The following example policy will stop all ASGs with the custodian_downtime tag at 10pm daily and start them back up at 10am daily, leaving them off during weekends. policies: - name: offhour-stop-22 resource: asg comments: | Daily stoppage at 10pm filters: - type: offhour tag: custodian_downtime offhour: 22 actions: - suspend - name: onhour-start-10 resource: asg comments: | Daily start at 10am filters: - type: onhour tag: custodian_downtime onhour: 10 actions: - resume For detailed information on offhours/onhours support and configuration, see offhours {.interpreted-text role=\"ref\"}.","title":"ASG - Offhours Support {#asgoffhours}"},{"location":"aws/examples/blocknonstandardregionresources/","text":"Block New Resources In Non-Standard Regions {#blocknonstandardregionresources} The following are examples of Cloud Custodian policies which detect the region a resource is being launched in and deletes the resource if it\\'s outside your standard approved regions. These examples block the full creation of the resources launched outside of the us-east-1 and eu-west-1 regions and then emails the event-owner (the person launching the resource) and the Cloud Team. This set of policies covers several of the common AWS services but you may add your desired services if supported by Cloud Custodian. While a proactive approach through IAM or AWS Organizations policies is the ideal way to go, that isn\\'t always possible or manageable for all users. These policies take a reactive approach and may be a fitting use case for some users. For the notify action to work you will need to have installed and configured the Cloud Custodian c7n-mailer tool. policies: - name: ec2-terminate-non-standard-region resource: ec2 description: | Any EC2 instance launched in a non standard region outside of us-east-1 and eu-west-1 will be terminated mode: type: cloudtrail events: - RunInstances filters: - type: event key: \"detail.awsRegion\" op: not-in value: - us-east-1 - eu-west-1 actions: - type: terminate force: true - type: notify template: default.html priority_header: 1 subject: \"EC2 SERVER TERMINATED - Non-Standard Region [custodian {{ account }} - {{ region }}]\" violation_desc: \"Launching resources outside of the standard regions is prohibited\" action_desc: \"Actions Taken: Your new EC2 server has been terminated. Please relaunch the server in your accounts standard region which is either eu-west-1 or us-east-1.\" to: - CloudTeam@Company.com - event-owner transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/XXXXXXXXXXX/cloud-custodian-mailer region: us-east-1 - name: asg-terminate-non-standard-region resource: asg mode: type: cloudtrail events: - source: autoscaling.amazonaws.com event: CreateAutoScalingGroup ids: requestParameters.autoScalingGroupName description: | Detect when a new AutoScaling Group is created in a non-standard region and delete it and notify the customer filters: - type: event key: \"detail.awsRegion\" op: not-in value: - us-east-1 - eu-west-1 actions: - type: delete force: true - type: notify template: default.html priority_header: 1 subject: \"ASG TERMINATED - Non-Standard Region [custodian {{ account }} - {{ region }}]\" violation_desc: \"Launching resources outside of the standard regions is prohibited\" action_desc: \"Actions Taken: Your new ASG has been terminated. Please relaunch the ASG in your accounts standard region which is either eu-west-1 or us-east-1.\" to: - CloudTeam@Company.com - event-owner transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/XXXXXXXXXXX/cloud-custodian-mailer region: us-east-1 - name: app-elb-terminate-non-standard-region resource: app-elb mode: type: cloudtrail events: - source: \"elasticloadbalancing.amazonaws.com\" event: CreateLoadBalancer ids: \"requestParameters.name\" description: | Detect when a new Application Load Balancer Group is created in a non-standard region and delete it and notify the customer filters: - type: event key: \"detail.awsRegion\" op: not-in value: - us-east-1 - eu-west-1 actions: - type: delete - type: notify template: default.html priority_header: 1 subject: \"App ELB TERMINATED - Non-Standard Region [custodian {{ account }} - {{ region }}]\" violation_desc: \"Launching resources outside of the standard regions is prohibited\" action_desc: \"Actions Taken: Your new App ELB has been deleted. Please relaunch the App ELB in your accounts standard region which is either eu-west-1 or us-east-1.\" to: - CloudTeam@Company.com - event-owner transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/XXXXXXXXXXX/cloud-custodian-mailer region: us-east-1 - name: elb-terminate-non-standard-region resource: elb mode: type: cloudtrail events: - CreateLoadBalancer description: | Detect when a new Load Balancer is created in a non-standard region and delete it and notify the customer filters: - type: event key: \"detail.awsRegion\" op: not-in value: - us-east-1 - eu-west-1 actions: - type: delete - type: notify template: default.html priority_header: 1 subject: \"ELB TERMINATED - Non-Standard Region [custodian {{ account }} - {{ region }}]\" violation_desc: \"Launching resources outside of the standard regions is prohibited\" action_desc: \"Actions Taken: Your new ELB has been deleted. Please relaunch the ELB in your accounts standard region which is either eu-west-1 or us-east-1.\" to: - CloudTeam@Company.com - event-owner transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/XXXXXXXXXXX/cloud-custodian-mailer region: us-east-1 - name: es-terminate-non-standard-region resource: elasticsearch mode: type: cloudtrail events: - CreateElasticsearchDomain description: | Detect when a new Elasticsearch Domain is created in a non-standard region and delete it and notify the customer filters: - type: event key: \"detail.awsRegion\" op: not-in value: - us-east-1 - eu-west-1 actions: - delete - type: notify template: default.html priority_header: 1 subject: \"ES DOMAIN TERMINATED - Non-Standard Region [custodian {{ account }} - {{ region }}]\" violation_desc: \"Launching resources outside of the standard regions is prohibited\" action_desc: \"Actions Taken: Your new Elasticsearch Domain has been deleted. Please relaunch the Domain in your accounts standard region which is either eu-west-1 or us-east-1.\" to: - CloudTeam@Company.com - event-owner transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/XXXXXXXXXXX/cloud-custodian-mailer region: us-east-1 - name: lambda-terminate-non-standard-region resource: lambda mode: type: cloudtrail events: - source: lambda.amazonaws.com event: CreateFunction20150331 ids: \"requestParameters.functionName\" description: | Detect when a new Lambda Function is created in a non-standard region and delete it and notify the customer filters: - type: event key: \"detail.awsRegion\" op: not-in value: - us-east-1 - eu-west-1 - not: - or: - type: value key: FunctionName op: regex value: ^(custodian?)\\w+ actions: - delete - type: notify template: default.html priority_header: 1 subject: \"LAMBDA DELETED - Non-Standard Region [custodian {{ account }} - {{ region }}]\" violation_desc: \"Launching resources outside of the standard regions is prohibited\" action_desc: \"Actions Taken: Your new Lambda Function has been deleted. Please relaunch in your accounts standard region which is either eu-west-1 or us-east-1.\" to: - CloudTeam@Company.com - event-owner transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/XXXXXXXXXXX/cloud-custodian-mailer region: us-east-1 - name: rds-terminate-non-standard-region resource: rds mode: type: cloudtrail events: - source: rds.amazonaws.com event: CreateDBInstance ids: \"requestParameters.dBInstanceIdentifier\" description: | Detect when a new RDS is created in a non-standard region and delete it and notify the customer filters: - type: event key: \"detail.awsRegion\" op: not-in value: - us-east-1 - eu-west-1 actions: - type: delete skip-snapshot: true - type: notify template: default.html priority_header: 1 subject: \"RDS DELETED - Non-Standard Region [custodian {{ account }} - {{ region }}]\" violation_desc: \"Launching resources outside of the standard regions is prohibited\" action_desc: \"Actions Taken: Your new RDS Database has been deleted. Please relaunch in your accounts standard region which is either eu-west-1 or us-east-1.\" to: - CloudTeam@Company.com - event-owner transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/XXXXXXXXXXX/cloud-custodian-mailer region: us-east-1 - name: rdscluster-terminate-non-standard-region resource: rds-cluster mode: type: cloudtrail events: - CreateCluster description: | Detect when a new RDS Cluster is created in a non-standard region and delete it and notify the customer filters: - type: event key: \"detail.awsRegion\" op: not-in value: - us-east-1 - eu-west-1 actions: - type: delete skip-snapshot: true delete-instances: true - type: notify template: default.html priority_header: 1 subject: \"RDS CLUSTER DELETED - Non-Standard Region [custodian {{ account }} - {{ region }}]\" violation_desc: \"Launching resources outside of the standard regions is prohibited\" action_desc: \"Actions Taken: Your new RDS Database Cluster has been deleted. Please relaunch in your accounts standard region which is either eu-west-1 or us-east-1.\" to: - CloudTeam@Company.com - event-owner transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/XXXXXXXXXXX/cloud-custodian-mailer region: us-east-1 - name: sg-terminate-non-standard-region resource: security-group mode: type: cloudtrail events: - source: ec2.amazonaws.com event: CreateSecurityGroup ids: \"responseElements.groupId\" description: | Detect when a new Security Group is created in a non-standard region and delete it and notify the customer filters: - type: event key: \"detail.awsRegion\" op: not-in value: - us-east-1 - eu-west-1 actions: - delete - type: notify template: default.html priority_header: 1 subject: \"SG DELETED - Non-Standard Region [custodian {{ account }} - {{ region }}]\" violation_desc: \"Launching resources outside of the standard regions is prohibited\" action_desc: \"Actions Taken: Your new Security Group has been deleted. Please recreate in your accounts standard region which is either eu-west-1 or us-east-1.\" to: - CloudTeam@Company.com - event-owner transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/XXXXXXXXXXX/cloud-custodian-mailer region: us-east-1 - name: ami-terminate-non-standard-region resource: ami mode: type: cloudtrail events: - source: \"ec2.amazonaws.com\" event: \"CreateImage\" ids: \"responseElements.imageId\" description: | Detect when a new Amazon Machine Image is created in a non-standard region and delete it and notify the customer filters: - type: event key: \"detail.awsRegion\" op: not-in value: - us-east-1 - eu-west-1 actions: - deregister - remove-launch-permissions - type: notify template: default.html priority_header: 1 subject: \"AMI DELETED - Non-Standard Region [custodian {{ account }} - {{ region }}]\" violation_desc: \"Launching resources outside of the standard regions is prohibited\" action_desc: \"Actions Taken: Your new Amazon Machine Image has been deleted. Please recreate in your accounts standard region which is either eu-west-1 or us-east-1.\" to: - CloudTeam@Company.com - event-owner transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/XXXXXXXXXXX/cloud-custodian-mailer region: us-east-1 - name: s3-terminate-non-standard-region resource: s3 mode: type: cloudtrail events: - CreateBucket role: arn:aws:iam::{account_id}:role/Cloud_Custodian_Role timeout: 200 description: | Detect when a new S3 Bucket is created in a non-standard region and delete it and notify the customer filters: - type: event key: \"detail.awsRegion\" op: not-in value: - us-east-1 - eu-west-1 actions: - type: delete remove-contents: true - type: notify template: default.html priority_header: 1 subject: \"S3 DELETED - Non-Standard Region [custodian {{ account }} - {{ region }}]\" violation_desc: \"Launching resources outside of the standard regions is prohibited\" action_desc: \"Actions Taken: Your new S3 Bucket has been deleted. Please recreate in your accounts standard region which is either eu-west-1 or us-east-1.\" to: - CloudTeam@Company.com - event-owner transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/XXXXXXXXXXX/cloud-custodian-mailer region: us-east-1 - name: dynamo-terminate-non-standard-region resource: dynamodb-table mode: type: cloudtrail events: - CreateTable description: | Detect when a new DynamoDB Table is created in a non-standard region and delete it and notify the customer filters: - type: event key: \"detail.awsRegion\" op: not-in value: - us-east-1 - eu-west-1 actions: - delete - type: notify template: default.html priority_header: 1 subject: \"DYNAMODB DELETED - Non-Standard Region [custodian {{ account }} - {{ region }}]\" violation_desc: \"Launching resources outside of the standard regions is prohibited\" action_desc: \"Actions Taken: Your new DynamoDB Table has been deleted. Please recreate in your accounts standard region which is either eu-west-1 or us-east-1.\" to: - CloudTeam@Company.com - event-owner transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/XXXXXXXXXXX/cloud-custodian-mailer region: us-east-1 - name: kinesis-terminate-non-standard-region resource: kinesis mode: type: cloudtrail events: - source: \"kinesis.amazonaws.com\" event: \"CreateStream\" ids: \"requestParameters.streamName\" description: | Detect when a new Kinesis Stream is created in a non-standard region and delete it and notify the customer filters: - type: event key: \"detail.awsRegion\" op: not-in value: - us-east-1 - eu-west-1 actions: - type: delete - type: notify template: default.html priority_header: 1 subject: \"KINESIS DELETED - Non-Standard Region [custodian {{ account }} - {{ region }}]\" violation_desc: \"Launching resources outside of the standard regions is prohibited\" action_desc: \"Actions Taken: Your new Kinesis Stream has been deleted. Please recreate in your accounts standard region which is either eu-west-1 or us-east-1.\" to: - CloudTeam@Company.com - event-owner transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/XXXXXXXXXXX/cloud-custodian-mailer region: us-east-1 - name: firehose-terminate-non-standard-region resource: firehose mode: type: cloudtrail events: - source: \"firehose.amazonaws.com\" event: \"CreateDeliveryStream\" ids: \"requestParameters.deliveryStreamName\" description: | Detect when a new Firehose is created in a non-standard region and delete it and notify the customer filters: - type: event key: \"detail.awsRegion\" op: not-in value: - us-east-1 - eu-west-1 actions: - type: delete - type: notify template: default.html priority_header: 1 subject: \"FIREHOSE DELETED - Non-Standard Region [custodian {{ account }} - {{ region }}]\" violation_desc: \"Launching resources outside of the standard regions is prohibited\" action_desc: \"Actions Taken: Your new Firehose has been deleted. Please recreate in your accounts standard region which is either eu-west-1 or us-east-1.\" to: - CloudTeam@Company.com - event-owner transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/XXXXXXXXXXX/cloud-custodian-mailer region: us-east-1","title":"Blocknonstandardregionresources"},{"location":"aws/examples/blocknonstandardregionresources/#block-new-resources-in-non-standard-regions-blocknonstandardregionresources","text":"The following are examples of Cloud Custodian policies which detect the region a resource is being launched in and deletes the resource if it\\'s outside your standard approved regions. These examples block the full creation of the resources launched outside of the us-east-1 and eu-west-1 regions and then emails the event-owner (the person launching the resource) and the Cloud Team. This set of policies covers several of the common AWS services but you may add your desired services if supported by Cloud Custodian. While a proactive approach through IAM or AWS Organizations policies is the ideal way to go, that isn\\'t always possible or manageable for all users. These policies take a reactive approach and may be a fitting use case for some users. For the notify action to work you will need to have installed and configured the Cloud Custodian c7n-mailer tool. policies: - name: ec2-terminate-non-standard-region resource: ec2 description: | Any EC2 instance launched in a non standard region outside of us-east-1 and eu-west-1 will be terminated mode: type: cloudtrail events: - RunInstances filters: - type: event key: \"detail.awsRegion\" op: not-in value: - us-east-1 - eu-west-1 actions: - type: terminate force: true - type: notify template: default.html priority_header: 1 subject: \"EC2 SERVER TERMINATED - Non-Standard Region [custodian {{ account }} - {{ region }}]\" violation_desc: \"Launching resources outside of the standard regions is prohibited\" action_desc: \"Actions Taken: Your new EC2 server has been terminated. Please relaunch the server in your accounts standard region which is either eu-west-1 or us-east-1.\" to: - CloudTeam@Company.com - event-owner transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/XXXXXXXXXXX/cloud-custodian-mailer region: us-east-1 - name: asg-terminate-non-standard-region resource: asg mode: type: cloudtrail events: - source: autoscaling.amazonaws.com event: CreateAutoScalingGroup ids: requestParameters.autoScalingGroupName description: | Detect when a new AutoScaling Group is created in a non-standard region and delete it and notify the customer filters: - type: event key: \"detail.awsRegion\" op: not-in value: - us-east-1 - eu-west-1 actions: - type: delete force: true - type: notify template: default.html priority_header: 1 subject: \"ASG TERMINATED - Non-Standard Region [custodian {{ account }} - {{ region }}]\" violation_desc: \"Launching resources outside of the standard regions is prohibited\" action_desc: \"Actions Taken: Your new ASG has been terminated. Please relaunch the ASG in your accounts standard region which is either eu-west-1 or us-east-1.\" to: - CloudTeam@Company.com - event-owner transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/XXXXXXXXXXX/cloud-custodian-mailer region: us-east-1 - name: app-elb-terminate-non-standard-region resource: app-elb mode: type: cloudtrail events: - source: \"elasticloadbalancing.amazonaws.com\" event: CreateLoadBalancer ids: \"requestParameters.name\" description: | Detect when a new Application Load Balancer Group is created in a non-standard region and delete it and notify the customer filters: - type: event key: \"detail.awsRegion\" op: not-in value: - us-east-1 - eu-west-1 actions: - type: delete - type: notify template: default.html priority_header: 1 subject: \"App ELB TERMINATED - Non-Standard Region [custodian {{ account }} - {{ region }}]\" violation_desc: \"Launching resources outside of the standard regions is prohibited\" action_desc: \"Actions Taken: Your new App ELB has been deleted. Please relaunch the App ELB in your accounts standard region which is either eu-west-1 or us-east-1.\" to: - CloudTeam@Company.com - event-owner transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/XXXXXXXXXXX/cloud-custodian-mailer region: us-east-1 - name: elb-terminate-non-standard-region resource: elb mode: type: cloudtrail events: - CreateLoadBalancer description: | Detect when a new Load Balancer is created in a non-standard region and delete it and notify the customer filters: - type: event key: \"detail.awsRegion\" op: not-in value: - us-east-1 - eu-west-1 actions: - type: delete - type: notify template: default.html priority_header: 1 subject: \"ELB TERMINATED - Non-Standard Region [custodian {{ account }} - {{ region }}]\" violation_desc: \"Launching resources outside of the standard regions is prohibited\" action_desc: \"Actions Taken: Your new ELB has been deleted. Please relaunch the ELB in your accounts standard region which is either eu-west-1 or us-east-1.\" to: - CloudTeam@Company.com - event-owner transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/XXXXXXXXXXX/cloud-custodian-mailer region: us-east-1 - name: es-terminate-non-standard-region resource: elasticsearch mode: type: cloudtrail events: - CreateElasticsearchDomain description: | Detect when a new Elasticsearch Domain is created in a non-standard region and delete it and notify the customer filters: - type: event key: \"detail.awsRegion\" op: not-in value: - us-east-1 - eu-west-1 actions: - delete - type: notify template: default.html priority_header: 1 subject: \"ES DOMAIN TERMINATED - Non-Standard Region [custodian {{ account }} - {{ region }}]\" violation_desc: \"Launching resources outside of the standard regions is prohibited\" action_desc: \"Actions Taken: Your new Elasticsearch Domain has been deleted. Please relaunch the Domain in your accounts standard region which is either eu-west-1 or us-east-1.\" to: - CloudTeam@Company.com - event-owner transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/XXXXXXXXXXX/cloud-custodian-mailer region: us-east-1 - name: lambda-terminate-non-standard-region resource: lambda mode: type: cloudtrail events: - source: lambda.amazonaws.com event: CreateFunction20150331 ids: \"requestParameters.functionName\" description: | Detect when a new Lambda Function is created in a non-standard region and delete it and notify the customer filters: - type: event key: \"detail.awsRegion\" op: not-in value: - us-east-1 - eu-west-1 - not: - or: - type: value key: FunctionName op: regex value: ^(custodian?)\\w+ actions: - delete - type: notify template: default.html priority_header: 1 subject: \"LAMBDA DELETED - Non-Standard Region [custodian {{ account }} - {{ region }}]\" violation_desc: \"Launching resources outside of the standard regions is prohibited\" action_desc: \"Actions Taken: Your new Lambda Function has been deleted. Please relaunch in your accounts standard region which is either eu-west-1 or us-east-1.\" to: - CloudTeam@Company.com - event-owner transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/XXXXXXXXXXX/cloud-custodian-mailer region: us-east-1 - name: rds-terminate-non-standard-region resource: rds mode: type: cloudtrail events: - source: rds.amazonaws.com event: CreateDBInstance ids: \"requestParameters.dBInstanceIdentifier\" description: | Detect when a new RDS is created in a non-standard region and delete it and notify the customer filters: - type: event key: \"detail.awsRegion\" op: not-in value: - us-east-1 - eu-west-1 actions: - type: delete skip-snapshot: true - type: notify template: default.html priority_header: 1 subject: \"RDS DELETED - Non-Standard Region [custodian {{ account }} - {{ region }}]\" violation_desc: \"Launching resources outside of the standard regions is prohibited\" action_desc: \"Actions Taken: Your new RDS Database has been deleted. Please relaunch in your accounts standard region which is either eu-west-1 or us-east-1.\" to: - CloudTeam@Company.com - event-owner transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/XXXXXXXXXXX/cloud-custodian-mailer region: us-east-1 - name: rdscluster-terminate-non-standard-region resource: rds-cluster mode: type: cloudtrail events: - CreateCluster description: | Detect when a new RDS Cluster is created in a non-standard region and delete it and notify the customer filters: - type: event key: \"detail.awsRegion\" op: not-in value: - us-east-1 - eu-west-1 actions: - type: delete skip-snapshot: true delete-instances: true - type: notify template: default.html priority_header: 1 subject: \"RDS CLUSTER DELETED - Non-Standard Region [custodian {{ account }} - {{ region }}]\" violation_desc: \"Launching resources outside of the standard regions is prohibited\" action_desc: \"Actions Taken: Your new RDS Database Cluster has been deleted. Please relaunch in your accounts standard region which is either eu-west-1 or us-east-1.\" to: - CloudTeam@Company.com - event-owner transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/XXXXXXXXXXX/cloud-custodian-mailer region: us-east-1 - name: sg-terminate-non-standard-region resource: security-group mode: type: cloudtrail events: - source: ec2.amazonaws.com event: CreateSecurityGroup ids: \"responseElements.groupId\" description: | Detect when a new Security Group is created in a non-standard region and delete it and notify the customer filters: - type: event key: \"detail.awsRegion\" op: not-in value: - us-east-1 - eu-west-1 actions: - delete - type: notify template: default.html priority_header: 1 subject: \"SG DELETED - Non-Standard Region [custodian {{ account }} - {{ region }}]\" violation_desc: \"Launching resources outside of the standard regions is prohibited\" action_desc: \"Actions Taken: Your new Security Group has been deleted. Please recreate in your accounts standard region which is either eu-west-1 or us-east-1.\" to: - CloudTeam@Company.com - event-owner transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/XXXXXXXXXXX/cloud-custodian-mailer region: us-east-1 - name: ami-terminate-non-standard-region resource: ami mode: type: cloudtrail events: - source: \"ec2.amazonaws.com\" event: \"CreateImage\" ids: \"responseElements.imageId\" description: | Detect when a new Amazon Machine Image is created in a non-standard region and delete it and notify the customer filters: - type: event key: \"detail.awsRegion\" op: not-in value: - us-east-1 - eu-west-1 actions: - deregister - remove-launch-permissions - type: notify template: default.html priority_header: 1 subject: \"AMI DELETED - Non-Standard Region [custodian {{ account }} - {{ region }}]\" violation_desc: \"Launching resources outside of the standard regions is prohibited\" action_desc: \"Actions Taken: Your new Amazon Machine Image has been deleted. Please recreate in your accounts standard region which is either eu-west-1 or us-east-1.\" to: - CloudTeam@Company.com - event-owner transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/XXXXXXXXXXX/cloud-custodian-mailer region: us-east-1 - name: s3-terminate-non-standard-region resource: s3 mode: type: cloudtrail events: - CreateBucket role: arn:aws:iam::{account_id}:role/Cloud_Custodian_Role timeout: 200 description: | Detect when a new S3 Bucket is created in a non-standard region and delete it and notify the customer filters: - type: event key: \"detail.awsRegion\" op: not-in value: - us-east-1 - eu-west-1 actions: - type: delete remove-contents: true - type: notify template: default.html priority_header: 1 subject: \"S3 DELETED - Non-Standard Region [custodian {{ account }} - {{ region }}]\" violation_desc: \"Launching resources outside of the standard regions is prohibited\" action_desc: \"Actions Taken: Your new S3 Bucket has been deleted. Please recreate in your accounts standard region which is either eu-west-1 or us-east-1.\" to: - CloudTeam@Company.com - event-owner transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/XXXXXXXXXXX/cloud-custodian-mailer region: us-east-1 - name: dynamo-terminate-non-standard-region resource: dynamodb-table mode: type: cloudtrail events: - CreateTable description: | Detect when a new DynamoDB Table is created in a non-standard region and delete it and notify the customer filters: - type: event key: \"detail.awsRegion\" op: not-in value: - us-east-1 - eu-west-1 actions: - delete - type: notify template: default.html priority_header: 1 subject: \"DYNAMODB DELETED - Non-Standard Region [custodian {{ account }} - {{ region }}]\" violation_desc: \"Launching resources outside of the standard regions is prohibited\" action_desc: \"Actions Taken: Your new DynamoDB Table has been deleted. Please recreate in your accounts standard region which is either eu-west-1 or us-east-1.\" to: - CloudTeam@Company.com - event-owner transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/XXXXXXXXXXX/cloud-custodian-mailer region: us-east-1 - name: kinesis-terminate-non-standard-region resource: kinesis mode: type: cloudtrail events: - source: \"kinesis.amazonaws.com\" event: \"CreateStream\" ids: \"requestParameters.streamName\" description: | Detect when a new Kinesis Stream is created in a non-standard region and delete it and notify the customer filters: - type: event key: \"detail.awsRegion\" op: not-in value: - us-east-1 - eu-west-1 actions: - type: delete - type: notify template: default.html priority_header: 1 subject: \"KINESIS DELETED - Non-Standard Region [custodian {{ account }} - {{ region }}]\" violation_desc: \"Launching resources outside of the standard regions is prohibited\" action_desc: \"Actions Taken: Your new Kinesis Stream has been deleted. Please recreate in your accounts standard region which is either eu-west-1 or us-east-1.\" to: - CloudTeam@Company.com - event-owner transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/XXXXXXXXXXX/cloud-custodian-mailer region: us-east-1 - name: firehose-terminate-non-standard-region resource: firehose mode: type: cloudtrail events: - source: \"firehose.amazonaws.com\" event: \"CreateDeliveryStream\" ids: \"requestParameters.deliveryStreamName\" description: | Detect when a new Firehose is created in a non-standard region and delete it and notify the customer filters: - type: event key: \"detail.awsRegion\" op: not-in value: - us-east-1 - eu-west-1 actions: - type: delete - type: notify template: default.html priority_header: 1 subject: \"FIREHOSE DELETED - Non-Standard Region [custodian {{ account }} - {{ region }}]\" violation_desc: \"Launching resources outside of the standard regions is prohibited\" action_desc: \"Actions Taken: Your new Firehose has been deleted. Please recreate in your accounts standard region which is either eu-west-1 or us-east-1.\" to: - CloudTeam@Company.com - event-owner transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/XXXXXXXXXXX/cloud-custodian-mailer region: us-east-1","title":"Block New Resources In Non-Standard Regions {#blocknonstandardregionresources}"},{"location":"aws/examples/dmsenforcessl/","text":"DMS - DB Migration Service Endpoint - Enforce SSL {#dmsenforcessl} The following example policies will allow you to enforce SSL connectivity on any new or modified DMS Endpoints. The supported SSL methods vary based on the database engine. See https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Security.SSL.html for more info. There are 2 policies to handle the different types of SSL. With sqlserver, mongodb, and postgres you can turn on the SSL mode to require without having to pass in a certificate. Most other database engines would require you to pass in the ARN of the CA certificate to use which is why automating those in a c7n policy is difficult and this example policy will just delete them instead. DMS certificate ARNS are unique per account and region which is why multi-account policy runs wouldn\\'t work. Both policies trigger off the creation or modification of any DMS endpoints so if a user tries to disable the SSL it would re-enable the SSL or delete the users endpoint and then email them depending on SSL modes supported. For the notify action in the second policy to work you must have setup the c7n_mailer tool: https://github.com/cloud-custodian/cloud-custodian/tree/master/tools/c7n_mailer policies: - name: dms-endpoint-enable-ssl-require-realtime resource: dms-endpoint description: | If the SSL Mode is none for a DMS Endpoint with engine of sql, mongo, or postgres it gets turned on to Require SSL setting mode: type: cloudtrail events: - source: dms.amazonaws.com event: CreateEndpoint ids: \"responseElements.endpoint.endpointArn\" - source: dms.amazonaws.com event: ModifyEndpoint ids: \"responseElements.endpoint.endpointArn\" filters: - or: - SslMode: none - type: event key: \"detail.requestParameters.sslMode\" op: eq value: \"none\" - or: - EngineName: sqlserver - EngineName: mongodb - EngineName: postgres actions: - type: modify-endpoint SslMode: require - name: dms-delete-endpoint-missing-ssl-ca-cert-realtime resource: dms-endpoint description: | If the SSL Mode is none for a DMS Endpoint with engine that is not one of sql, mongo, or postgres the endpoint is deleted and an email is sent stating that CA Certificates need to be used as a requirement mode: type: cloudtrail events: - source: dms.amazonaws.com event: CreateEndpoint ids: \"responseElements.endpoint.endpointArn\" - source: dms.amazonaws.com event: ModifyEndpoint ids: \"responseElements.endpoint.endpointArn\" filters: - or: - SslMode: none - type: event key: \"detail.requestParameters.sslMode\" op: eq value: \"none\" - or: - EngineName: aurora - EngineName: mariadb - EngineName: mysql - EngineName: sybase - EngineName: oracle actions: - delete - type: notify template: default.html priority_header: 1 subject: DMS Endpoint Deleted As It's Non-Compliant! - [custodian {{ account }} - {{ region }}] violation_desc: | Per regulations all DMS Endpoints have to use SSL connections and your endpoint was setup as 'none' for SSL mode! action_desc: | Actions Taken: You are required to enable SSL on your endpoint for a secure transmission of data. This incident has been reported and the invalid endpoint has been deleted. Please launch a new endpoint using SSL to: - CloudCustodian@Company.com - resource-owner - event-owner transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/123456789012/cloud-custodian-mailer region: us-east-1","title":"Dmsenforcessl"},{"location":"aws/examples/dmsenforcessl/#dms-db-migration-service-endpoint-enforce-ssl-dmsenforcessl","text":"The following example policies will allow you to enforce SSL connectivity on any new or modified DMS Endpoints. The supported SSL methods vary based on the database engine. See https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Security.SSL.html for more info. There are 2 policies to handle the different types of SSL. With sqlserver, mongodb, and postgres you can turn on the SSL mode to require without having to pass in a certificate. Most other database engines would require you to pass in the ARN of the CA certificate to use which is why automating those in a c7n policy is difficult and this example policy will just delete them instead. DMS certificate ARNS are unique per account and region which is why multi-account policy runs wouldn\\'t work. Both policies trigger off the creation or modification of any DMS endpoints so if a user tries to disable the SSL it would re-enable the SSL or delete the users endpoint and then email them depending on SSL modes supported. For the notify action in the second policy to work you must have setup the c7n_mailer tool: https://github.com/cloud-custodian/cloud-custodian/tree/master/tools/c7n_mailer policies: - name: dms-endpoint-enable-ssl-require-realtime resource: dms-endpoint description: | If the SSL Mode is none for a DMS Endpoint with engine of sql, mongo, or postgres it gets turned on to Require SSL setting mode: type: cloudtrail events: - source: dms.amazonaws.com event: CreateEndpoint ids: \"responseElements.endpoint.endpointArn\" - source: dms.amazonaws.com event: ModifyEndpoint ids: \"responseElements.endpoint.endpointArn\" filters: - or: - SslMode: none - type: event key: \"detail.requestParameters.sslMode\" op: eq value: \"none\" - or: - EngineName: sqlserver - EngineName: mongodb - EngineName: postgres actions: - type: modify-endpoint SslMode: require - name: dms-delete-endpoint-missing-ssl-ca-cert-realtime resource: dms-endpoint description: | If the SSL Mode is none for a DMS Endpoint with engine that is not one of sql, mongo, or postgres the endpoint is deleted and an email is sent stating that CA Certificates need to be used as a requirement mode: type: cloudtrail events: - source: dms.amazonaws.com event: CreateEndpoint ids: \"responseElements.endpoint.endpointArn\" - source: dms.amazonaws.com event: ModifyEndpoint ids: \"responseElements.endpoint.endpointArn\" filters: - or: - SslMode: none - type: event key: \"detail.requestParameters.sslMode\" op: eq value: \"none\" - or: - EngineName: aurora - EngineName: mariadb - EngineName: mysql - EngineName: sybase - EngineName: oracle actions: - delete - type: notify template: default.html priority_header: 1 subject: DMS Endpoint Deleted As It's Non-Compliant! - [custodian {{ account }} - {{ region }}] violation_desc: | Per regulations all DMS Endpoints have to use SSL connections and your endpoint was setup as 'none' for SSL mode! action_desc: | Actions Taken: You are required to enable SSL on your endpoint for a secure transmission of data. This incident has been reported and the invalid endpoint has been deleted. Please launch a new endpoint using SSL to: - CloudCustodian@Company.com - resource-owner - event-owner transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/123456789012/cloud-custodian-mailer region: us-east-1","title":"DMS - DB Migration Service Endpoint - Enforce SSL {#dmsenforcessl}"},{"location":"aws/examples/ebsgarbagecollect/","text":"EBS - Garbage Collect Unattached Volumes Use the mark-for-op action to mark a resource for action later. One common pattern to follow is to mark a resource with an operation (example: delete) in n days. In the subsequent days leading up to the marked date, run a unmark or untag policy if the resource has become compliant in the mean time. You can use this principle to implement garbage collection on resources. In this example, Custodian will first mark all unattached EBS volumes for deletion. The next policy will then unmark any volume that has been attached and has the maid_status tag, indicating that it had been previously marked. Finally, the third policy will filter in any resources that have been marked and run the delete action. It is important to note that the delete policy will need to be run on the day that the resource is marked for, else the resource will still exist in the account. The mark operation only tags the resource with metadata about the upcoming operation. Operationally, the policy still must be executed on the day that is specified in the tag. Note: all resources that are marked-for-op up to and including the current date will be filtered in when utilizing the marked-for-op filter. ``` {.yaml} - name: ebs-mark-unattached-deletion resource: ebs comments: | Mark any unattached EBS volumes for deletion in 30 days. Volumes set to not delete on instance termination do have valid use cases as data drives, but 99% of the time they appear to be just garbage creation. filters: - Attachments: [] - \"tag:maid_status\": absent actions: - type: mark-for-op op: delete days: 30 name: ebs-unmark-attached-deletion resource: ebs comments: | Unmark any attached EBS volumes that were scheduled for deletion if they are currently attached filters: type: value key: \"Attachments[0].Device\" value: not-null \"tag:maid_status\": not-null actions: unmark name: ebs-delete-marked resource: ebs comments: | Delete any attached EBS volumes that were scheduled for deletion filters: type: marked-for-op op: delete actions: delete ```","title":"Ebsgarbagecollect"},{"location":"aws/examples/ebsgarbagecollect/#ebs-garbage-collect-unattached-volumes","text":"Use the mark-for-op action to mark a resource for action later. One common pattern to follow is to mark a resource with an operation (example: delete) in n days. In the subsequent days leading up to the marked date, run a unmark or untag policy if the resource has become compliant in the mean time. You can use this principle to implement garbage collection on resources. In this example, Custodian will first mark all unattached EBS volumes for deletion. The next policy will then unmark any volume that has been attached and has the maid_status tag, indicating that it had been previously marked. Finally, the third policy will filter in any resources that have been marked and run the delete action. It is important to note that the delete policy will need to be run on the day that the resource is marked for, else the resource will still exist in the account. The mark operation only tags the resource with metadata about the upcoming operation. Operationally, the policy still must be executed on the day that is specified in the tag. Note: all resources that are marked-for-op up to and including the current date will be filtered in when utilizing the marked-for-op filter. ``` {.yaml} - name: ebs-mark-unattached-deletion resource: ebs comments: | Mark any unattached EBS volumes for deletion in 30 days. Volumes set to not delete on instance termination do have valid use cases as data drives, but 99% of the time they appear to be just garbage creation. filters: - Attachments: [] - \"tag:maid_status\": absent actions: - type: mark-for-op op: delete days: 30 name: ebs-unmark-attached-deletion resource: ebs comments: | Unmark any attached EBS volumes that were scheduled for deletion if they are currently attached filters: type: value key: \"Attachments[0].Device\" value: not-null \"tag:maid_status\": not-null actions: unmark name: ebs-delete-marked resource: ebs comments: | Delete any attached EBS volumes that were scheduled for deletion filters: type: marked-for-op op: delete actions: delete ```","title":"EBS - Garbage Collect Unattached Volumes"},{"location":"aws/examples/ebssnapshots/","text":"EBS - Create and Manage Snapshots {#ebssnapshots} The following example policy will snapshot all EBS volumes attached to EC2 instances and copy the instances tags to the snapshot. Then when the snapshots are 7 days old they will get deleted so you always have a rolling 7 days worth of snapshots. policies: - name: ec2-create-ebs-snapshots resource: ec2 actions: - type: snapshot copy-tags: - CreatorName - \"Resource Contact\" - \"Resource Purpose\" - Environment - \"Billing Cost Center\" - Name tags: CloudCustodian: true - name: ebs-delete-old-ebs-snapshots resource: ebs-snapshot filters: - type: age days: 7 op: ge - \"tag:custodian_snapshot\": present actions: - delete","title":"Ebssnapshots"},{"location":"aws/examples/ebssnapshots/#ebs-create-and-manage-snapshots-ebssnapshots","text":"The following example policy will snapshot all EBS volumes attached to EC2 instances and copy the instances tags to the snapshot. Then when the snapshots are 7 days old they will get deleted so you always have a rolling 7 days worth of snapshots. policies: - name: ec2-create-ebs-snapshots resource: ec2 actions: - type: snapshot copy-tags: - CreatorName - \"Resource Contact\" - \"Resource Purpose\" - Environment - \"Billing Cost Center\" - Name tags: CloudCustodian: true - name: ebs-delete-old-ebs-snapshots resource: ebs-snapshot filters: - type: age days: 7 op: ge - \"tag:custodian_snapshot\": present actions: - delete","title":"EBS - Create and Manage Snapshots {#ebssnapshots}"},{"location":"aws/examples/ebsunencrypted/","text":"EBS - Delete Unencrypted policies: - name: terminate-unencrypted-ebs description: | Terminate all unencrypted EBS volumes upon creation resource: ebs mode: type: cloudtrail events: - CreateVolume filters: - Encrypted: false actions: - delete","title":"Ebsunencrypted"},{"location":"aws/examples/ebsunencrypted/#ebs-delete-unencrypted","text":"policies: - name: terminate-unencrypted-ebs description: | Terminate all unencrypted EBS volumes upon creation resource: ebs mode: type: cloudtrail events: - CreateVolume filters: - Encrypted: false actions: - delete","title":"EBS - Delete Unencrypted"},{"location":"aws/examples/ec2-auto-tag-user/","text":"EC2 - auto-tag aws userName on resources Note that this can work for other resources besides EC2, and the principalId is optional. principalId tag is useful if you want to enforce users not being able to shut down each others VMs unless their principalId matches (meaning they originally spun up the resource). Documentation about principalId here: https://aws.amazon.com/blogs/security/how-to-automatically-tag-amazon-ec2-resources-in-response-to-api-events/ {.yaml} policies: - name: ec2-auto-tag-user resource: ec2 mode: type: cloudtrail role: arn:aws:iam::{account_id}:role/custodian-auto-tagger # note {account_id} is optional. If you put that there instead of # your actual account number, when the policy is provisioned it # will automatically inherit the account_id properly events: - RunInstances filters: - tag:CreatorName: absent actions: - type: auto-tag-user tag: CreatorName principal_id_tag: CreatorId","title":"Ec2 auto tag user"},{"location":"aws/examples/ec2-auto-tag-user/#ec2-auto-tag-aws-username-on-resources","text":"Note that this can work for other resources besides EC2, and the principalId is optional. principalId tag is useful if you want to enforce users not being able to shut down each others VMs unless their principalId matches (meaning they originally spun up the resource). Documentation about principalId here: https://aws.amazon.com/blogs/security/how-to-automatically-tag-amazon-ec2-resources-in-response-to-api-events/ {.yaml} policies: - name: ec2-auto-tag-user resource: ec2 mode: type: cloudtrail role: arn:aws:iam::{account_id}:role/custodian-auto-tagger # note {account_id} is optional. If you put that there instead of # your actual account number, when the policy is provisioned it # will automatically inherit the account_id properly events: - RunInstances filters: - tag:CreatorName: absent actions: - type: auto-tag-user tag: CreatorName principal_id_tag: CreatorId","title":"EC2 - auto-tag aws userName on resources"},{"location":"aws/examples/ec2offhours/","text":"EC2 - Offhours Support {#ec2offhours} Offhours are based on current time of the machine that is running custodian. Note, in this case you could tag an instance with the following two tags: StopAfterHours: off=(M-F,18);tz=est; and StartAfterHours: on=(M-F,8) . This would have the instance turn off every weekday at 6pm NY time, and turn on every day at 8am California time (since if no tz is set, it uses the default which is pt). Note when custodian runs, if it\\'s 6:00pm or 6:59 pm NY time, it will shut down the VM you tagged this way. The key is the hour integer on the NY clock matching 18. If custodian runs at 5:59pm or 7:00pm NY time, it won\\'t shut down the VM. Same idea for starting. The reason we filter for only seeing instances older than 1 hour, if a dev is on a VM that is shut down by the off hours schedule, and they turn it back on, if we run custodian again we don\\'t want to keep shutting down the VM on the dev repeatedly. policies: - name: stop-after-hours resource: ec2 filters: - type: offhour tag: CustodianOffHours default_tz: pt offhour: 19 - type: instance-age hours: 1 actions: - stop - name: start-after-hours resource: ec2 filters: - type: onhour tag: CustodianOffHours default_tz: pt onhour: 7 - type: value value: 1 key: LaunchTime op: less-than value_type: age actions: - start For detailed information on offhours/onhours support and configuration, see offhours {.interpreted-text role=\"ref\"}.","title":"Ec2offhours"},{"location":"aws/examples/ec2offhours/#ec2-offhours-support-ec2offhours","text":"Offhours are based on current time of the machine that is running custodian. Note, in this case you could tag an instance with the following two tags: StopAfterHours: off=(M-F,18);tz=est; and StartAfterHours: on=(M-F,8) . This would have the instance turn off every weekday at 6pm NY time, and turn on every day at 8am California time (since if no tz is set, it uses the default which is pt). Note when custodian runs, if it\\'s 6:00pm or 6:59 pm NY time, it will shut down the VM you tagged this way. The key is the hour integer on the NY clock matching 18. If custodian runs at 5:59pm or 7:00pm NY time, it won\\'t shut down the VM. Same idea for starting. The reason we filter for only seeing instances older than 1 hour, if a dev is on a VM that is shut down by the off hours schedule, and they turn it back on, if we run custodian again we don\\'t want to keep shutting down the VM on the dev repeatedly. policies: - name: stop-after-hours resource: ec2 filters: - type: offhour tag: CustodianOffHours default_tz: pt offhour: 19 - type: instance-age hours: 1 actions: - stop - name: start-after-hours resource: ec2 filters: - type: onhour tag: CustodianOffHours default_tz: pt onhour: 7 - type: value value: 1 key: LaunchTime op: less-than value_type: age actions: - start For detailed information on offhours/onhours support and configuration, see offhours {.interpreted-text role=\"ref\"}.","title":"EC2 - Offhours Support {#ec2offhours}"},{"location":"aws/examples/ec2oldinstances/","text":"EC2 - Old Instance Report - name: ec2-old-instances resource: ec2 comment: | Report running instances older than 60 days filters: - \"State.Name\": running - type: instance-age days: 60 # Use Case: Report all AMIs that are 120+ days or older - name: ancient-images-report resource: ami comment: | Report on all images older than 90 days which should be de-registered. filters: - type: image-age days: 120 Instance Age Filter : The instance age filter allows for filtering the set of EC2 instances by their LaunchTime, i.e. all instances older than 60 or 90 days. The default date value is 60 days if otherwise unspecified. Configuring a specific value for instance-age to report all instances older than 90 days. ``` {.yaml} policies: - name: old-instances resource: ec2 filters: - type: instance-age days: 90 ```","title":"Ec2oldinstances"},{"location":"aws/examples/ec2oldinstances/#ec2-old-instance-report","text":"- name: ec2-old-instances resource: ec2 comment: | Report running instances older than 60 days filters: - \"State.Name\": running - type: instance-age days: 60 # Use Case: Report all AMIs that are 120+ days or older - name: ancient-images-report resource: ami comment: | Report on all images older than 90 days which should be de-registered. filters: - type: image-age days: 120 Instance Age Filter : The instance age filter allows for filtering the set of EC2 instances by their LaunchTime, i.e. all instances older than 60 or 90 days. The default date value is 60 days if otherwise unspecified. Configuring a specific value for instance-age to report all instances older than 90 days. ``` {.yaml} policies: - name: old-instances resource: ec2 filters: - type: instance-age days: 90 ```","title":"EC2 - Old Instance Report"},{"location":"aws/examples/ec2poweronstoppedforpatching/","text":"EC2 - Power On For Scheduled Patching {#ec2poweronstoppedforpatching} The following example policies will automatically create CloudWatch cron rate triggered Lambda functions in your account and region. The Lambda functions will be triggered on the cron rate expression schedule you provide in the mode section of the policy. The following example policies find all EC2 instances that are both in a stopped state, and have a tag called Patch Group with a value of Linux Dev . Those instances are then started and tagged with an additional tag of PowerOffWhenDone and a value of True so that they can be stopped again after the patching window. Then all instances with the Linux Dev Patch Group get another tag called PatchingInProgress with a value of True . The PatchingInProgress tag can be used by other policies such as offhours policies where the presence of that tag would exclude it from being stopped by the offhours. When the patching window is done the last 2 policies in this example will remove the PatchingInProgress tag from all instances in that group and remove the PowerOffWhenDone tag and stop those instances that were previously stopped. The cron expressions for this example read as the following: cron(0 3 ? 1/1 SUN#1 *) means trigger on the 1st Sunday of every month at 3:00 UTC then cron(0 13 ? 1/1 SUN#1 *) is the same day at 13:00 UTC which allows for a 10 Hour patching window. Learn more on AWS cron rate expressions https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/ScheduledEvents.html policies: - name: power-on-patch-group-linux-dev resource: ec2 mode: type: periodic schedule: \"cron(0 3 ? 1/1 SUN#1 *)\" filters: - \"State.Name\": stopped - type: value key: tag:Patch Group op: eq value: \"Linux Dev\" actions: - start - type: tag key: PowerOffWhenDone value: \"True\" - name: patching-exception-tag-linux-dev resource: ec2 mode: type: periodic schedule: \"cron(0 3 ? 1/1 SUN#1 *)\" filters: - type: value key: tag:Patch Group op: eq value: \"Linux Dev\" actions: - type: tag key: PatchingInProgress value: \"True\" - name: patching-exception-removal-linux-dev resource: ec2 mode: type: periodic schedule: \"cron(0 13 ? 1/1 SUN#1 *)\" filters: - type: value key: tag:Patch Group op: eq value: \"Linux Dev\" actions: - type: unmark tags: [\"PatchingInProgress\"] - name: power-down-patch-group-linux-dev resource: ec2 mode: type: periodic schedule: \"cron(0 13 ? 1/1 SUN#1 *)\" filters: - \"State.Name\": running - \"tag:PowerOffWhenDone\": present - type: value key: tag:Patch Group op: eq value: \"Linux Dev\" actions: - stop - type: unmark tags: [\"PowerOffWhenDone\"] Note that the notify action requires the cloud custodian mailer tool to be installed.","title":"Ec2poweronstoppedforpatching"},{"location":"aws/examples/ec2poweronstoppedforpatching/#ec2-power-on-for-scheduled-patching-ec2poweronstoppedforpatching","text":"The following example policies will automatically create CloudWatch cron rate triggered Lambda functions in your account and region. The Lambda functions will be triggered on the cron rate expression schedule you provide in the mode section of the policy. The following example policies find all EC2 instances that are both in a stopped state, and have a tag called Patch Group with a value of Linux Dev . Those instances are then started and tagged with an additional tag of PowerOffWhenDone and a value of True so that they can be stopped again after the patching window. Then all instances with the Linux Dev Patch Group get another tag called PatchingInProgress with a value of True . The PatchingInProgress tag can be used by other policies such as offhours policies where the presence of that tag would exclude it from being stopped by the offhours. When the patching window is done the last 2 policies in this example will remove the PatchingInProgress tag from all instances in that group and remove the PowerOffWhenDone tag and stop those instances that were previously stopped. The cron expressions for this example read as the following: cron(0 3 ? 1/1 SUN#1 *) means trigger on the 1st Sunday of every month at 3:00 UTC then cron(0 13 ? 1/1 SUN#1 *) is the same day at 13:00 UTC which allows for a 10 Hour patching window. Learn more on AWS cron rate expressions https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/ScheduledEvents.html policies: - name: power-on-patch-group-linux-dev resource: ec2 mode: type: periodic schedule: \"cron(0 3 ? 1/1 SUN#1 *)\" filters: - \"State.Name\": stopped - type: value key: tag:Patch Group op: eq value: \"Linux Dev\" actions: - start - type: tag key: PowerOffWhenDone value: \"True\" - name: patching-exception-tag-linux-dev resource: ec2 mode: type: periodic schedule: \"cron(0 3 ? 1/1 SUN#1 *)\" filters: - type: value key: tag:Patch Group op: eq value: \"Linux Dev\" actions: - type: tag key: PatchingInProgress value: \"True\" - name: patching-exception-removal-linux-dev resource: ec2 mode: type: periodic schedule: \"cron(0 13 ? 1/1 SUN#1 *)\" filters: - type: value key: tag:Patch Group op: eq value: \"Linux Dev\" actions: - type: unmark tags: [\"PatchingInProgress\"] - name: power-down-patch-group-linux-dev resource: ec2 mode: type: periodic schedule: \"cron(0 13 ? 1/1 SUN#1 *)\" filters: - \"State.Name\": running - \"tag:PowerOffWhenDone\": present - type: value key: tag:Patch Group op: eq value: \"Linux Dev\" actions: - stop - type: unmark tags: [\"PowerOffWhenDone\"] Note that the notify action requires the cloud custodian mailer tool to be installed.","title":"EC2 - Power On For Scheduled Patching {#ec2poweronstoppedforpatching}"},{"location":"aws/examples/ec2unpatchedworkflow/","text":"EC2 - Terminate Unpatchable Instances {#ec2unpatchedworkflow} The following example policy workflow uses the mark-for-op and marked-for-op filters and actions to chain together a set of policies to accomplish a task. In this example it will find and tag any instances that are in a stopped state. The example specifies a custom tag called c7n_stopped_instance and the value of the tag will be an op action of terminate for 60 days in the future. The reasoning behind terminating unpatchable instances is after 60 days the instance will be far enough behind on patching and virus defs(if used) that starting the instance after 60 days would present too large of a security risk. Note the use of the skew option with the marked-for-op filter in some of the policies to notify the resource owners X number of days ahead of the scheduled marked-for-op action date. policies: - name: ec2-mark-stopped-instance resource: ec2 description: | Mark any stopped ec2 instance for deletion in 60 days If an instance has not been started for 60 days or over then they will be deleted similar to internal policies as it wont be patched. filters: - \"tag:c7n_stopped_instance\": absent - \"State.Name\": stopped actions: - type: mark-for-op tag: c7n_stopped_instance op: terminate days: 60 - name: ec2-unmark-previously-stopped resource: ec2 description: | Unmark/untag any ec2 instance that was scheduled for deletion due to being stopped if they are currently running. filters: - \"State.Name\": running - \"tag:c7n_stopped_instance\": present actions: - type: unmark tags: [\"c7n_stopped_instance\"] - name: ec2-notify-before-delete-marked-14-days resource: ec2 description: | Notify on any ec2 instances that will be deleted in 14 days if not started comments: | Your EC2 server will be terminated in 14 days if not started and patched by then. Please start your stopped servers and leave them on for 24 hours minimum to allow for patching to occur. filters: - type: marked-for-op tag: c7n_stopped_instance op: terminate skew: 14 actions: - type: notify template: default.html priority_header: 2 subject: \"EC2 Stopped Instance Termination Scheduled! [custodian {{ account }} - {{ region }}]\" violation_desc: \"EC2(s) have been in a stopped state for 45 days and at 60 days will be termianted:\" action_desc: | Your EC2 server will be terminated in 14 days if not started and patched by then. Please start your stopped servers and leave them on for 24 hours minimum to allow for patching to occur. to: - CloudCustodian@Company.com - resource-owner transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/12345678900/cloud-custodian-mailer region: us-east-1 - name: ec2-notify-before-delete-marked-7-days resource: ec2 description: | Notify on any ec2 instances that will be deleted in 7 days if not started filters: - type: marked-for-op tag: c7n_stopped_instance op: terminate skew: 7 actions: - type: notify template: default.html priority_header: 1 subject: \"EC2 Stopped Instance Termination Scheduled! [custodian {{ account }} - {{ region }}]\" violation_desc: \"EC2(s) have been in a stopped state for 53 days and at 60 days will be termianted:\" action_desc: | Your EC2 server will be terminated in 7 days if not started and patched by then. Please start your stopped servers and leave them on for 24 hours minimum to allow for patching to occur. to: - CloudCustodian@Company.com - resource-owner transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/12345678900/cloud-custodian-mailer region: us-east-1 - name: ec2-delete-marked resource: ec2 description: | Terminate and notify on any ec2 instances that were scheduled for deletion if its been stopped for 60 days and no longer up-to-date on patching. filters: - type: marked-for-op tag: c7n_stopped_instance op: terminate actions: - type: terminate force: true - type: notify template: default.html priority_header: 1 subject: \"EC2 Stopped Instance Terminated [custodian {{ account }} - {{ region }}]\" violation_desc: \"EC2(s) had been stopped for 60 days and have now been terminated:\" action_desc: | Your EC2 server has been terminated as its patching is too far out-of-date and beyond the 60 day window. to: - CloudCustodian@Company.com - resource-owner transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/12345678900/cloud-custodian-mailer region: us-east-1","title":"Ec2unpatchedworkflow"},{"location":"aws/examples/ec2unpatchedworkflow/#ec2-terminate-unpatchable-instances-ec2unpatchedworkflow","text":"The following example policy workflow uses the mark-for-op and marked-for-op filters and actions to chain together a set of policies to accomplish a task. In this example it will find and tag any instances that are in a stopped state. The example specifies a custom tag called c7n_stopped_instance and the value of the tag will be an op action of terminate for 60 days in the future. The reasoning behind terminating unpatchable instances is after 60 days the instance will be far enough behind on patching and virus defs(if used) that starting the instance after 60 days would present too large of a security risk. Note the use of the skew option with the marked-for-op filter in some of the policies to notify the resource owners X number of days ahead of the scheduled marked-for-op action date. policies: - name: ec2-mark-stopped-instance resource: ec2 description: | Mark any stopped ec2 instance for deletion in 60 days If an instance has not been started for 60 days or over then they will be deleted similar to internal policies as it wont be patched. filters: - \"tag:c7n_stopped_instance\": absent - \"State.Name\": stopped actions: - type: mark-for-op tag: c7n_stopped_instance op: terminate days: 60 - name: ec2-unmark-previously-stopped resource: ec2 description: | Unmark/untag any ec2 instance that was scheduled for deletion due to being stopped if they are currently running. filters: - \"State.Name\": running - \"tag:c7n_stopped_instance\": present actions: - type: unmark tags: [\"c7n_stopped_instance\"] - name: ec2-notify-before-delete-marked-14-days resource: ec2 description: | Notify on any ec2 instances that will be deleted in 14 days if not started comments: | Your EC2 server will be terminated in 14 days if not started and patched by then. Please start your stopped servers and leave them on for 24 hours minimum to allow for patching to occur. filters: - type: marked-for-op tag: c7n_stopped_instance op: terminate skew: 14 actions: - type: notify template: default.html priority_header: 2 subject: \"EC2 Stopped Instance Termination Scheduled! [custodian {{ account }} - {{ region }}]\" violation_desc: \"EC2(s) have been in a stopped state for 45 days and at 60 days will be termianted:\" action_desc: | Your EC2 server will be terminated in 14 days if not started and patched by then. Please start your stopped servers and leave them on for 24 hours minimum to allow for patching to occur. to: - CloudCustodian@Company.com - resource-owner transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/12345678900/cloud-custodian-mailer region: us-east-1 - name: ec2-notify-before-delete-marked-7-days resource: ec2 description: | Notify on any ec2 instances that will be deleted in 7 days if not started filters: - type: marked-for-op tag: c7n_stopped_instance op: terminate skew: 7 actions: - type: notify template: default.html priority_header: 1 subject: \"EC2 Stopped Instance Termination Scheduled! [custodian {{ account }} - {{ region }}]\" violation_desc: \"EC2(s) have been in a stopped state for 53 days and at 60 days will be termianted:\" action_desc: | Your EC2 server will be terminated in 7 days if not started and patched by then. Please start your stopped servers and leave them on for 24 hours minimum to allow for patching to occur. to: - CloudCustodian@Company.com - resource-owner transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/12345678900/cloud-custodian-mailer region: us-east-1 - name: ec2-delete-marked resource: ec2 description: | Terminate and notify on any ec2 instances that were scheduled for deletion if its been stopped for 60 days and no longer up-to-date on patching. filters: - type: marked-for-op tag: c7n_stopped_instance op: terminate actions: - type: terminate force: true - type: notify template: default.html priority_header: 1 subject: \"EC2 Stopped Instance Terminated [custodian {{ account }} - {{ region }}]\" violation_desc: \"EC2(s) had been stopped for 60 days and have now been terminated:\" action_desc: | Your EC2 server has been terminated as its patching is too far out-of-date and beyond the 60 day window. to: - CloudCustodian@Company.com - resource-owner transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/12345678900/cloud-custodian-mailer region: us-east-1","title":"EC2 - Terminate Unpatchable Instances {#ec2unpatchedworkflow}"},{"location":"aws/examples/eipgarbagecollect/","text":"EIP - Garbage Collect Unattached Elastic IPs Use the mark-for-op action to mark a resource for action later. One common pattern to follow is to mark a resource with an operation (example: release) in n days. In the subsequent days leading up to the marked date, run a unmark or untag policy if the resource has become compliant in the mean time. You can use this principle to implement garbage collection on resources. In this example, Custodian will first mark all unattached Elastic IPs for removal. The next policy will then unmark any EIP that has been attached and has the maid_status tag, indicating that it had been previously marked. Finally, the third policy will filter in any resources that have been marked and run the release action. It is important to note that the release policy will need to be run on the day that the resource is marked for, else the resource will still exist in the account. The mark operation only tags the resource with metadata about the upcoming operation. Operationally, the policy still must be executed on the day that is specified in the tag. Note: all resources that are marked-for-op up to and including the current date will be filtered in when utilizing the marked-for-op filter. ``` {.yaml} vars: notify: &notify type: notify to: - slack://#slack-channel subject: \"EIP - No Instances Attached - [custodian {{ account }} - {{ region }}]\" transport: type: sqs queue: https://sqs.us-east-2.amazonaws.com/123456789012/mailer region: us-east-2 run_mode: &run_mode type: periodic schedule: \"rate(1 day)\" tags: app: \"c7n\" env: \"tools\" account: \"{account_id}\" eip_filters: &eip_filters - type: value key: InstanceId value: absent - type: value key: AssociationId value: absent policies: - name: unused-eip-mark resource: network-addr description: \"Mark any EIP with no instances attached for action in 7 days\" filters: - \"tag:maid_status_eip\": absent - and: eip_filters mode: <<: run_mode actions: - type: mark-for-op tag: maid_status_eip days: 7 op: release name: unused-eip-unmark-if-in-use resource: network-addr description: | Remove the maid_status_eip tag from any eip which has instances attached filters: \"tag:maid_status_eip\": not-null not: or: eip_filters mode: <<: run_mode actions: type: remove-tag tags: [maid_status_eip] name: unused-eip-action resource: network-addr description: \"Release EIP after 7 days of having no instances\" filters: \"tag:maid_status_eip\": not-null type: marked-for-op op: release tag: maid_status_eip mode: <<: *run_mode actions: type: release <<: *notify action_desc: \"EIP released\" violation_desc: \"EIP has been unused for 7 days\" ```","title":"Eipgarbagecollect"},{"location":"aws/examples/eipgarbagecollect/#eip-garbage-collect-unattached-elastic-ips","text":"Use the mark-for-op action to mark a resource for action later. One common pattern to follow is to mark a resource with an operation (example: release) in n days. In the subsequent days leading up to the marked date, run a unmark or untag policy if the resource has become compliant in the mean time. You can use this principle to implement garbage collection on resources. In this example, Custodian will first mark all unattached Elastic IPs for removal. The next policy will then unmark any EIP that has been attached and has the maid_status tag, indicating that it had been previously marked. Finally, the third policy will filter in any resources that have been marked and run the release action. It is important to note that the release policy will need to be run on the day that the resource is marked for, else the resource will still exist in the account. The mark operation only tags the resource with metadata about the upcoming operation. Operationally, the policy still must be executed on the day that is specified in the tag. Note: all resources that are marked-for-op up to and including the current date will be filtered in when utilizing the marked-for-op filter. ``` {.yaml} vars: notify: &notify type: notify to: - slack://#slack-channel subject: \"EIP - No Instances Attached - [custodian {{ account }} - {{ region }}]\" transport: type: sqs queue: https://sqs.us-east-2.amazonaws.com/123456789012/mailer region: us-east-2 run_mode: &run_mode type: periodic schedule: \"rate(1 day)\" tags: app: \"c7n\" env: \"tools\" account: \"{account_id}\" eip_filters: &eip_filters - type: value key: InstanceId value: absent - type: value key: AssociationId value: absent policies: - name: unused-eip-mark resource: network-addr description: \"Mark any EIP with no instances attached for action in 7 days\" filters: - \"tag:maid_status_eip\": absent - and: eip_filters mode: <<: run_mode actions: - type: mark-for-op tag: maid_status_eip days: 7 op: release name: unused-eip-unmark-if-in-use resource: network-addr description: | Remove the maid_status_eip tag from any eip which has instances attached filters: \"tag:maid_status_eip\": not-null not: or: eip_filters mode: <<: run_mode actions: type: remove-tag tags: [maid_status_eip] name: unused-eip-action resource: network-addr description: \"Release EIP after 7 days of having no instances\" filters: \"tag:maid_status_eip\": not-null type: marked-for-op op: release tag: maid_status_eip mode: <<: *run_mode actions: type: release <<: *notify action_desc: \"EIP released\" violation_desc: \"EIP has been unused for 7 days\" ```","title":"EIP - Garbage Collect Unattached Elastic IPs"},{"location":"aws/examples/elbdeleteinetfacing/","text":"ELB - Delete New Internet-Facing ELBs {#elbdeleteinetfacing} The following example policy will automatically create a CloudWatch Event Rule triggered Lambda function in your account and region which will be triggered anytime a user creates a new classic Elastic Load Balancer. If the ELB is set to be internet-facing then delete it right away at launch. This provides near real-time auto-remediation (typically within 1-2 mins) of the ELB being created. Having such a quick auto-remediation action greatly reduces an attack window! By notifying the customer who tried to perform the action it helps drive user behaviour as well and lets them know why their ELBs keep deleting at launch! ;) policies: - name: elb-delete-new-internet-facing resource: elb mode: type: cloudtrail events: - CreateLoadBalancer description: | Any newly created Classic Load Balanacers launched with a internet-facing schema will be deleted right away. filters: - type: event key: \"detail.requestParameters.scheme\" op: eq value: \"internet-facing\" actions: - delete - type: notify template: default.html priority_header: 1 subject: \"Deleted New Internet-Facing ELB - [custodian {{ account }} - {{ region }}]\" violation_desc: \"Internet-Facing ELBs are not allowed and are deleted at launch.\" action_desc: | \"Actions Taken: Your new ELB has been deleted. Please launch a new non-internet-facing ELB\" to: - CloudCustodian@Company.com - event-owner transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/12345678900/cloud-custodian-mailer region: us-east-1 By including - event-owner in the notify\\'s to: field it tells Cloud Custodian to extract the id of the user who made the API call for the event and email them. Being that the above policy runs in a cloudtrail mode the API call\\'s metadata event is present which is why the example uses event-owner. If you were to remove the mode: statement on the example policy and run it in a poll mode instead you could change - event-owner to - resource-owner which would rely on the resources tags for a id or email to send the notification to as no API event would be available at that time. Note that the notify action requires the cloud custodian mailer tool to be installed.","title":"Elbdeleteinetfacing"},{"location":"aws/examples/elbdeleteinetfacing/#elb-delete-new-internet-facing-elbs-elbdeleteinetfacing","text":"The following example policy will automatically create a CloudWatch Event Rule triggered Lambda function in your account and region which will be triggered anytime a user creates a new classic Elastic Load Balancer. If the ELB is set to be internet-facing then delete it right away at launch. This provides near real-time auto-remediation (typically within 1-2 mins) of the ELB being created. Having such a quick auto-remediation action greatly reduces an attack window! By notifying the customer who tried to perform the action it helps drive user behaviour as well and lets them know why their ELBs keep deleting at launch! ;) policies: - name: elb-delete-new-internet-facing resource: elb mode: type: cloudtrail events: - CreateLoadBalancer description: | Any newly created Classic Load Balanacers launched with a internet-facing schema will be deleted right away. filters: - type: event key: \"detail.requestParameters.scheme\" op: eq value: \"internet-facing\" actions: - delete - type: notify template: default.html priority_header: 1 subject: \"Deleted New Internet-Facing ELB - [custodian {{ account }} - {{ region }}]\" violation_desc: \"Internet-Facing ELBs are not allowed and are deleted at launch.\" action_desc: | \"Actions Taken: Your new ELB has been deleted. Please launch a new non-internet-facing ELB\" to: - CloudCustodian@Company.com - event-owner transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/12345678900/cloud-custodian-mailer region: us-east-1 By including - event-owner in the notify\\'s to: field it tells Cloud Custodian to extract the id of the user who made the API call for the event and email them. Being that the above policy runs in a cloudtrail mode the API call\\'s metadata event is present which is why the example uses event-owner. If you were to remove the mode: statement on the example policy and run it in a poll mode instead you could change - event-owner to - resource-owner which would rely on the resources tags for a id or email to send the notification to as no API event would be available at that time. Note that the notify action requires the cloud custodian mailer tool to be installed.","title":"ELB - Delete New Internet-Facing ELBs {#elbdeleteinetfacing}"},{"location":"aws/examples/elbgarbagecollection/","text":"ELB - Delete Unused Elastic Load Balancers {#elbgarbagecollection} The following example policy workflow uses the mark-for-op and marked-for-op filters and actions to chain together a set of policies to accomplish a task. In this example it will find any ELB that isn\\'t attached to any instances and tag it with a delete op and date 14 days out. The policy workflow will also email the ELB resource owner to inform them of the upcoming deletion if the ELB remains unused. If the customer adds an instance back to their ELB it will get unmarked so it doesn\\'t get deleted. Note the use of the notify action requires the Cloud Custodian mailer to be installed and configured. policies: - name: elb-mark-unused-for-deletion resource: elb description: | Mark any ELB with no instances attached for deletion in 14 days. Also send an email to the ELB resource owner informing them its unused. filters: - \"tag:maid_status\": absent - Instances: [] actions: - type: mark-for-op tag: maid_status op: delete days: 14 - type: notify template: default.html priority_header: 1 subject: \"ELB - No Instances Attached - [custodian {{ account }} - {{ region }}]\" violation_desc: \"No Instances Are Attached To The Following ELB(s):\" action_desc: | Actions Taken: The unused ELBs have been marked for deletion in 14 if they remain unused. If you still need the ELBs listed below, please attach instances to them, otherwise please delete them if not needed anymore. to: - CloudCustodian@Company.com - resource-owner transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/12345678900/cloud-custodian-mailer region: us-east-1 - name: elb-unmark-if-in-use resource: elb description: | Remove the maid_status tag from any elb which has instances attached so it doesn't get deleted by the following policy filters: - \"tag:maid_status\": not-null - not: - Instances: [] actions: - type: remove-tag tags: [maid_status] - name: elb-delete-unused resource: elb description: | Delete any marked ELB which has no instances attached if it has been that way for 14 days or more. filters: - type: marked-for-op op: delete actions: - delete - type: notify template: default.html priority_header: 1 subject: \"ELB - Deleted Stale ELB - [custodian {{ account }} - {{ region }}]\" violation_desc: \"No Instances Are Attached To ELB for over 14 days:\" action_desc: \"Actions Taken: The ELB has been deleted\" to: - CloudCustodian@Company.com - resource-owner transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/12345678900/cloud-custodian-mailer region: us-east-1","title":"Elbgarbagecollection"},{"location":"aws/examples/elbgarbagecollection/#elb-delete-unused-elastic-load-balancers-elbgarbagecollection","text":"The following example policy workflow uses the mark-for-op and marked-for-op filters and actions to chain together a set of policies to accomplish a task. In this example it will find any ELB that isn\\'t attached to any instances and tag it with a delete op and date 14 days out. The policy workflow will also email the ELB resource owner to inform them of the upcoming deletion if the ELB remains unused. If the customer adds an instance back to their ELB it will get unmarked so it doesn\\'t get deleted. Note the use of the notify action requires the Cloud Custodian mailer to be installed and configured. policies: - name: elb-mark-unused-for-deletion resource: elb description: | Mark any ELB with no instances attached for deletion in 14 days. Also send an email to the ELB resource owner informing them its unused. filters: - \"tag:maid_status\": absent - Instances: [] actions: - type: mark-for-op tag: maid_status op: delete days: 14 - type: notify template: default.html priority_header: 1 subject: \"ELB - No Instances Attached - [custodian {{ account }} - {{ region }}]\" violation_desc: \"No Instances Are Attached To The Following ELB(s):\" action_desc: | Actions Taken: The unused ELBs have been marked for deletion in 14 if they remain unused. If you still need the ELBs listed below, please attach instances to them, otherwise please delete them if not needed anymore. to: - CloudCustodian@Company.com - resource-owner transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/12345678900/cloud-custodian-mailer region: us-east-1 - name: elb-unmark-if-in-use resource: elb description: | Remove the maid_status tag from any elb which has instances attached so it doesn't get deleted by the following policy filters: - \"tag:maid_status\": not-null - not: - Instances: [] actions: - type: remove-tag tags: [maid_status] - name: elb-delete-unused resource: elb description: | Delete any marked ELB which has no instances attached if it has been that way for 14 days or more. filters: - type: marked-for-op op: delete actions: - delete - type: notify template: default.html priority_header: 1 subject: \"ELB - Deleted Stale ELB - [custodian {{ account }} - {{ region }}]\" violation_desc: \"No Instances Are Attached To ELB for over 14 days:\" action_desc: \"Actions Taken: The ELB has been deleted\" to: - CloudCustodian@Company.com - resource-owner transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/12345678900/cloud-custodian-mailer region: us-east-1","title":"ELB - Delete Unused Elastic Load Balancers {#elbgarbagecollection}"},{"location":"aws/examples/elbsslblacklist/","text":"ELB - SSL Blacklist - name: elb-ssl-whitelist description: | HTTPS/SSL ELBs should only have whitelisted ciphers/protocols resource: elb mode: type: cloudtrail events: - CreateLoadBalancer - CreateLoadBalancerPolicy - SetLoadBalancerPoliciesOfListener filters: - type: ssl-policy blacklist: - Protocol-TLSv1 - Protocol-TLSv1.1 - Protocol-TLSv1.2 actions: - delete","title":"Elbsslblacklist"},{"location":"aws/examples/elbsslblacklist/#elb-ssl-blacklist","text":"- name: elb-ssl-whitelist description: | HTTPS/SSL ELBs should only have whitelisted ciphers/protocols resource: elb mode: type: cloudtrail events: - CreateLoadBalancer - CreateLoadBalancerPolicy - SetLoadBalancerPoliciesOfListener filters: - type: ssl-policy blacklist: - Protocol-TLSv1 - Protocol-TLSv1.1 - Protocol-TLSv1.2 actions: - delete","title":"ELB - SSL Blacklist"},{"location":"aws/examples/elbsslwhitelist/","text":"ELB - SSL Whitelist - name: elb-ssl-whitelist description: | HTTPS/SSL ELBs should only have whitelisted ciphers/protocols resource: elb mode: type: cloudtrail events: - CreateLoadBalancer - CreateLoadBalancerPolicy - SetLoadBalancerPoliciesOfListener filters: - type: ssl-policy whitelist: &POLICY - Protocol-TLSv1 - Protocol-TLSv1.1 - Protocol-TLSv1.2 actions: - type: set-ssl-listener-policy name: CustodianEnforcedPolicy attributes: *POLICY","title":"Elbsslwhitelist"},{"location":"aws/examples/elbsslwhitelist/#elb-ssl-whitelist","text":"- name: elb-ssl-whitelist description: | HTTPS/SSL ELBs should only have whitelisted ciphers/protocols resource: elb mode: type: cloudtrail events: - CreateLoadBalancer - CreateLoadBalancerPolicy - SetLoadBalancerPoliciesOfListener filters: - type: ssl-policy whitelist: &POLICY - Protocol-TLSv1 - Protocol-TLSv1.1 - Protocol-TLSv1.2 actions: - type: set-ssl-listener-policy name: CustodianEnforcedPolicy attributes: *POLICY","title":"ELB - SSL Whitelist"},{"location":"aws/examples/iamsetrolepolicy/","text":"IAM - Manage Whether A Specific IAM Policy is Attached to Roles Attach required IAM policy to Roles without it: - name: iam-attach-policy resource: iam-role filters: - type: no-specific-managed-policy value: my-iam-policy actions: - type: set-policy state: attached arn: arn:aws:iam::123456789012:policy/my-iam-policy Detach undesired IAM policy from Roles with it: - name: iam-detach-policy resource: iam-role filters: - type: has-specific-managed-policy value: my-iam-policy actions: - type: set-policy state: detached arn: arn:aws:iam::123456789012:policy/my-iam-policy","title":"Iamsetrolepolicy"},{"location":"aws/examples/iamsetrolepolicy/#iam-manage-whether-a-specific-iam-policy-is-attached-to-roles","text":"Attach required IAM policy to Roles without it: - name: iam-attach-policy resource: iam-role filters: - type: no-specific-managed-policy value: my-iam-policy actions: - type: set-policy state: attached arn: arn:aws:iam::123456789012:policy/my-iam-policy Detach undesired IAM policy from Roles with it: - name: iam-detach-policy resource: iam-role filters: - type: has-specific-managed-policy value: my-iam-policy actions: - type: set-policy state: detached arn: arn:aws:iam::123456789012:policy/my-iam-policy","title":"IAM - Manage Whether A Specific IAM Policy is Attached to Roles"},{"location":"aws/examples/lambdaerrorsnotify/","text":"Lambda - Notify On Lambda Errors {#lambdaerrorsnotify} The following example policy will run hourly as a CloudWatch Scheduled Event triggered Lambda function. The policies filters will check each Lambdas CloudWatch Metrics for errors. If there are any errors in an hour period and the Lambda function is not tagged with Custodian_Lambda_Error_Exclude then the policy will take the action of notifying the Lambda function owner and the cloud team. These notifications can help developers by informing them if unexpected errors occur so they can be quickly addressed. For the notify action in the policy to work you must have setup the c7n_mailer tool: https://github.com/cloud-custodian/cloud-custodian/tree/master/tools/c7n_mailer Mailer Setup Guide: https://devops4solutions.com/cloud-custodian-configure-email/ policies: - name: lambda-invocation-errors resource: lambda description: | Hourly check that finds any Lambda functions that have any errors within the last hour and notifies the customer and Cloud Team. mode: type: periodic schedule: \"rate(1 hour)\" timeout: 300 tags: ResourceContact: \"cloudteam@company.com\" ResourcePurpose: \"Created by Cloud Custodian Automated Fleet Management\" Environment: prd filters: - type: metrics name: Errors days: 0.068 period: 3600 statistics: Sum op: greater-than value: 0 - not: - \"tag:Custodian_Lambda_Error_Exclude\": present actions: - type: notify template: default.html priority_header: 1 subject: \"Lambda Function Errors Occuring! - [custodian {{ account }} - {{ region }}]\" violation_desc: | \"There has been one or more code errors occuring on this lambda function in the last hour:\" action_desc: | \"Actions Taken: Please investigate this lambda function as errors reported. To exclude the below function from this scan please add a tag with a Key called Custodian_Lambda_Error_Exclude with any value to the lambda function. to: - CloudCustodian@Company.com - resource-owner transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/1234567890/cloud-custodian-mailer region: us-east-1","title":"Lambdaerrorsnotify"},{"location":"aws/examples/lambdaerrorsnotify/#lambda-notify-on-lambda-errors-lambdaerrorsnotify","text":"The following example policy will run hourly as a CloudWatch Scheduled Event triggered Lambda function. The policies filters will check each Lambdas CloudWatch Metrics for errors. If there are any errors in an hour period and the Lambda function is not tagged with Custodian_Lambda_Error_Exclude then the policy will take the action of notifying the Lambda function owner and the cloud team. These notifications can help developers by informing them if unexpected errors occur so they can be quickly addressed. For the notify action in the policy to work you must have setup the c7n_mailer tool: https://github.com/cloud-custodian/cloud-custodian/tree/master/tools/c7n_mailer Mailer Setup Guide: https://devops4solutions.com/cloud-custodian-configure-email/ policies: - name: lambda-invocation-errors resource: lambda description: | Hourly check that finds any Lambda functions that have any errors within the last hour and notifies the customer and Cloud Team. mode: type: periodic schedule: \"rate(1 hour)\" timeout: 300 tags: ResourceContact: \"cloudteam@company.com\" ResourcePurpose: \"Created by Cloud Custodian Automated Fleet Management\" Environment: prd filters: - type: metrics name: Errors days: 0.068 period: 3600 statistics: Sum op: greater-than value: 0 - not: - \"tag:Custodian_Lambda_Error_Exclude\": present actions: - type: notify template: default.html priority_header: 1 subject: \"Lambda Function Errors Occuring! - [custodian {{ account }} - {{ region }}]\" violation_desc: | \"There has been one or more code errors occuring on this lambda function in the last hour:\" action_desc: | \"Actions Taken: Please investigate this lambda function as errors reported. To exclude the below function from this scan please add a tag with a Key called Custodian_Lambda_Error_Exclude with any value to the lambda function. to: - CloudCustodian@Company.com - resource-owner transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/1234567890/cloud-custodian-mailer region: us-east-1","title":"Lambda - Notify On Lambda Errors {#lambdaerrorsnotify}"},{"location":"aws/examples/offhours/","text":"Example offhours policy {#offhours} ::: {.automodule} c7n.filters.offhours :::","title":"Offhours"},{"location":"aws/examples/offhours/#example-offhours-policy-offhours","text":"::: {.automodule} c7n.filters.offhours :::","title":"Example offhours policy {#offhours}"},{"location":"aws/examples/rdsdeleteunused/","text":"RDS - Delete Unused Databases With No Connections {#rdsdeleteunused} The following example policy workflow uses the mark-for-op and marked-for-op filters and actions to chain together a set of policies to accomplish a task. In this example it will find any RDS that is older than 14 days that has had no connections to it in the last 14 days and tag it with a delete op and date 14 days out. The policy workflow will also email the RDS resource owner to inform them of the upcoming stopping and deletion if the RDS remains unused. If a customer connects to the RDS before the 14 day window it will get unmarked so it doesn\\'t get deleted. Note the use of the notify action requires the Cloud Custodian mailer to be installed and configured. vars: metrics-filters: &metrics-filter type: metrics name: DatabaseConnections days: 14 value: 0 op: equal policies: - name: rds-unused-databases-notify-step1 resource: rds description: | Take the average number of connections over 14 days for databases that are greater than 14 days old and notify the resources owner on any unused RDS and mark for delete action in 14 days. filters: - \"tag:c7n_rds_unused\": absent - type: value value_type: age key: InstanceCreateTime value: 14 op: greater-than - <<: *metrics-filter - or: - \"tag:Resource Contact\": present - \"tag:CreatorName\": present actions: - type: mark-for-op tag: c7n_rds_unused op: delete days: 14 - type: notify template: default.html priority_header: 1 subject: \"RDS - Unused Database - [custodian {{ account }} - {{ region }}]\" violation_desc: \"RDS Instance has had no connections in the last 2 weeks and is unused:\" action_desc: | \"Actions Taken: Database deletion has been scheduled for 14 days from now. At this point we are just notifying you of the upcoming deletion if not used.\" to: - CloudCustodian@Company.com - resource-owner transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/12345678900/cloud-custodian-mailer region: us-east-1 - name: rds-unused-databases-notify-step2 resource: rds description: | Take the average number of connections over 21 days and notify on any unused RDS that have already been marked for delete filters: - \"tag:c7n_rds_unused\": present - type: marked-for-op tag: c7n_rds_unused op: delete skew: 7 - type: value value_type: age key: InstanceCreateTime value: 21 op: gte - <<: *metrics-filter - or: - \"tag:Resource Contact\": present - \"tag:CreatorName\": present actions: - type: notify template: default.html priority_header: 1 subject: \"RDS - URGENT - Unused Database - [custodian {{ account }} - {{ region }}]\" violation_desc: | \"RDS Instance has had no connections in the last 3 weeks and is unused and will be stopped hourly in 5 days (if supported by DB type) and then deleted 2 days after its stopped:\" action_desc: | \"Actions Taken: Hourly database stopping and email will occur in 5 days and deleted will occur in 7 days. At this point we are just notifying you of the upcoming stoppage and deleted if not used\" to: - resource-owner transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/12345678900/cloud-custodian-mailer region: us-east-1 - name: rds-unused-databases-stop-and-nag-hourly-step3 resource: rds mode: type: periodic schedule: \"rate(1 hour)\" timeout: 300 description: | This policy deploys a Lambda function with an hourly CloudWatch Event Schedule trigger. The policy takes the average number of connections over 26 days and stops the RDS and notifies the resource owner hourly on any of their unused databases that have already been marked for deletion. filters: - \"tag:c7n_rds_unused\": present - type: marked-for-op tag: c7n_rds_unused op: delete skew: 1 - type: value value_type: age key: InstanceCreateTime value: 26 op: gte - <<: *metrics-filter - or: - \"tag:Resource Contact\": present - \"tag:CreatorName\": present actions: - type: notify template: default.html priority_header: 1 subject: \"RDS - URGENT!!! - Unused Database! - [custodian {{ account }} - {{ region }}]\" violation_desc: | \"RDS Instance has had no connections in the last 26 days and is unused and will be deleted in less than 48 hours\" action_desc: | \"Actions Taken: Hourly Stopping of RDS and notify. Deletion will occur in less than 48 hours. Please connect to the RDS or snapshot it if you don't need it at this time.\" to: - resource-owner transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/12345678900/cloud-custodian-mailer region: us-east-1 - name: rds-unused-databases-delete-step4 resource: rds description: | Take the average number of connections over 28 days and delete any unused databases that have already been marked for delete filters: - \"tag:c7n_rds_unused\": present - type: marked-for-op tag: c7n_rds_unused op: delete - type: value value_type: age key: InstanceCreateTime value: 28 op: gte - <<: *metrics-filter - or: - \"tag:Resource Contact\": present - \"tag:CreatorName\": present actions: - type: delete skip-snapshot: true - type: notify template: default.html priority_header: 1 subject: \"RDS - URGENT!!! - Unused Database Deleted! - [custodian {{ account }} - {{ region }}]\" violation_desc: \"RDS Instance has had no connections in the last 28 days and has been deleted.\" action_desc: \"Actions Taken: RDS Instance(s) have been deleted.\" to: - CloudCustodian@Company.com - resource-owner transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/12345678900/cloud-custodian-mailer region: us-east-1 - name: rds-unused-databases-unmark resource: rds description: | The policy takes the average number of connections over 14 days and if there are connections then unmark the RDS instance and notify the resource owner. filters: - \"tag:c7n_rds_unused\": present - type: value value_type: age key: InstanceCreateTime value: 14 op: gte - type: metrics name: DatabaseConnections days: 14 value: 0 op: gt - or: - \"tag:Resource Contact\": present - \"tag:CreatorName\": present actions: - type: unmark tags: [\"c7n_rds_unused\"] - type: notify template: default.html priority_header: 1 subject: \"RDS - Previously Unused DB Unmarked! - [custodian {{ account }} - {{ region }}]\" violation_desc: | \"RDS Instance that previously had no connections for over 2 weeks is now showing connections and it has been unmarked for deletion.\" action_desc: \"Actions Taken: RDS Instance(s) have been unmarked. No further action needed\" to: - CloudCustodian@Company.com - resource-owner transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/12345678900/cloud-custodian-mailer region: us-east-1","title":"Rdsdeleteunused"},{"location":"aws/examples/rdsdeleteunused/#rds-delete-unused-databases-with-no-connections-rdsdeleteunused","text":"The following example policy workflow uses the mark-for-op and marked-for-op filters and actions to chain together a set of policies to accomplish a task. In this example it will find any RDS that is older than 14 days that has had no connections to it in the last 14 days and tag it with a delete op and date 14 days out. The policy workflow will also email the RDS resource owner to inform them of the upcoming stopping and deletion if the RDS remains unused. If a customer connects to the RDS before the 14 day window it will get unmarked so it doesn\\'t get deleted. Note the use of the notify action requires the Cloud Custodian mailer to be installed and configured. vars: metrics-filters: &metrics-filter type: metrics name: DatabaseConnections days: 14 value: 0 op: equal policies: - name: rds-unused-databases-notify-step1 resource: rds description: | Take the average number of connections over 14 days for databases that are greater than 14 days old and notify the resources owner on any unused RDS and mark for delete action in 14 days. filters: - \"tag:c7n_rds_unused\": absent - type: value value_type: age key: InstanceCreateTime value: 14 op: greater-than - <<: *metrics-filter - or: - \"tag:Resource Contact\": present - \"tag:CreatorName\": present actions: - type: mark-for-op tag: c7n_rds_unused op: delete days: 14 - type: notify template: default.html priority_header: 1 subject: \"RDS - Unused Database - [custodian {{ account }} - {{ region }}]\" violation_desc: \"RDS Instance has had no connections in the last 2 weeks and is unused:\" action_desc: | \"Actions Taken: Database deletion has been scheduled for 14 days from now. At this point we are just notifying you of the upcoming deletion if not used.\" to: - CloudCustodian@Company.com - resource-owner transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/12345678900/cloud-custodian-mailer region: us-east-1 - name: rds-unused-databases-notify-step2 resource: rds description: | Take the average number of connections over 21 days and notify on any unused RDS that have already been marked for delete filters: - \"tag:c7n_rds_unused\": present - type: marked-for-op tag: c7n_rds_unused op: delete skew: 7 - type: value value_type: age key: InstanceCreateTime value: 21 op: gte - <<: *metrics-filter - or: - \"tag:Resource Contact\": present - \"tag:CreatorName\": present actions: - type: notify template: default.html priority_header: 1 subject: \"RDS - URGENT - Unused Database - [custodian {{ account }} - {{ region }}]\" violation_desc: | \"RDS Instance has had no connections in the last 3 weeks and is unused and will be stopped hourly in 5 days (if supported by DB type) and then deleted 2 days after its stopped:\" action_desc: | \"Actions Taken: Hourly database stopping and email will occur in 5 days and deleted will occur in 7 days. At this point we are just notifying you of the upcoming stoppage and deleted if not used\" to: - resource-owner transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/12345678900/cloud-custodian-mailer region: us-east-1 - name: rds-unused-databases-stop-and-nag-hourly-step3 resource: rds mode: type: periodic schedule: \"rate(1 hour)\" timeout: 300 description: | This policy deploys a Lambda function with an hourly CloudWatch Event Schedule trigger. The policy takes the average number of connections over 26 days and stops the RDS and notifies the resource owner hourly on any of their unused databases that have already been marked for deletion. filters: - \"tag:c7n_rds_unused\": present - type: marked-for-op tag: c7n_rds_unused op: delete skew: 1 - type: value value_type: age key: InstanceCreateTime value: 26 op: gte - <<: *metrics-filter - or: - \"tag:Resource Contact\": present - \"tag:CreatorName\": present actions: - type: notify template: default.html priority_header: 1 subject: \"RDS - URGENT!!! - Unused Database! - [custodian {{ account }} - {{ region }}]\" violation_desc: | \"RDS Instance has had no connections in the last 26 days and is unused and will be deleted in less than 48 hours\" action_desc: | \"Actions Taken: Hourly Stopping of RDS and notify. Deletion will occur in less than 48 hours. Please connect to the RDS or snapshot it if you don't need it at this time.\" to: - resource-owner transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/12345678900/cloud-custodian-mailer region: us-east-1 - name: rds-unused-databases-delete-step4 resource: rds description: | Take the average number of connections over 28 days and delete any unused databases that have already been marked for delete filters: - \"tag:c7n_rds_unused\": present - type: marked-for-op tag: c7n_rds_unused op: delete - type: value value_type: age key: InstanceCreateTime value: 28 op: gte - <<: *metrics-filter - or: - \"tag:Resource Contact\": present - \"tag:CreatorName\": present actions: - type: delete skip-snapshot: true - type: notify template: default.html priority_header: 1 subject: \"RDS - URGENT!!! - Unused Database Deleted! - [custodian {{ account }} - {{ region }}]\" violation_desc: \"RDS Instance has had no connections in the last 28 days and has been deleted.\" action_desc: \"Actions Taken: RDS Instance(s) have been deleted.\" to: - CloudCustodian@Company.com - resource-owner transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/12345678900/cloud-custodian-mailer region: us-east-1 - name: rds-unused-databases-unmark resource: rds description: | The policy takes the average number of connections over 14 days and if there are connections then unmark the RDS instance and notify the resource owner. filters: - \"tag:c7n_rds_unused\": present - type: value value_type: age key: InstanceCreateTime value: 14 op: gte - type: metrics name: DatabaseConnections days: 14 value: 0 op: gt - or: - \"tag:Resource Contact\": present - \"tag:CreatorName\": present actions: - type: unmark tags: [\"c7n_rds_unused\"] - type: notify template: default.html priority_header: 1 subject: \"RDS - Previously Unused DB Unmarked! - [custodian {{ account }} - {{ region }}]\" violation_desc: | \"RDS Instance that previously had no connections for over 2 weeks is now showing connections and it has been unmarked for deletion.\" action_desc: \"Actions Taken: RDS Instance(s) have been unmarked. No further action needed\" to: - CloudCustodian@Company.com - resource-owner transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/12345678900/cloud-custodian-mailer region: us-east-1","title":"RDS - Delete Unused Databases With No Connections {#rdsdeleteunused}"},{"location":"aws/examples/rdspublicunencrypted/","text":"RDS - Terminate Unencrypted Public Instances - name: terminate-unencrypted-public-rds description: | Terminate all unencrypted or publicly available RDS upon creation resource: rds mode: type: cloudtrail events: - CreateDBInstance filters: - or: - StorageEncrypted: false - PubliclyAccessible: true actions: - type: delete skip-snapshot: true","title":"Rdspublicunencrypted"},{"location":"aws/examples/rdspublicunencrypted/#rds-terminate-unencrypted-public-instances","text":"- name: terminate-unencrypted-public-rds description: | Terminate all unencrypted or publicly available RDS upon creation resource: rds mode: type: cloudtrail events: - CreateDBInstance filters: - or: - StorageEncrypted: false - PubliclyAccessible: true actions: - type: delete skip-snapshot: true","title":"RDS - Terminate Unencrypted Public Instances"},{"location":"aws/examples/s3configurenewbucket/","text":"S3 - Configure New Buckets Settings and Standards {#s3configurenewbucket} The following example policy will automatically create a CloudWatch Event Rule triggered Lambda function in your account and region which will be triggered anytime a new S3 bucket is created in that region. The policy then applies several configurations such as enabling the default S3 AES256 bucket encryption, turns on object versioning, creates a s3 object lifecycle, enables logging on the bucket, and tags the user that created the bucket. When using the toggle-logging action as shown below you must make sure the s3 bucket the logs are getting sent to already exists. Buckets can only send logs to logging buckets in the same region as it so you may need to create multiple logging buckets per account if you use more than 1 region. In the below example the logging buckets would be named using account and region like the following: 0123456789012-us-east-1-s3-logs The S3 bucket lifecycle will help to save S3 costs by getting rid of old object versions and moving objects from standard storage class to infrequent access storage after 180 days in this example. policies: - name: s3-configure-standards-real-time resource: s3 description: | This policy is triggered when a new S3 bucket is created and it applies the AWS AES256 Default Bucket Encryption, Tags the creators ID, enables object versioning, configures the bucket lifecycle and enables logging. mode: type: cloudtrail events: - CreateBucket role: arn:aws:iam::{account_id}:role/Cloud_Custodian_S3_Lambda_Role timeout: 200 actions: - type: auto-tag-user tag: CreatorName - type: set-bucket-encryption - type: toggle-versioning enabled: true - type: toggle-logging target_bucket: \"{account_id}-{region}-s3-logs\" target_prefix: \"{source_bucket_name}/\" - type: configure-lifecycle rules: - ID: company-s3-lifecycle Status: Enabled Filter: Prefix: / Transitions: - Days: 180 StorageClass: STANDARD_IA NoncurrentVersionExpiration: NoncurrentDays: 35","title":"S3configurenewbucket"},{"location":"aws/examples/s3configurenewbucket/#s3-configure-new-buckets-settings-and-standards-s3configurenewbucket","text":"The following example policy will automatically create a CloudWatch Event Rule triggered Lambda function in your account and region which will be triggered anytime a new S3 bucket is created in that region. The policy then applies several configurations such as enabling the default S3 AES256 bucket encryption, turns on object versioning, creates a s3 object lifecycle, enables logging on the bucket, and tags the user that created the bucket. When using the toggle-logging action as shown below you must make sure the s3 bucket the logs are getting sent to already exists. Buckets can only send logs to logging buckets in the same region as it so you may need to create multiple logging buckets per account if you use more than 1 region. In the below example the logging buckets would be named using account and region like the following: 0123456789012-us-east-1-s3-logs The S3 bucket lifecycle will help to save S3 costs by getting rid of old object versions and moving objects from standard storage class to infrequent access storage after 180 days in this example. policies: - name: s3-configure-standards-real-time resource: s3 description: | This policy is triggered when a new S3 bucket is created and it applies the AWS AES256 Default Bucket Encryption, Tags the creators ID, enables object versioning, configures the bucket lifecycle and enables logging. mode: type: cloudtrail events: - CreateBucket role: arn:aws:iam::{account_id}:role/Cloud_Custodian_S3_Lambda_Role timeout: 200 actions: - type: auto-tag-user tag: CreatorName - type: set-bucket-encryption - type: toggle-versioning enabled: true - type: toggle-logging target_bucket: \"{account_id}-{region}-s3-logs\" target_prefix: \"{source_bucket_name}/\" - type: configure-lifecycle rules: - ID: company-s3-lifecycle Status: Enabled Filter: Prefix: / Transitions: - Days: 180 StorageClass: STANDARD_IA NoncurrentVersionExpiration: NoncurrentDays: 35","title":"S3 - Configure New Buckets Settings and Standards {#s3configurenewbucket}"},{"location":"aws/examples/s3denypublicobjectacls/","text":"S3 - Block Public S3 Object ACLs {#s3denypublicobjectacls} The following example policies will append a S3 bucket policy to every S3 bucket with a policy statement called DenyS3PublicObjectACL This will prevent any object in these buckets from being set to public-read, public-read-write ,or authenticated-read (Any authenticated AWS user, not just local to account). Being that S3 object permissions can be hard to track and restrict due to the huge amount of S3 objects usually present in accounts, this policy allows you to prevent the issue from occurring in the first place and helps maintain tighter S3 security to avoid accidentally setting sensitive S3 objects to public. Note the S3 bucket policy has a NotPrincipal statement with several \\\"AWS\\\": arns. These arns are owned by AWS and they are used for the AWS logging services for Log Delivery Group, ELB Logs, and Redshift Logs. The ELB and Redshift arns are region specific (sample includesus-east-1 and eu-west-1) so depending on the regions you are utilizing you might need to add or remove addtional arns found here: Redshift Log Accounts: https://docs.aws.amazon.com/redshift/latest/mgmt/db-auditing.html ELB Log Accounts: https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/enable-access-logs.html policies: - name: s3-deny-public-object-acl-poll resource: s3 description: | Appends a bucket policy statement to all existing s3 buckets to deny anyone except for the AWS Logging Services from setting s3 objects in the bucket to public-read, public-read-write, or any authenticated AWS user. actions: - type: set-statements statements: - Sid: \"DenyS3PublicObjectACL\" Effect: \"Deny\" Action: \"s3:PutObjectAcl\" NotPrincipal: \"AWS\": - \"arn:aws:iam::858827067514:root\" - \"arn:aws:iam::193672423079:user/logs\" - \"arn:aws:iam::210876761215:user/logs\" - \"arn:aws:iam::127311923021:root\" - \"arn:aws:iam::156460612806:root\" Resource: - \"arn:aws:s3:::{bucket_name}/*\" - \"arn:aws:s3:::{bucket_name}\" Condition: StringEqualsIgnoreCaseIfExists: 's3:x-amz-acl': - \"public-read\" - \"public-read-write\" - \"authenticated-read\" - name: s3-deny-public-object-acl-realtime resource: s3 mode: type: cloudtrail events: - CreateBucket - source: 's3.amazonaws.com' event: PutBucketPolicy ids: \"requestParameters.bucketName\" role: arn:aws:iam::{account_id}:role/Cloud_Custodian_Role timeout: 200 description: | Appends a bucket policy statement to an s3 bucket when it detects a policy change to the bucket or a new bucket is created which will deny anyone except some AWS logging services from setting s3 objects in the bucket to public-read, public-read-write, or any authenticated AWS user. actions: - type: set-statements statements: - Sid: \"DenyS3PublicObjectACL\" Effect: \"Deny\" Action: \"s3:PutObjectAcl\" NotPrincipal: \"AWS\": - \"arn:aws:iam::858827067514:root\" - \"arn:aws:iam::193672423079:user/logs\" - \"arn:aws:iam::210876761215:user/logs\" - \"arn:aws:iam::127311923021:root\" - \"arn:aws:iam::156460612806:root\" Resource: - \"arn:aws:s3:::{bucket_name}/*\" - \"arn:aws:s3:::{bucket_name}\" Condition: StringEqualsIgnoreCaseIfExists: 's3:x-amz-acl': - \"public-read\" - \"public-read-write\" - \"authenticated-read\"","title":"S3denypublicobjectacls"},{"location":"aws/examples/s3denypublicobjectacls/#s3-block-public-s3-object-acls-s3denypublicobjectacls","text":"The following example policies will append a S3 bucket policy to every S3 bucket with a policy statement called DenyS3PublicObjectACL This will prevent any object in these buckets from being set to public-read, public-read-write ,or authenticated-read (Any authenticated AWS user, not just local to account). Being that S3 object permissions can be hard to track and restrict due to the huge amount of S3 objects usually present in accounts, this policy allows you to prevent the issue from occurring in the first place and helps maintain tighter S3 security to avoid accidentally setting sensitive S3 objects to public. Note the S3 bucket policy has a NotPrincipal statement with several \\\"AWS\\\": arns. These arns are owned by AWS and they are used for the AWS logging services for Log Delivery Group, ELB Logs, and Redshift Logs. The ELB and Redshift arns are region specific (sample includesus-east-1 and eu-west-1) so depending on the regions you are utilizing you might need to add or remove addtional arns found here: Redshift Log Accounts: https://docs.aws.amazon.com/redshift/latest/mgmt/db-auditing.html ELB Log Accounts: https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/enable-access-logs.html policies: - name: s3-deny-public-object-acl-poll resource: s3 description: | Appends a bucket policy statement to all existing s3 buckets to deny anyone except for the AWS Logging Services from setting s3 objects in the bucket to public-read, public-read-write, or any authenticated AWS user. actions: - type: set-statements statements: - Sid: \"DenyS3PublicObjectACL\" Effect: \"Deny\" Action: \"s3:PutObjectAcl\" NotPrincipal: \"AWS\": - \"arn:aws:iam::858827067514:root\" - \"arn:aws:iam::193672423079:user/logs\" - \"arn:aws:iam::210876761215:user/logs\" - \"arn:aws:iam::127311923021:root\" - \"arn:aws:iam::156460612806:root\" Resource: - \"arn:aws:s3:::{bucket_name}/*\" - \"arn:aws:s3:::{bucket_name}\" Condition: StringEqualsIgnoreCaseIfExists: 's3:x-amz-acl': - \"public-read\" - \"public-read-write\" - \"authenticated-read\" - name: s3-deny-public-object-acl-realtime resource: s3 mode: type: cloudtrail events: - CreateBucket - source: 's3.amazonaws.com' event: PutBucketPolicy ids: \"requestParameters.bucketName\" role: arn:aws:iam::{account_id}:role/Cloud_Custodian_Role timeout: 200 description: | Appends a bucket policy statement to an s3 bucket when it detects a policy change to the bucket or a new bucket is created which will deny anyone except some AWS logging services from setting s3 objects in the bucket to public-read, public-read-write, or any authenticated AWS user. actions: - type: set-statements statements: - Sid: \"DenyS3PublicObjectACL\" Effect: \"Deny\" Action: \"s3:PutObjectAcl\" NotPrincipal: \"AWS\": - \"arn:aws:iam::858827067514:root\" - \"arn:aws:iam::193672423079:user/logs\" - \"arn:aws:iam::210876761215:user/logs\" - \"arn:aws:iam::127311923021:root\" - \"arn:aws:iam::156460612806:root\" Resource: - \"arn:aws:s3:::{bucket_name}/*\" - \"arn:aws:s3:::{bucket_name}\" Condition: StringEqualsIgnoreCaseIfExists: 's3:x-amz-acl': - \"public-read\" - \"public-read-write\" - \"authenticated-read\"","title":"S3 - Block Public S3 Object ACLs {#s3denypublicobjectacls}"},{"location":"aws/examples/s3encryption/","text":"S3 - Encryption Enable Bucket Encryption The following policy will enable bucket encryption on all s3 buckets. policies: - name: s3-set-bucket-encryption resource: s3 actions: - type: set-bucket-encryption crypto: AES256 enabled: True Remediate Existing Will scan all keys in the bucket for unencrypted keys and by default remediate them such that they are encrypted. policies: - name: s3-key-encryption resource: s3 actions: - type: encrypt-keys crypto: aws:kms Options crypto for determining the crypto mechanism, this can either be aws:kms or AES256 (default) key-id for specifying the customer KMS key to use for the SSE, if the crypto value passed is aws:kms the AWS default KMS key will be used instead. report-only generate reports of unencrypted keys in a bucket, but do not remediate them. Remediate Incoming Note: the set-bucket-encryption action is a much more effective way of enabling encryption on a bucket. Will scan all newly created objects and remediate them such that they are encrypted. policies: - name: s3-attach-encryption resource: s3 actions: - type: attach-encrypt role: arn:aws:iam::123456789012:role/my-role topic: arn:aws:sns::123456789012:my-topic Options role for the role the encrypting Lambda should run as (not necessary if you provide --assume-role on the command line). topic for the SNS topic to subscribe the Lambda to. If you set topic to default then we will reuse any existing SNS topic that specifies s3:ObjectCreated:* , or set one up if needed. If topic is missing, then we\\'ll attach via a bucket notification. Bucket Policy Note: the set-bucket-encryption action is a much more effective way of enabling encryption on a bucket. Adds an encryption required bucket policy and merges with extant policy statements. Note filters should be used to avoid hitting any buckets that are being written to by AWS services, as these do not write encrypted and will be blocked by this policy. policies: - name: s3-encryption-policy resource: s3 actions: - encryption-policy","title":"S3encryption"},{"location":"aws/examples/s3encryption/#s3-encryption","text":"","title":"S3 - Encryption"},{"location":"aws/examples/s3encryption/#enable-bucket-encryption","text":"The following policy will enable bucket encryption on all s3 buckets. policies: - name: s3-set-bucket-encryption resource: s3 actions: - type: set-bucket-encryption crypto: AES256 enabled: True","title":"Enable Bucket Encryption"},{"location":"aws/examples/s3encryption/#remediate-existing","text":"Will scan all keys in the bucket for unencrypted keys and by default remediate them such that they are encrypted. policies: - name: s3-key-encryption resource: s3 actions: - type: encrypt-keys crypto: aws:kms","title":"Remediate Existing"},{"location":"aws/examples/s3encryption/#options","text":"crypto for determining the crypto mechanism, this can either be aws:kms or AES256 (default) key-id for specifying the customer KMS key to use for the SSE, if the crypto value passed is aws:kms the AWS default KMS key will be used instead. report-only generate reports of unencrypted keys in a bucket, but do not remediate them.","title":"Options"},{"location":"aws/examples/s3encryption/#remediate-incoming","text":"Note: the set-bucket-encryption action is a much more effective way of enabling encryption on a bucket. Will scan all newly created objects and remediate them such that they are encrypted. policies: - name: s3-attach-encryption resource: s3 actions: - type: attach-encrypt role: arn:aws:iam::123456789012:role/my-role topic: arn:aws:sns::123456789012:my-topic","title":"Remediate Incoming"},{"location":"aws/examples/s3encryption/#options_1","text":"role for the role the encrypting Lambda should run as (not necessary if you provide --assume-role on the command line). topic for the SNS topic to subscribe the Lambda to. If you set topic to default then we will reuse any existing SNS topic that specifies s3:ObjectCreated:* , or set one up if needed. If topic is missing, then we\\'ll attach via a bucket notification.","title":"Options"},{"location":"aws/examples/s3encryption/#bucket-policy","text":"Note: the set-bucket-encryption action is a much more effective way of enabling encryption on a bucket. Adds an encryption required bucket policy and merges with extant policy statements. Note filters should be used to avoid hitting any buckets that are being written to by AWS services, as these do not write encrypted and will be blocked by this policy. policies: - name: s3-encryption-policy resource: s3 actions: - encryption-policy","title":"Bucket Policy"},{"location":"aws/examples/s3globalgrants/","text":"S3 - Global Grants Scan buckets that allow for global access in their ACLs and delete the associated ACL permissions. policies: - name: s3-global-access resource: s3 filters: - type: global-grants actions: - type: delete-global-grants grantees: - \"http://acs.amazonaws.com/groups/global/AllUsers\" - \"http://acs.amazonaws.com/groups/global/AuthenticatedUsers\"","title":"S3globalgrants"},{"location":"aws/examples/s3globalgrants/#s3-global-grants","text":"Scan buckets that allow for global access in their ACLs and delete the associated ACL permissions. policies: - name: s3-global-access resource: s3 filters: - type: global-grants actions: - type: delete-global-grants grantees: - \"http://acs.amazonaws.com/groups/global/AllUsers\" - \"http://acs.amazonaws.com/groups/global/AuthenticatedUsers\"","title":"S3 - Global Grants"},{"location":"aws/examples/sagemakernotebookdeletepublicorunencrypted/","text":"SageMaker Notebook - Delete Public or Unencrypted {#sagemakernotebookdeletepublicorunencrypted} The following example policy chain will detect if new SageMaker Notebooks are internet-facing (public) or unencrypted (not using KMS) at launch and then tag, stop, and delete the notebook and email the customer and cloud custodian admin. SageMaker Notebooks cannot be deleted unless they are in a Stopped status and they cannot be stopped until they are in a InService status which is why this needs a chain of policies that will trigger in order using tags and scheduled Lambda runs. policies: - name: sagemaker-notebook-auto-tag-user resource: sagemaker-notebook description: | When a new Sagemaker notebook is created tag the creators ID to CreatorName tag mode: type: cloudtrail events: - source: sagemaker.amazonaws.com event: CreateNotebookInstance ids: \"responseElements.notebookInstanceArn\" actions: - type: auto-tag-user tag: CreatorName - name: sagemaker-notebook-tag-non-compliant resource: sagemaker-notebook description: | When a new Sagemaker Notebook is created that is public or not encrypted it will get tagged for stopping and then deletion mode: type: cloudtrail events: - source: sagemaker.amazonaws.com event: CreateNotebookInstance ids: \"responseElements.notebookInstanceArn\" filters: - or: - \"DirectInternetAccess\": \"Enabled\" - \"KmsKeyId\": absent actions: - type: tag key: NonCompliantTag value: \"TRUE\" - name: sagemaker-notebook-stop-non-compliant resource: sagemaker-notebook description: | If a SageMaker Notebook is tagged with NonCompliantTag then it gets stopped and tagged with NonCompliantTagStopped for deletion mode: type: periodic schedule: \"rate(5 minutes)\" timeout: 45 filters: - \"tag:NonCompliantTag\": \"TRUE\" - \"NotebookInstanceStatus\": \"InService\" actions: - type: tag key: NonCompliantTagStopped value: \"TRUE\" - stop - name: sagemaker-notebook-delete-non-compliant resource: sagemaker-notebook description: | When a new Sagemaker notebook is tagged as non-compliant and in a stopped state, delete it mode: type: periodic schedule: \"rate(5 minutes)\" timeout: 45 filters: - \"tag:NonCompliantTagStopped\": \"TRUE\" - \"NotebookInstanceStatus\": \"Stopped\" actions: - delete - type: notify template: default.html priority_header: 1 subject: SageMaker Notebook - Deleted! - [custodian {{ account }} - {{ region }}] violation_desc: | Public facing (Non-VPC) OR Non-Encrypted Sagemaker Notebooks Are Prohibited! All Notebooks Must Be in VPC mode and encrypted! action_desc: | Actions Taken: Your SageMaker Notebook Instance has been deleted due to being non-compliant. Please create a new SageMaker notebook in VPC mode with KMS encryption enabled. to: - CloudCustodian@Company.com - resource-owner transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/123456789123/cloud-custodian-mailer region: us-east-1","title":"Sagemakernotebookdeletepublicorunencrypted"},{"location":"aws/examples/sagemakernotebookdeletepublicorunencrypted/#sagemaker-notebook-delete-public-or-unencrypted-sagemakernotebookdeletepublicorunencrypted","text":"The following example policy chain will detect if new SageMaker Notebooks are internet-facing (public) or unencrypted (not using KMS) at launch and then tag, stop, and delete the notebook and email the customer and cloud custodian admin. SageMaker Notebooks cannot be deleted unless they are in a Stopped status and they cannot be stopped until they are in a InService status which is why this needs a chain of policies that will trigger in order using tags and scheduled Lambda runs. policies: - name: sagemaker-notebook-auto-tag-user resource: sagemaker-notebook description: | When a new Sagemaker notebook is created tag the creators ID to CreatorName tag mode: type: cloudtrail events: - source: sagemaker.amazonaws.com event: CreateNotebookInstance ids: \"responseElements.notebookInstanceArn\" actions: - type: auto-tag-user tag: CreatorName - name: sagemaker-notebook-tag-non-compliant resource: sagemaker-notebook description: | When a new Sagemaker Notebook is created that is public or not encrypted it will get tagged for stopping and then deletion mode: type: cloudtrail events: - source: sagemaker.amazonaws.com event: CreateNotebookInstance ids: \"responseElements.notebookInstanceArn\" filters: - or: - \"DirectInternetAccess\": \"Enabled\" - \"KmsKeyId\": absent actions: - type: tag key: NonCompliantTag value: \"TRUE\" - name: sagemaker-notebook-stop-non-compliant resource: sagemaker-notebook description: | If a SageMaker Notebook is tagged with NonCompliantTag then it gets stopped and tagged with NonCompliantTagStopped for deletion mode: type: periodic schedule: \"rate(5 minutes)\" timeout: 45 filters: - \"tag:NonCompliantTag\": \"TRUE\" - \"NotebookInstanceStatus\": \"InService\" actions: - type: tag key: NonCompliantTagStopped value: \"TRUE\" - stop - name: sagemaker-notebook-delete-non-compliant resource: sagemaker-notebook description: | When a new Sagemaker notebook is tagged as non-compliant and in a stopped state, delete it mode: type: periodic schedule: \"rate(5 minutes)\" timeout: 45 filters: - \"tag:NonCompliantTagStopped\": \"TRUE\" - \"NotebookInstanceStatus\": \"Stopped\" actions: - delete - type: notify template: default.html priority_header: 1 subject: SageMaker Notebook - Deleted! - [custodian {{ account }} - {{ region }}] violation_desc: | Public facing (Non-VPC) OR Non-Encrypted Sagemaker Notebooks Are Prohibited! All Notebooks Must Be in VPC mode and encrypted! action_desc: | Actions Taken: Your SageMaker Notebook Instance has been deleted due to being non-compliant. Please create a new SageMaker notebook in VPC mode with KMS encryption enabled. to: - CloudCustodian@Company.com - resource-owner transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/123456789123/cloud-custodian-mailer region: us-east-1","title":"SageMaker Notebook - Delete Public or Unencrypted {#sagemakernotebookdeletepublicorunencrypted}"},{"location":"aws/examples/securitygroupsaddpermission/","text":"Security Groups - add permission {#securitygroupsaddpermission} The following example policy will automatically create a CloudWatch Event Rule triggered Lambda function in your account and region which will be triggered anytime a user creates or modifies a security group. This provides near real-time auto-remediation action (typically within a minute) of the security group change. Having such a quick auto-remediation action greatly reduces any attack window! User defined rule is added to the filtered results. policies: - name: sg-add-permission resource: security-group description: | Add rule to a security group. Filter any security group that allows 0.0.0.0/0 or ::/0 (IPv6) ingress on port 22, remove the rule and add user defined sg rule mode: type: cloudtrail events: - source: ec2.amazonaws.com event: AuthorizeSecurityGroupIngress ids: \"requestParameters.groupId\" - source: ec2.amazonaws.com event: RevokeSecurityGroupIngress ids: \"requestParameters.groupId\" filters: - or: - type: ingress IpProtocol: \"-1\" Ports: [22] Cidr: \"0.0.0.0/0\" - type: ingress IpProtocol: \"-1\" Ports: [22] CidrV6: \"::/0\" actions: - type: set-permissions # remove the permission matched by a previous ingress filter. remove-ingress: matched # add a list of permissions to the group. add-ingress: # full syntax/parameters to authorize can be used. - IpPermissions: - IpProtocol: TCP FromPort: 22 ToPort: 22 IpRanges: - Description: Ops SSH Access CidrIp: \"1.1.1.1/32\" - Description: Security SSH Access CidrIp: \"2.2.2.2/32\"","title":"Securitygroupsaddpermission"},{"location":"aws/examples/securitygroupsaddpermission/#security-groups-add-permission-securitygroupsaddpermission","text":"The following example policy will automatically create a CloudWatch Event Rule triggered Lambda function in your account and region which will be triggered anytime a user creates or modifies a security group. This provides near real-time auto-remediation action (typically within a minute) of the security group change. Having such a quick auto-remediation action greatly reduces any attack window! User defined rule is added to the filtered results. policies: - name: sg-add-permission resource: security-group description: | Add rule to a security group. Filter any security group that allows 0.0.0.0/0 or ::/0 (IPv6) ingress on port 22, remove the rule and add user defined sg rule mode: type: cloudtrail events: - source: ec2.amazonaws.com event: AuthorizeSecurityGroupIngress ids: \"requestParameters.groupId\" - source: ec2.amazonaws.com event: RevokeSecurityGroupIngress ids: \"requestParameters.groupId\" filters: - or: - type: ingress IpProtocol: \"-1\" Ports: [22] Cidr: \"0.0.0.0/0\" - type: ingress IpProtocol: \"-1\" Ports: [22] CidrV6: \"::/0\" actions: - type: set-permissions # remove the permission matched by a previous ingress filter. remove-ingress: matched # add a list of permissions to the group. add-ingress: # full syntax/parameters to authorize can be used. - IpPermissions: - IpProtocol: TCP FromPort: 22 ToPort: 22 IpRanges: - Description: Ops SSH Access CidrIp: \"1.1.1.1/32\" - Description: Security SSH Access CidrIp: \"2.2.2.2/32\"","title":"Security Groups - add permission {#securitygroupsaddpermission}"},{"location":"aws/examples/securitygroupsdetectremediate/","text":"Security Groups - Detect and Remediate Violations {#securitygroupsdetectremediate} The following example policy will automatically create a CloudWatch Event Rule triggered Lambda function in your account and region which will be triggered anytime a user creates or modifies a security group. This provides near real-time auto-remediation action (typically within a minute) of the security group change. Having such a quick auto-remediation action greatly reduces any attack window! By notifying the customer who tried to perform the action it helps drive user behaviour and lets them know why the security group keeps reverting their 0.0.0.0/0 rule additions on them! policies: - name: high-risk-security-groups-remediate resource: security-group description: | Remove any rule from a security group that allows 0.0.0.0/0 or ::/0 (IPv6) ingress and notify the user who added the violating rule. mode: type: cloudtrail events: - source: ec2.amazonaws.com event: AuthorizeSecurityGroupIngress ids: \"requestParameters.groupId\" - source: ec2.amazonaws.com event: AuthorizeSecurityGroupEgress ids: \"requestParameters.groupId\" - source: ec2.amazonaws.com event: RevokeSecurityGroupEgress ids: \"requestParameters.groupId\" - source: ec2.amazonaws.com event: RevokeSecurityGroupIngress ids: \"requestParameters.groupId\" filters: - or: - type: ingress Cidr: value: \"0.0.0.0/0\" - type: ingress CidrV6: value: \"::/0\" actions: - type: remove-permissions ingress: matched - type: notify template: default.html priority_header: 1 subject: \"Open Security Group Rule Created-[custodian {{ account }} - {{ region }}]\" violation_desc: \"Security Group(s) Which Had Rules Open To The World:\" action_desc: | \"Actions Taken: The Violating Security Group Rule Has Been Removed As It Typically Allows Direct Incoming Public Internet Traffic Access To Your Resource Which Violates Our Company's Cloud Security Policy. Please Refer To Our Company's Cloud Security Best Practices Documentation. If This Ingress Rule Is Required You May Contact The Security Team To Request An Exception.\" to: - CloudCustodian@Company.com - event-owner transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/12345678900/cloud-custodian-mailer region: us-east-1 By including - event-owner in the notify\\'s to: field it tells Cloud Custodian to extract the id of the user who made the API call for the event and email them. Being that the above policy runs in a cloudtrail mode the API call\\'s metadata event is present which is why the example uses event-owner. If you were to remove the mode: statement on the example policy and run it in a poll mode instead you could change - event-owner to - resource-owner which would rely on the resources tags for a id or email to send the notification to as no API event would be available at that time. Note that the notify action requires the cloud custodian mailer tool to be installed.","title":"Securitygroupsdetectremediate"},{"location":"aws/examples/securitygroupsdetectremediate/#security-groups-detect-and-remediate-violations-securitygroupsdetectremediate","text":"The following example policy will automatically create a CloudWatch Event Rule triggered Lambda function in your account and region which will be triggered anytime a user creates or modifies a security group. This provides near real-time auto-remediation action (typically within a minute) of the security group change. Having such a quick auto-remediation action greatly reduces any attack window! By notifying the customer who tried to perform the action it helps drive user behaviour and lets them know why the security group keeps reverting their 0.0.0.0/0 rule additions on them! policies: - name: high-risk-security-groups-remediate resource: security-group description: | Remove any rule from a security group that allows 0.0.0.0/0 or ::/0 (IPv6) ingress and notify the user who added the violating rule. mode: type: cloudtrail events: - source: ec2.amazonaws.com event: AuthorizeSecurityGroupIngress ids: \"requestParameters.groupId\" - source: ec2.amazonaws.com event: AuthorizeSecurityGroupEgress ids: \"requestParameters.groupId\" - source: ec2.amazonaws.com event: RevokeSecurityGroupEgress ids: \"requestParameters.groupId\" - source: ec2.amazonaws.com event: RevokeSecurityGroupIngress ids: \"requestParameters.groupId\" filters: - or: - type: ingress Cidr: value: \"0.0.0.0/0\" - type: ingress CidrV6: value: \"::/0\" actions: - type: remove-permissions ingress: matched - type: notify template: default.html priority_header: 1 subject: \"Open Security Group Rule Created-[custodian {{ account }} - {{ region }}]\" violation_desc: \"Security Group(s) Which Had Rules Open To The World:\" action_desc: | \"Actions Taken: The Violating Security Group Rule Has Been Removed As It Typically Allows Direct Incoming Public Internet Traffic Access To Your Resource Which Violates Our Company's Cloud Security Policy. Please Refer To Our Company's Cloud Security Best Practices Documentation. If This Ingress Rule Is Required You May Contact The Security Team To Request An Exception.\" to: - CloudCustodian@Company.com - event-owner transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/12345678900/cloud-custodian-mailer region: us-east-1 By including - event-owner in the notify\\'s to: field it tells Cloud Custodian to extract the id of the user who made the API call for the event and email them. Being that the above policy runs in a cloudtrail mode the API call\\'s metadata event is present which is why the example uses event-owner. If you were to remove the mode: statement on the example policy and run it in a poll mode instead you could change - event-owner to - resource-owner which would rely on the resources tags for a id or email to send the notification to as no API event would be available at that time. Note that the notify action requires the cloud custodian mailer tool to be installed.","title":"Security Groups - Detect and Remediate Violations {#securitygroupsdetectremediate}"},{"location":"aws/examples/tagcompliance/","text":"Tag Compliance Across Resources (EC2, ASG, ELB, S3, etc) Tag : Tags instances matching filters with a \\'c7n_status\\' tag by default and configurable value. Here\\'s an example of renaming an extant tag ``` {.yaml} policies: - name: ec2-tag-instances resource: ec2 filters: - \"tag:CostCenter\": foobar actions: - type: tag key: CostCenter value: barrum ``` Report on Tag Compliance : ``` {.yaml} policies: - name: ec2-tag-compliance resource: ec2 comment: | Report on total count of non compliant instances filters: - or: - \"tag:Owner\": absent - \"tag:CostCenter\": absent - \"tag:Project\": absent ``` Enforce Tag Compliance : All EC2 non-AutoScaling instances that do not have the three required tags (CostCenter, Owner, Project) will be stopped hourly after 2 days, and terminated after 5 days. ``` {.yaml} policies: - name: ec2-tag-compliance-mark resource: ec2 comment: | Find all (non-ASG) instances that are not conformant to tagging policies, and tag them for stoppage in 1 days. filters: - \"tag:aws:autoscaling:groupName\": absent - \"tag:c7n_status\": absent - or: - \"tag:Owner\": absent - \"tag:CostCenter\": absent - \"tag:Project\": absent actions: - type: mark-for-op op: stop days: 1 - name: ec2-tag-compliance-unmark resource: ec2 comment: | Any instances which have previously been marked as non compliant with tag policies, that are now compliant should be unmarked as non-compliant. filters: - \"tag:Owner\": not-null - \"tag:CostCenter\": not-null - \"tag:Project\": not-null - \"tag:c7n_status\": not-null actions: - unmark - start - name: ec2-tag-compliance-stop resource: ec2 comment: | Stop all non autoscaling group instances previously marked for stoppage by today's date, and schedule termination in 2 days. Also verify that they continue to not meet tagging policies. filters: - \"tag:aws:autoscaling:groupName\": absent - type: marked-for-op op: stop - or: - \"tag:Owner\": absent - \"tag:CostCenter\": absent - \"tag:Project\": absent actions: - stop - type: mark-for-op op: terminate days: 3 - name: ec2-tag-compliance-terminate resource: ec2 comment: | Terminate all stopped instances marked for termination by today's date. filters: - \"tag:aws:autoscaling:groupName\": absent - type: marked-for-op op: terminate - or: - \"tag:Owner\": absent - \"tag:CostCenter\": absent - \"tag:Project\": absent actions: - type: terminate force: true - name: ec2-tag-compliance-nag-stop resource: ec2 comment: | Stop all instances marked for termination every hour starting 1 day before their termination. filters: - \"tag:aws:autoscaling:groupName\": absent - type: marked-for-op op: terminate skew: 1 - or: - \"tag:CostCenter\": absent - \"tag:Owner\": absent - \"tag:Project\": absent actions: - stop ``` Enforce Tag Compliance : All AutoScaling Groups that do not have the 5 required tags: (Resource Contact, Billing Cost Center, Environment, Resource Purpose, Business Unit) will be suspended and stopped once after 24 hours and then hourly after 2 days, and terminated after 3 days. We are using a custom tag named c7n_tag_compliance ``` {.yaml} vars: tag-filters: &tag-compliance-filters - \"tag:Resource Contact\": absent - \"tag:Billing Cost Center\": absent - \"tag:Environment\": absent - \"tag:Resource Purpose\": absent - \"tag:Business Unit\": absent policies: - name: asg-tag-compliance-mark-new-day-0 resource: asg mode: type: cloudtrail events: - source: autoscaling.amazonaws.com event: CreateAutoScalingGroup ids: requestParameters.autoScalingGroupName description: | Marks newly launched non-compliant ASGs if missing any of the required tags also tags the owners. comments: | Your ASG and ASG instances do not have all the required tags on them and will be suspended in 24 hours if all the required tags have not been added. If tags are not made compliant after 3 days your ASG and instances will be deleted. filters: - \"tag:c7n_tag_compliance\": absent - or: *tag-compliance-filters actions: - type: mark-for-op tag: c7n_tag_compliance op: suspend days: 1 - type: auto-tag-user tag: CreatorName principal_id_tag: CreatorId - type: notify template: default.html priority_header: 1 subject: \"ASG - Missing Required Tags - [custodian {{ account }} - {{ region }}]\" violation_desc: | Your ASG and related servers are missing the required tags and is now marked for suspension if tags not added within 24 hours: action_desc: | \"Actions Taken: The ASG is marked to be suspended tomorrow if required tags don't get added to the ASG\" to: - CloudCustodian@Company.com - event-owner transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/XXXXXXXXXX/cloud-custodian-mailer region: us-east-1 - name: asg-tag-compliance-unmark resource: asg mode: type: periodic schedule: \"rate(5 minutes)\" description: | Any ASG which have previously been marked as non compliant with tag policies, that are now compliant should be unmarked as non-compliant. comments: | Thank you for adding the required tags to your ASG! It is now compliant and has been resumed if it was in a suspended state. filters: - \"tag:c7n_tag_compliance\": not-null - \"tag:Resource Contact\": not-null - \"tag:Billing Cost Center\": not-null - \"tag:Environment\": not-null - \"tag:Resource Purpose\": not-null - \"tag:Business Unit\": not-null actions: - type: unmark key: \"c7n_tag_compliance\" - resume - type: propagate-tags tags: - \"Resource Contact\" - \"Billing Cost Center\" - \"Environment\" - \"Resource Purpose\" - \"Business Unit\" - type: notify template: default.html priority_header: 1 subject: \"ASG - AutoScaling Group is now compliant - [custodian {{ account }} - {{ region }}]\" violation_desc: | \"Your ASG which was previously missing required tags is now compliant and won't be suspended:\" action_desc: | \"Actions Taken: The ASG has been unmarked for suspending as its now compliant with tags\" to: - resource-owner transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/XXXXXXXXXX/cloud-custodian-mailer region: us-east-1 - name: asg-tag-compliance-suspend-day-1 resource: asg mode: type: periodic schedule: \"rate(1 hour)\" description: | Suspends the ASG and resizes to 0 instances as the tags are still not compliant comments: | Your ASG has been suspended and resized to 0 instances as they do not have all the required tags on them. Please login to AWS and add the required tags to your ASG. Starting tomorrow hourly emails and suspensions will start occuring if the ASG is still not compliant. The following day your ASG will be deleted. filters: - or: *tag-compliance-filters - type: marked-for-op tag: c7n_tag_compliance op: suspend - type: value key: CreatedTime op: gte value_type: age value: 1 actions: - suspend - type: mark-for-op tag: c7n_tag_compliance op: delete days: 2 - type: notify template: default.html priority_header: 1 subject: \"ASG - !!!! Missing Required Tags !!!! - [custodian {{ account }} - {{ region }}]\" violation_desc: | \"Your ASG is missing the required tags and will be deleted in 2 days if still not compliant. Until then the ASG will be suspended every hour until tagged:\" action_desc: | \"Actions Taken: The ASG has been suspended as it doesn't meet tagging requirements. Please tag your ASG. ASG will be deleted in 2 days if not tagged.\" to: - resource-owner transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/XXXXXXXXXX/cloud-custodian-mailer region: us-east-1 - name: asg-tag-compliance-nag-stop-day-2 resource: asg mode: type: periodic schedule: \"rate(1 hour)\" description: | Suspends ASGT and stops ASG instances every hour starting 1 day before their deletion if tags are still not compliant. filters: - or: *tag-compliance-filters - type: marked-for-op tag: c7n_tag_compliance op: delete skew: 1 - type: value key: CreatedTime op: gte value_type: age value: 2 actions: - suspend - type: notify template: default.html priority_header: 1 subject: \"ASG - AutoScaling Group Suspended!!! - [custodian {{ account }} - {{ region }}]\" violation_desc: | \"Your ASG is missing the required tags and will be deleted in less than 1 day if still not compliant. Until then the ASG will be suspended every hour until tagged or Deleted:\" action_desc: | \"Actions Taken: The ASG has been suspended and set to 0 instances as it doesn't meet tagging requirements. Please tag your ASG now. ASG will be deleted in less than 1 day if not tagged.\" to: - resource-owner transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/XXXXXXXXXX/cloud-custodian-mailer region: us-east-1 - name: asg-tag-compliance-delete-day3 resource: asg mode: type: periodic schedule: \"rate(1 hour)\" description: | Delete all ASG marked for deletion by today's date. comments: | Your ASG has been deleted as it still did not meet the required tag compliance! filters: - or: *tag-compliance-filters - type: marked-for-op tag: c7n_tag_compliance op: delete - type: value key: CreatedTime op: gte value_type: age value: 3 actions: - type: delete force: true - type: notify template: default.html priority_header: 1 subject: \"ASG - ASG Deleted Due To Missing Tags - [custodian {{ account }} - {{ region }}]\" violation_desc: \"Your ASG is still missing the required tags :\" action_desc: | \"Actions Taken: The ASG has been Deleted. A new ASG will need to be launched to replace this if needed. Please make sure to tag the new ASG\" to: - CloudCustodian@Company.com - resource-owner transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/XXXXXXXXXX/cloud-custodian-mailer region: us-east-1 ```","title":"Tagcompliance"},{"location":"aws/examples/tagcompliance/#tag-compliance-across-resources-ec2-asg-elb-s3-etc","text":"Tag : Tags instances matching filters with a \\'c7n_status\\' tag by default and configurable value. Here\\'s an example of renaming an extant tag ``` {.yaml} policies: - name: ec2-tag-instances resource: ec2 filters: - \"tag:CostCenter\": foobar actions: - type: tag key: CostCenter value: barrum ``` Report on Tag Compliance : ``` {.yaml} policies: - name: ec2-tag-compliance resource: ec2 comment: | Report on total count of non compliant instances filters: - or: - \"tag:Owner\": absent - \"tag:CostCenter\": absent - \"tag:Project\": absent ``` Enforce Tag Compliance : All EC2 non-AutoScaling instances that do not have the three required tags (CostCenter, Owner, Project) will be stopped hourly after 2 days, and terminated after 5 days. ``` {.yaml} policies: - name: ec2-tag-compliance-mark resource: ec2 comment: | Find all (non-ASG) instances that are not conformant to tagging policies, and tag them for stoppage in 1 days. filters: - \"tag:aws:autoscaling:groupName\": absent - \"tag:c7n_status\": absent - or: - \"tag:Owner\": absent - \"tag:CostCenter\": absent - \"tag:Project\": absent actions: - type: mark-for-op op: stop days: 1 - name: ec2-tag-compliance-unmark resource: ec2 comment: | Any instances which have previously been marked as non compliant with tag policies, that are now compliant should be unmarked as non-compliant. filters: - \"tag:Owner\": not-null - \"tag:CostCenter\": not-null - \"tag:Project\": not-null - \"tag:c7n_status\": not-null actions: - unmark - start - name: ec2-tag-compliance-stop resource: ec2 comment: | Stop all non autoscaling group instances previously marked for stoppage by today's date, and schedule termination in 2 days. Also verify that they continue to not meet tagging policies. filters: - \"tag:aws:autoscaling:groupName\": absent - type: marked-for-op op: stop - or: - \"tag:Owner\": absent - \"tag:CostCenter\": absent - \"tag:Project\": absent actions: - stop - type: mark-for-op op: terminate days: 3 - name: ec2-tag-compliance-terminate resource: ec2 comment: | Terminate all stopped instances marked for termination by today's date. filters: - \"tag:aws:autoscaling:groupName\": absent - type: marked-for-op op: terminate - or: - \"tag:Owner\": absent - \"tag:CostCenter\": absent - \"tag:Project\": absent actions: - type: terminate force: true - name: ec2-tag-compliance-nag-stop resource: ec2 comment: | Stop all instances marked for termination every hour starting 1 day before their termination. filters: - \"tag:aws:autoscaling:groupName\": absent - type: marked-for-op op: terminate skew: 1 - or: - \"tag:CostCenter\": absent - \"tag:Owner\": absent - \"tag:Project\": absent actions: - stop ``` Enforce Tag Compliance : All AutoScaling Groups that do not have the 5 required tags: (Resource Contact, Billing Cost Center, Environment, Resource Purpose, Business Unit) will be suspended and stopped once after 24 hours and then hourly after 2 days, and terminated after 3 days. We are using a custom tag named c7n_tag_compliance ``` {.yaml} vars: tag-filters: &tag-compliance-filters - \"tag:Resource Contact\": absent - \"tag:Billing Cost Center\": absent - \"tag:Environment\": absent - \"tag:Resource Purpose\": absent - \"tag:Business Unit\": absent policies: - name: asg-tag-compliance-mark-new-day-0 resource: asg mode: type: cloudtrail events: - source: autoscaling.amazonaws.com event: CreateAutoScalingGroup ids: requestParameters.autoScalingGroupName description: | Marks newly launched non-compliant ASGs if missing any of the required tags also tags the owners. comments: | Your ASG and ASG instances do not have all the required tags on them and will be suspended in 24 hours if all the required tags have not been added. If tags are not made compliant after 3 days your ASG and instances will be deleted. filters: - \"tag:c7n_tag_compliance\": absent - or: *tag-compliance-filters actions: - type: mark-for-op tag: c7n_tag_compliance op: suspend days: 1 - type: auto-tag-user tag: CreatorName principal_id_tag: CreatorId - type: notify template: default.html priority_header: 1 subject: \"ASG - Missing Required Tags - [custodian {{ account }} - {{ region }}]\" violation_desc: | Your ASG and related servers are missing the required tags and is now marked for suspension if tags not added within 24 hours: action_desc: | \"Actions Taken: The ASG is marked to be suspended tomorrow if required tags don't get added to the ASG\" to: - CloudCustodian@Company.com - event-owner transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/XXXXXXXXXX/cloud-custodian-mailer region: us-east-1 - name: asg-tag-compliance-unmark resource: asg mode: type: periodic schedule: \"rate(5 minutes)\" description: | Any ASG which have previously been marked as non compliant with tag policies, that are now compliant should be unmarked as non-compliant. comments: | Thank you for adding the required tags to your ASG! It is now compliant and has been resumed if it was in a suspended state. filters: - \"tag:c7n_tag_compliance\": not-null - \"tag:Resource Contact\": not-null - \"tag:Billing Cost Center\": not-null - \"tag:Environment\": not-null - \"tag:Resource Purpose\": not-null - \"tag:Business Unit\": not-null actions: - type: unmark key: \"c7n_tag_compliance\" - resume - type: propagate-tags tags: - \"Resource Contact\" - \"Billing Cost Center\" - \"Environment\" - \"Resource Purpose\" - \"Business Unit\" - type: notify template: default.html priority_header: 1 subject: \"ASG - AutoScaling Group is now compliant - [custodian {{ account }} - {{ region }}]\" violation_desc: | \"Your ASG which was previously missing required tags is now compliant and won't be suspended:\" action_desc: | \"Actions Taken: The ASG has been unmarked for suspending as its now compliant with tags\" to: - resource-owner transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/XXXXXXXXXX/cloud-custodian-mailer region: us-east-1 - name: asg-tag-compliance-suspend-day-1 resource: asg mode: type: periodic schedule: \"rate(1 hour)\" description: | Suspends the ASG and resizes to 0 instances as the tags are still not compliant comments: | Your ASG has been suspended and resized to 0 instances as they do not have all the required tags on them. Please login to AWS and add the required tags to your ASG. Starting tomorrow hourly emails and suspensions will start occuring if the ASG is still not compliant. The following day your ASG will be deleted. filters: - or: *tag-compliance-filters - type: marked-for-op tag: c7n_tag_compliance op: suspend - type: value key: CreatedTime op: gte value_type: age value: 1 actions: - suspend - type: mark-for-op tag: c7n_tag_compliance op: delete days: 2 - type: notify template: default.html priority_header: 1 subject: \"ASG - !!!! Missing Required Tags !!!! - [custodian {{ account }} - {{ region }}]\" violation_desc: | \"Your ASG is missing the required tags and will be deleted in 2 days if still not compliant. Until then the ASG will be suspended every hour until tagged:\" action_desc: | \"Actions Taken: The ASG has been suspended as it doesn't meet tagging requirements. Please tag your ASG. ASG will be deleted in 2 days if not tagged.\" to: - resource-owner transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/XXXXXXXXXX/cloud-custodian-mailer region: us-east-1 - name: asg-tag-compliance-nag-stop-day-2 resource: asg mode: type: periodic schedule: \"rate(1 hour)\" description: | Suspends ASGT and stops ASG instances every hour starting 1 day before their deletion if tags are still not compliant. filters: - or: *tag-compliance-filters - type: marked-for-op tag: c7n_tag_compliance op: delete skew: 1 - type: value key: CreatedTime op: gte value_type: age value: 2 actions: - suspend - type: notify template: default.html priority_header: 1 subject: \"ASG - AutoScaling Group Suspended!!! - [custodian {{ account }} - {{ region }}]\" violation_desc: | \"Your ASG is missing the required tags and will be deleted in less than 1 day if still not compliant. Until then the ASG will be suspended every hour until tagged or Deleted:\" action_desc: | \"Actions Taken: The ASG has been suspended and set to 0 instances as it doesn't meet tagging requirements. Please tag your ASG now. ASG will be deleted in less than 1 day if not tagged.\" to: - resource-owner transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/XXXXXXXXXX/cloud-custodian-mailer region: us-east-1 - name: asg-tag-compliance-delete-day3 resource: asg mode: type: periodic schedule: \"rate(1 hour)\" description: | Delete all ASG marked for deletion by today's date. comments: | Your ASG has been deleted as it still did not meet the required tag compliance! filters: - or: *tag-compliance-filters - type: marked-for-op tag: c7n_tag_compliance op: delete - type: value key: CreatedTime op: gte value_type: age value: 3 actions: - type: delete force: true - type: notify template: default.html priority_header: 1 subject: \"ASG - ASG Deleted Due To Missing Tags - [custodian {{ account }} - {{ region }}]\" violation_desc: \"Your ASG is still missing the required tags :\" action_desc: | \"Actions Taken: The ASG has been Deleted. A new ASG will need to be launched to replace this if needed. Please make sure to tag the new ASG\" to: - CloudCustodian@Company.com - resource-owner transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/XXXXXXXXXX/cloud-custodian-mailer region: us-east-1 ```","title":"Tag Compliance Across Resources (EC2, ASG, ELB, S3, etc)"},{"location":"aws/examples/vpcflowlog/","text":"VPC - Flow Log Configuration Check {#accountaccountflowlog} The following example policy will find any VPC Flow Log in your region that is not properly configured and notify a group via email. Ensuring VPC Flow Logs are enabled and setup properly is very important for compliance and security. Flow Logs themselves capture IP traffic information to and from network interfaces and can be used for troubleshooting traffic issues and monitoring network traffic as a security tool. See more info on example dashboarding of VPC Flow Logs using Elasticsearch and Kibana https://aws.amazon.com/blogs/aws/cloudwatch-logs-subscription-consumer-elasticsearch-kibana-dashboards/ policies: - name: vpc-flow-log-check resource: vpc filters: - not: - type: flow-logs enabled: true set-op: or op: equal traffic-type: all log-group: myVPCFlowLogs status: active actions: - type: notify template: default.html priority_header: 1 subject: \"Cloud Custodian - VPC Flow Log(s) Not Setup Properly\" violation_desc: \"The Following Flow Logs Are Invalid:\" action_desc: \"Actions Taken: Notification Only\" to: - CloudCustodian@Company.com transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/12345678900/cloud-custodian-mailer region: us-east-1 Note that the notify action requires the cloud custodian mailer tool to be installed.","title":"Vpcflowlog"},{"location":"aws/examples/vpcflowlog/#vpc-flow-log-configuration-check-accountaccountflowlog","text":"The following example policy will find any VPC Flow Log in your region that is not properly configured and notify a group via email. Ensuring VPC Flow Logs are enabled and setup properly is very important for compliance and security. Flow Logs themselves capture IP traffic information to and from network interfaces and can be used for troubleshooting traffic issues and monitoring network traffic as a security tool. See more info on example dashboarding of VPC Flow Logs using Elasticsearch and Kibana https://aws.amazon.com/blogs/aws/cloudwatch-logs-subscription-consumer-elasticsearch-kibana-dashboards/ policies: - name: vpc-flow-log-check resource: vpc filters: - not: - type: flow-logs enabled: true set-op: or op: equal traffic-type: all log-group: myVPCFlowLogs status: active actions: - type: notify template: default.html priority_header: 1 subject: \"Cloud Custodian - VPC Flow Log(s) Not Setup Properly\" violation_desc: \"The Following Flow Logs Are Invalid:\" action_desc: \"Actions Taken: Notification Only\" to: - CloudCustodian@Company.com transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/12345678900/cloud-custodian-mailer region: us-east-1 Note that the notify action requires the cloud custodian mailer tool to be installed.","title":"VPC - Flow Log Configuration Check {#accountaccountflowlog}"},{"location":"aws/examples/vpcpeeringcrossaccount/","text":"VPC - Notify On Invalid External Peering Connections {#vpcpeeringcrossaccount} The following example policy will automatically create a CloudWatch Event Rule triggered Lambda function in your account and region which will be triggered anytime a new VPC Peering Connection is created. The policy will then check to see if the peering accepter account id and peering requester account id are both AWS account numbers owned by you. This is done by having the account ids from the CloudWatch Event compared against a S3 hosted CSV of your AWS account numbers. You must provide the CSV file of your account numbers or you can hardcode your account numbers into the policy if you have a small static number of accounts. The CSV would look something like: \\\"271212121293\\\",\\\"171717171716\\\",\\\"27272727272724\\\",\\\"121212112128\\\",\\\"118118118118\\\" policies: - name: vpc-peering-cross-account-checker-real-time resource: peering-connection mode: type: cloudtrail events: - source: ec2.amazonaws.com event: CreateVpcPeeringConnection ids: 'responseElements.vpcPeeringConnection.vpcPeeringConnectionId' timeout: 90 memory: 256 role: arn:aws:iam::{account_id}:role/Cloud_Custodian_EC2_Lambda_Role description: | When a new peering connection is created the Accepter and Requester account numbers are compared and if they aren't both internally owned accounts then the cloud and security teams are notified to investigate and delete the peering connection. filters: - or: - type: event key: \"detail.responseElements.vpcPeeringConnection.accepterVpcInfo.ownerId\" op: not-in value_from: url: s3://s3bucketname/AccountNumbers.csv format: csv2dict - type: event key: \"detail.responseElements.vpcPeeringConnection.requesterVpcInfo.ownerId\" op: not-in value_from: url: s3://s3bucketname/AccountNumbers.csv format: csv2dict actions: - type: notify template: default.html priority_header: 1 subject: \"ATTN!! External VPC Peering Violation [custodian {{ account }} - {{ region }}]\" violation_desc: | VPC Peers are not to be setup to or from external AWS accounts so this policy verifies that both the source and destination accounts are internally owned. If the peering connection is going to/from an external account, this policy will email the Cloud and Security Teams as well as the customer. action_desc: | Please investigate this VPC Peering connection and terminate it if it's connecting to a unapproved external VPC to: - CloudTeam@company.com - security@company.com - resource-contact transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/XXXXXXXXXXXXXXX/cloud-custodian-mailer region: us-east-1 The following policy runs in pull mode and will scan all existing vpc peering connections to see if any of them have external connections. You will notice that the filters syntax to pull the accepter and requester ids is slightly different between these 2 policies. The first one pulls the information from the CloudTrail API event metadata and the second policy uses information pulled back from a describe_vpc_peering_connections API call. Using both policies allows you to check both new and existing peering connections. policies: - name: vpc-peering-cross-account-checker-pull resource: peering-connection description: | Checks existing VPC Peering Connections to see if the Accepter and Requester account numbers are both internally owned accounts. If a connection is going to/from an external AWS account then the cloud and security teams are notified of the violating peering connection. filters: - or: - type: value key: \"RequesterVpcInfo.OwnerId\" op: not-in value_from: url: s3://s3bucketname/AccountNumbers.csv format: csv2dict - type: value key: \"AccepterVpcInfo.OwnerId\" op: not-in value_from: url: s3://s3bucketname/AccountNumbers.csv format: csv2dict actions: - type: notify template: default.html priority_header: 1 subject: \"ATTN!! External VPC Peering Violation [custodian {{ account }} - {{ region }}]\" violation_desc: | VPC Peers are not to be setup to or from external AWS accounts so this policy verifies that both the source and destination accounts are internally owned. If the peering connection is going to/from an external account, this policy will email the Cloud and Security Teams as well as the customer. action_desc: | Please investigate this VPC Peering connection and terminate it if it's connecting to a unapproved external VPC to: - CloudTeam@company.com - security@company.com - resource-contact transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/XXXXXXXXXXXXXXX/cloud-custodian-mailer region: us-east-1 Note that for email delivery to work with the notify action, the cloud custodian mailer tool must be installed, configured, and running. See https://github.com/cloud-custodian/cloud-custodian/tree/master/tools/c7n_mailer for docs.","title":"Vpcpeeringcrossaccount"},{"location":"aws/examples/vpcpeeringcrossaccount/#vpc-notify-on-invalid-external-peering-connections-vpcpeeringcrossaccount","text":"The following example policy will automatically create a CloudWatch Event Rule triggered Lambda function in your account and region which will be triggered anytime a new VPC Peering Connection is created. The policy will then check to see if the peering accepter account id and peering requester account id are both AWS account numbers owned by you. This is done by having the account ids from the CloudWatch Event compared against a S3 hosted CSV of your AWS account numbers. You must provide the CSV file of your account numbers or you can hardcode your account numbers into the policy if you have a small static number of accounts. The CSV would look something like: \\\"271212121293\\\",\\\"171717171716\\\",\\\"27272727272724\\\",\\\"121212112128\\\",\\\"118118118118\\\" policies: - name: vpc-peering-cross-account-checker-real-time resource: peering-connection mode: type: cloudtrail events: - source: ec2.amazonaws.com event: CreateVpcPeeringConnection ids: 'responseElements.vpcPeeringConnection.vpcPeeringConnectionId' timeout: 90 memory: 256 role: arn:aws:iam::{account_id}:role/Cloud_Custodian_EC2_Lambda_Role description: | When a new peering connection is created the Accepter and Requester account numbers are compared and if they aren't both internally owned accounts then the cloud and security teams are notified to investigate and delete the peering connection. filters: - or: - type: event key: \"detail.responseElements.vpcPeeringConnection.accepterVpcInfo.ownerId\" op: not-in value_from: url: s3://s3bucketname/AccountNumbers.csv format: csv2dict - type: event key: \"detail.responseElements.vpcPeeringConnection.requesterVpcInfo.ownerId\" op: not-in value_from: url: s3://s3bucketname/AccountNumbers.csv format: csv2dict actions: - type: notify template: default.html priority_header: 1 subject: \"ATTN!! External VPC Peering Violation [custodian {{ account }} - {{ region }}]\" violation_desc: | VPC Peers are not to be setup to or from external AWS accounts so this policy verifies that both the source and destination accounts are internally owned. If the peering connection is going to/from an external account, this policy will email the Cloud and Security Teams as well as the customer. action_desc: | Please investigate this VPC Peering connection and terminate it if it's connecting to a unapproved external VPC to: - CloudTeam@company.com - security@company.com - resource-contact transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/XXXXXXXXXXXXXXX/cloud-custodian-mailer region: us-east-1 The following policy runs in pull mode and will scan all existing vpc peering connections to see if any of them have external connections. You will notice that the filters syntax to pull the accepter and requester ids is slightly different between these 2 policies. The first one pulls the information from the CloudTrail API event metadata and the second policy uses information pulled back from a describe_vpc_peering_connections API call. Using both policies allows you to check both new and existing peering connections. policies: - name: vpc-peering-cross-account-checker-pull resource: peering-connection description: | Checks existing VPC Peering Connections to see if the Accepter and Requester account numbers are both internally owned accounts. If a connection is going to/from an external AWS account then the cloud and security teams are notified of the violating peering connection. filters: - or: - type: value key: \"RequesterVpcInfo.OwnerId\" op: not-in value_from: url: s3://s3bucketname/AccountNumbers.csv format: csv2dict - type: value key: \"AccepterVpcInfo.OwnerId\" op: not-in value_from: url: s3://s3bucketname/AccountNumbers.csv format: csv2dict actions: - type: notify template: default.html priority_header: 1 subject: \"ATTN!! External VPC Peering Violation [custodian {{ account }} - {{ region }}]\" violation_desc: | VPC Peers are not to be setup to or from external AWS accounts so this policy verifies that both the source and destination accounts are internally owned. If the peering connection is going to/from an external account, this policy will email the Cloud and Security Teams as well as the customer. action_desc: | Please investigate this VPC Peering connection and terminate it if it's connecting to a unapproved external VPC to: - CloudTeam@company.com - security@company.com - resource-contact transport: type: sqs queue: https://sqs.us-east-1.amazonaws.com/XXXXXXXXXXXXXXX/cloud-custodian-mailer region: us-east-1 Note that for email delivery to work with the notify action, the cloud custodian mailer tool must be installed, configured, and running. See https://github.com/cloud-custodian/cloud-custodian/tree/master/tools/c7n_mailer for docs.","title":"VPC - Notify On Invalid External Peering Connections {#vpcpeeringcrossaccount}"},{"location":"aws/topics/","text":"AWS Topics {#topics} Deeper dives on custodian integration with particular services or common best practices. ::: {.toctree titlesonly=\"\"} config.rst securityhub.rst ssm.rst xray.rst :::","title":"Index"},{"location":"aws/topics/#aws-topics-topics","text":"Deeper dives on custodian integration with particular services or common best practices. ::: {.toctree titlesonly=\"\"} config.rst securityhub.rst ssm.rst xray.rst :::","title":"AWS Topics {#topics}"},{"location":"aws/topics/config/","text":"AWS Config Custodian has deep integration with config, a custodian policy: Can be deployed as config-rule for any resource type supported by config. Can use config as resource database instead of querying service describe apis. Custodian supports server side querying resources with Config\\'s SQL expression language. Can filter resources based on their compliance with one or more config rules. Can be deployed as a config-poll-rule against any resource type supported by cloudformation. Custodian does the legwork of normalizing the resource description from config\\'s idiosyncratic format to one that looks like describe api call output, so policies can utilize config with a simple change of source or execution mode. Config Source You can use config as a cmdb of resources instead of doing describes by adding source: config to any policy on a resource type that config supports. This also supports doing arbitrary sql selects (via config\\'s select resources api) on the resources in addition to the standard custodian filters. policies: - name: dynamdb-checker resource: aws.dynamodb-table source: config query: - clause: \"resourceId = 'MyTable'\" filters: - SSEDescription: absent Config Rule Custodian is also one of the easiest ways of authoring custom config rules. For any config supported resource, you can just add a mode with type:config-rule to have the policy deployed as a custom config rule lambda. policies: - name: ec2-checker resource: aws.ec2 mode: type: config-rule role: MyLambdaConfigRole filters: - type: image tag: \"NotSupported\" value: absent Filter Custodian also supports filtering resources based on their compliance with other config-rules. policies: - name: ec2-remediate-non-compliant resource: aws.ec2 filters: - type: config-compliance rules: [my_other_config_rule, some_other_rule] states: [NON_COMPLIANT] actions: - stop Config Poll Rule For resources not supported natively by AWS Config, an execution mode of type: config-poll-rule can be used for any resource supported by CloudFormation. This is effectively a periodic policy that queries the resource\\'s service api and filters resources to evaluate compliance/non-compliance and then records results to AWS Config. CloudFormation resources are only partially supported by AWS Config, and are not supported for [source: config]{.title-ref} nor do they support resource timeline or resource attributes. policies: - name: kinesis-one-stream resource: aws.kinesis mode: type: config-poll-rule role: custodian-config-role schedule: Three_Hours filters: - tag:App: Dev","title":"Config"},{"location":"aws/topics/config/#aws-config","text":"Custodian has deep integration with config, a custodian policy: Can be deployed as config-rule for any resource type supported by config. Can use config as resource database instead of querying service describe apis. Custodian supports server side querying resources with Config\\'s SQL expression language. Can filter resources based on their compliance with one or more config rules. Can be deployed as a config-poll-rule against any resource type supported by cloudformation. Custodian does the legwork of normalizing the resource description from config\\'s idiosyncratic format to one that looks like describe api call output, so policies can utilize config with a simple change of source or execution mode.","title":"AWS Config"},{"location":"aws/topics/config/#config-source","text":"You can use config as a cmdb of resources instead of doing describes by adding source: config to any policy on a resource type that config supports. This also supports doing arbitrary sql selects (via config\\'s select resources api) on the resources in addition to the standard custodian filters. policies: - name: dynamdb-checker resource: aws.dynamodb-table source: config query: - clause: \"resourceId = 'MyTable'\" filters: - SSEDescription: absent","title":"Config Source"},{"location":"aws/topics/config/#config-rule","text":"Custodian is also one of the easiest ways of authoring custom config rules. For any config supported resource, you can just add a mode with type:config-rule to have the policy deployed as a custom config rule lambda. policies: - name: ec2-checker resource: aws.ec2 mode: type: config-rule role: MyLambdaConfigRole filters: - type: image tag: \"NotSupported\" value: absent","title":"Config Rule"},{"location":"aws/topics/config/#filter","text":"Custodian also supports filtering resources based on their compliance with other config-rules. policies: - name: ec2-remediate-non-compliant resource: aws.ec2 filters: - type: config-compliance rules: [my_other_config_rule, some_other_rule] states: [NON_COMPLIANT] actions: - stop","title":"Filter"},{"location":"aws/topics/config/#config-poll-rule","text":"For resources not supported natively by AWS Config, an execution mode of type: config-poll-rule can be used for any resource supported by CloudFormation. This is effectively a periodic policy that queries the resource\\'s service api and filters resources to evaluate compliance/non-compliance and then records results to AWS Config. CloudFormation resources are only partially supported by AWS Config, and are not supported for [source: config]{.title-ref} nor do they support resource timeline or resource attributes. policies: - name: kinesis-one-stream resource: aws.kinesis mode: type: config-poll-rule role: custodian-config-role schedule: Three_Hours filters: - tag:App: Dev","title":"Config Poll Rule"},{"location":"aws/topics/securityhub/","text":"Security Hub {#aws-securityhub} Security Hub gives a centralized dashboard of security events across data feeds from many different tools. Custodian supports deep integration with security hub to support the following use cases. post and update findings on any resource type to security hub See post-finding action <aws.common.actions.post-finding> {.interpreted-text role=\"ref\"} filtering resources on the basis of extant findings See finding filter <aws.common.filters.finding> {.interpreted-text role=\"ref\"} lambda execution mode triggered on ingestion of security hub findings [mode: hub-finding]{.title-ref} lambda execution mode as a custom action in the security hub ui. Note custodian security hub actions work against both findings and insights. [mode: hub-action]{.title-ref} Getting Started To post findings with cloud-custodian (v0.9+) you need to enable the product integration from the security hub console. From the left side menu click integrations, search for Cloud Custodian, and enable the Cloud Custodian integration. Modes Execute a policy lambda in response to security hub finding event or action. This policy will provision a lambda and security hub custom action. The action can be invoked on a finding or insight result (collection of findings). The action name will have the resource type prefixed as custodian actions are resource specific. policies: - name: remediate resource: aws.ec2 mode: type: hub-action role: MyRole actions: - snapshot - type: set-instance-profile name: null - stop This policy will provision a lambda that will process high alert findings from guard duty (note custodian also has support for guard duty events directly). policies: - name: remediate resource: aws.iam mode: type: hub-finding role: MyRole filters: - type: event key: detail.findings[].ProductFields.aws/securityhub/ProductName value: GuardDuty - type: event key: detail.findings[].ProductFields.aws/securityhub/ProductName value: GuardDuty actions: - remove-keys Note, for custodian we support additional resources in the finding via the Other resource, so these modes work for resources that security hub doesn\\'t natively support. https://docs.aws.amazon.com/securityhub/latest/userguide/securityhub-cloudwatch-events.html The Amazon Security Finding Format is documented at https://docs.aws.amazon.com/securityhub/latest/userguide/securityhub-findings-format.html Cloud Custodian ASFF Attribute Rationale derived from execution context: AwsAccountId Direct account_id metadata.json Confidence User supplied value in policy.actions.compliance_status policy configuration metadata.json Confidence User supplied value in policy.actions.confidence policy configuration metadata.json Criticality User supplied value in policy.actions.criticality policy configuration derived value: current_time aka Now CreatedAt Timestamp when the finding record was composed for the first time metadata.json Description User supplied value in policy.actions.description policy configuration metadata.json policy.name GeneratorId Policy is equivilent to a Rule digest of region, account_id, policy Id Enforces uniqueness by contents, and Resource Arn/Id combinding policy attributes with resource attributes metadata.json policy.resource_type ProductFields.resource Direct fixed value: CloudCustodian ProductFields.ProviderName Name of project derived value from executable: ProductFields.ProviderVersioN Direct version fixed value: ACTIVE RecordState Always ACTIVE at Create/Update time metadata.json Recommendation.Text User supplied value in policy.actions.recommendation policy configuration metadata.json Recommendation.Url User supplied value in policy.actions.recommendation_url policy configuration resources.json attributes Resources.Details.\\${Type}.* Direct Mapping from columns in Describe output resources.json Arn Resources.Id Direct resource.json Tags Resources.Tags Direct metadata.json policy.resource_type Resources.Type Direct Hardcoded Fixed Value: \\\"2018-10-08\\\" SchemaVersion Only Valid value metadata.json Severity.Label User supplied value in policy.actions.severity_label policy configuration metadata.json Severity.Normalized User supplied value in policy.actions.severity_normalized policy configuration metadata.json policy.name Title Primary Identifer metadata.json policy.actions.types Types User supplied value in policy configuration derived value: current_time aka Now UpdatedAt Timestamp when the finding record update is composed : Mapping data in Cloud Custodian into the Amazon Security Finding Format (ASFF)","title":"Securityhub"},{"location":"aws/topics/securityhub/#security-hub-aws-securityhub","text":"Security Hub gives a centralized dashboard of security events across data feeds from many different tools. Custodian supports deep integration with security hub to support the following use cases. post and update findings on any resource type to security hub See post-finding action <aws.common.actions.post-finding> {.interpreted-text role=\"ref\"} filtering resources on the basis of extant findings See finding filter <aws.common.filters.finding> {.interpreted-text role=\"ref\"} lambda execution mode triggered on ingestion of security hub findings [mode: hub-finding]{.title-ref} lambda execution mode as a custom action in the security hub ui. Note custodian security hub actions work against both findings and insights. [mode: hub-action]{.title-ref}","title":"Security Hub {#aws-securityhub}"},{"location":"aws/topics/securityhub/#getting-started","text":"To post findings with cloud-custodian (v0.9+) you need to enable the product integration from the security hub console. From the left side menu click integrations, search for Cloud Custodian, and enable the Cloud Custodian integration.","title":"Getting Started"},{"location":"aws/topics/securityhub/#modes","text":"Execute a policy lambda in response to security hub finding event or action. This policy will provision a lambda and security hub custom action. The action can be invoked on a finding or insight result (collection of findings). The action name will have the resource type prefixed as custodian actions are resource specific. policies: - name: remediate resource: aws.ec2 mode: type: hub-action role: MyRole actions: - snapshot - type: set-instance-profile name: null - stop This policy will provision a lambda that will process high alert findings from guard duty (note custodian also has support for guard duty events directly). policies: - name: remediate resource: aws.iam mode: type: hub-finding role: MyRole filters: - type: event key: detail.findings[].ProductFields.aws/securityhub/ProductName value: GuardDuty - type: event key: detail.findings[].ProductFields.aws/securityhub/ProductName value: GuardDuty actions: - remove-keys Note, for custodian we support additional resources in the finding via the Other resource, so these modes work for resources that security hub doesn\\'t natively support. https://docs.aws.amazon.com/securityhub/latest/userguide/securityhub-cloudwatch-events.html The Amazon Security Finding Format is documented at https://docs.aws.amazon.com/securityhub/latest/userguide/securityhub-findings-format.html Cloud Custodian ASFF Attribute Rationale derived from execution context: AwsAccountId Direct account_id metadata.json Confidence User supplied value in policy.actions.compliance_status policy configuration metadata.json Confidence User supplied value in policy.actions.confidence policy configuration metadata.json Criticality User supplied value in policy.actions.criticality policy configuration derived value: current_time aka Now CreatedAt Timestamp when the finding record was composed for the first time metadata.json Description User supplied value in policy.actions.description policy configuration metadata.json policy.name GeneratorId Policy is equivilent to a Rule digest of region, account_id, policy Id Enforces uniqueness by contents, and Resource Arn/Id combinding policy attributes with resource attributes metadata.json policy.resource_type ProductFields.resource Direct fixed value: CloudCustodian ProductFields.ProviderName Name of project derived value from executable: ProductFields.ProviderVersioN Direct version fixed value: ACTIVE RecordState Always ACTIVE at Create/Update time metadata.json Recommendation.Text User supplied value in policy.actions.recommendation policy configuration metadata.json Recommendation.Url User supplied value in policy.actions.recommendation_url policy configuration resources.json attributes Resources.Details.\\${Type}.* Direct Mapping from columns in Describe output resources.json Arn Resources.Id Direct resource.json Tags Resources.Tags Direct metadata.json policy.resource_type Resources.Type Direct Hardcoded Fixed Value: \\\"2018-10-08\\\" SchemaVersion Only Valid value metadata.json Severity.Label User supplied value in policy.actions.severity_label policy configuration metadata.json Severity.Normalized User supplied value in policy.actions.severity_normalized policy configuration metadata.json policy.name Title Primary Identifer metadata.json policy.actions.types Types User supplied value in policy configuration derived value: current_time aka Now UpdatedAt Timestamp when the finding record update is composed : Mapping data in Cloud Custodian into the Amazon Security Finding Format (ASFF)","title":"Modes"},{"location":"aws/topics/ssm/","text":"AWS Systems Manager EC2 Systems Manager Cloud custodian enables several use cases with SSM. Filtering instances on the basis of their association to SSM. See ssm filter <aws.ec2.filters.ssm> {.interpreted-text role=\"ref\"} Executing actions on instances with an ssm agent installed. See ec2 send command action <aws.ec2.actions.send-command> {.interpreted-text role=\"ref\"} Ops Center AWS Systems Manager Ops Center is an operations dashboard that provides a central location to collect, triage, and remediate operation issues related to resource in an AWS account. It automatically pulls in contextual information from Cloud Trail and AWS Config to enable easier triage. For more details on Ops Center see. https://docs.aws.amazon.com/systems-manager/latest/userguide/OpsCenter.html Cloud Custodian has deep support for integrating with SSM Ops Center. With custodian you can. Filter resources by extant ops items associated to them. See ops-item filter <aws.common.filters.ops-item> {.interpreted-text role=\"ref\"} Post new ops items for a resource from any custodian policy. See post-item action <aws.common.actions.post-item> {.interpreted-text role=\"ref\"} Manage ops items as a resource, to resolve or update ops items. See ops-item resource <aws.ops-item> {.interpreted-text role=\"ref\"} OmniSSM Custodian project maintains a tool OmniSSM for enabling cross account, and cross cloud provider systems manager setups using the data center/hybrid mode support of SSM. It provides for secure introduction (using signed metadata documents), automated discovery and garbage collection using a set of golang lambda functions. See tools section of the docs for more info.","title":"Ssm"},{"location":"aws/topics/ssm/#aws-systems-manager","text":"","title":"AWS Systems Manager"},{"location":"aws/topics/ssm/#ec2-systems-manager","text":"Cloud custodian enables several use cases with SSM. Filtering instances on the basis of their association to SSM. See ssm filter <aws.ec2.filters.ssm> {.interpreted-text role=\"ref\"} Executing actions on instances with an ssm agent installed. See ec2 send command action <aws.ec2.actions.send-command> {.interpreted-text role=\"ref\"}","title":"EC2 Systems Manager"},{"location":"aws/topics/ssm/#ops-center","text":"AWS Systems Manager Ops Center is an operations dashboard that provides a central location to collect, triage, and remediate operation issues related to resource in an AWS account. It automatically pulls in contextual information from Cloud Trail and AWS Config to enable easier triage. For more details on Ops Center see. https://docs.aws.amazon.com/systems-manager/latest/userguide/OpsCenter.html Cloud Custodian has deep support for integrating with SSM Ops Center. With custodian you can. Filter resources by extant ops items associated to them. See ops-item filter <aws.common.filters.ops-item> {.interpreted-text role=\"ref\"} Post new ops items for a resource from any custodian policy. See post-item action <aws.common.actions.post-item> {.interpreted-text role=\"ref\"} Manage ops items as a resource, to resolve or update ops items. See ops-item resource <aws.ops-item> {.interpreted-text role=\"ref\"}","title":"Ops Center"},{"location":"aws/topics/ssm/#omnissm","text":"Custodian project maintains a tool OmniSSM for enabling cross account, and cross cloud provider systems manager setups using the data center/hybrid mode support of SSM. It provides for secure introduction (using signed metadata documents), automated discovery and garbage collection using a set of golang lambda functions. See tools section of the docs for more info.","title":"OmniSSM"},{"location":"aws/topics/xray/","text":"AWS X-Ray Support Custodian supports tracing policy execution using [AWS X-Ray \\<https://aws.amazon.com/xray/>]{.title-ref} As a pre-requisite the [aws_xray_sdk]{.title-ref} package must be installed. [pip install aws_xray_sdk]{.title-ref} The package comes pre-installed on the custodian docker images. The xray support can be enabled on the command line using: custodian run -s out --trace xray custodian.yml Note custodian does not require an X-Ray daemon running as it will stream traces directly sending to the X-Ray service. It will use the daemon if the AWS_XRAY_DAEMON_ADDRESS environment variable is set. Lambda policies should set tracing_config to enable use of the daemon. By default custodian XRay integration will use the account\\'s xray sampling rules sampling can be turned off by setting a flag: custodian run -s out --trace xray://?sampling=off custodian.yml Note XRay integration is enabled for the entire process, it cannot be configured per policy.","title":"Xray"},{"location":"aws/topics/xray/#aws-x-ray-support","text":"Custodian supports tracing policy execution using [AWS X-Ray \\<https://aws.amazon.com/xray/>]{.title-ref} As a pre-requisite the [aws_xray_sdk]{.title-ref} package must be installed. [pip install aws_xray_sdk]{.title-ref} The package comes pre-installed on the custodian docker images. The xray support can be enabled on the command line using: custodian run -s out --trace xray custodian.yml Note custodian does not require an X-Ray daemon running as it will stream traces directly sending to the X-Ray service. It will use the daemon if the AWS_XRAY_DAEMON_ADDRESS environment variable is set. Lambda policies should set tracing_config to enable use of the daemon. By default custodian XRay integration will use the account\\'s xray sampling rules sampling can be turned off by setting a flag: custodian run -s out --trace xray://?sampling=off custodian.yml Note XRay integration is enabled for the entire process, it cannot be configured per policy.","title":"AWS X-Ray Support"},{"location":"azure/gettingstarted/","text":"Getting Started {#azure_gettingstarted} Write your first policy {#azure_write-policy} Cloud Custodian is a stateless rules engine that filters Azure resources and takes actions on based on policies that you define. Cloud Custodian policies are expressed in YAML and include the following: The type of resource to run the policy against Filters to narrow down the set of resources Actions to take on the filtered set of resources Our first policy filters to a VM of a specific name, then adds the tag Hello: World . Create a file named custodian.yml with the following content. Update my_vm_name to match an existing VM. Note: Some text editors (VSCode) inject invalid whitespace characters when copy/pasting YAML from a browser policies: - name: my-first-policy description: | Adds a tag to a virtual machines resource: azure.vm filters: - type: value key: name value: my_vm_name actions: - type: tag tag: Hello value: World Run your policy {#azure_run-policy} Choose one of the supported authentication mechanisms , and either log in to Azure CLI or set environment variables as documented in azure_authentication {.interpreted-text role=\"ref\"}. custodian run --output-dir=. custodian.yml If successful, you should see output like the following on the command line: 2016-12-20 08:35:06,133: custodian.policy:INFO Running policy my-first-policy resource: azure.vm 2016-12-20 08:35:07,514: custodian.policy:INFO policy: my-first-policy resource:azure.vm has count:1 time:1.38 2016-12-20 08:35:08,188: custodian.policy:INFO policy: my-first-policy action: tag: 1 execution_time: 0.67 You should also find a new my-first-policy directory with a log and a resources.json file. See filters {.interpreted-text role=\"ref\"} for more information on the features of the Value filter used in this sample. (Optional) Run your policy with Azure Monitoring {#monitor-azure-cc} Cloud Custodian policies can emit logs and metrics to Application Insights when the policy executes. Please refer to the azure_monitoring {.interpreted-text role=\"ref\"} section for further details. View policy results {#azure_view_policy_reults} The resources.json file shows you the raw data that results from your policy after filtering. This file can help you understand the fields available for your resources while developing your policy. Custodian Report Custodian has a report feature that allows the resources.json file to be viewed more concisely. By default, this will output data in a CSV format, but report also provides other output formats such as grid that are more digestable. When run, the result will look like this: +------------+------------+-----------------+-------------------------------------+ | name | location | resourceGroup | properties.hardwareProfile.vmSize | +============+============+=================+=====================================+ | my_vm_name | westus | my_vm_rg | Standard_D2_v2 | +------------+------------+-----------------+-------------------------------------+ The fields produced by custodian report vary by resource (i.e. properties.hardwareProfile.vmSize); however, you can add additional fields to your report by using the --field parameter. For example, if you want to see a list of tags on this resource: custodian report --output-dir=. --format grid --field tags=tags custodian.yml Result: +------------+------------+-----------------+-------------------------------------+----------------------------+ | name | location | resourceGroup | properties.hardwareProfile.vmSize | tagHeader | +============+============+=================+=====================================+============================+ | my_vm_name | westus | my_vm_rg | Standard_D2_v2 | {'custodian-tagged': True} | +------------+------------+-----------------+-------------------------------------+----------------------------+ The field parameter has the format --field header=field where header is the name of the column header in the report, and field is the JMESPath of a specific field to include in the output. All available fields for a resource can be found in the resources.json file. Next Steps Notify users of policy violations using a Logic App <azure_examples_notifications_logic_app> {.interpreted-text role=\"ref\"} More example policies <azure_examples> {.interpreted-text role=\"ref\"}","title":"Gettingstarted"},{"location":"azure/gettingstarted/#getting-started-azure_gettingstarted","text":"","title":"Getting Started {#azure_gettingstarted}"},{"location":"azure/gettingstarted/#write-your-first-policy-azure_write-policy","text":"Cloud Custodian is a stateless rules engine that filters Azure resources and takes actions on based on policies that you define. Cloud Custodian policies are expressed in YAML and include the following: The type of resource to run the policy against Filters to narrow down the set of resources Actions to take on the filtered set of resources Our first policy filters to a VM of a specific name, then adds the tag Hello: World . Create a file named custodian.yml with the following content. Update my_vm_name to match an existing VM. Note: Some text editors (VSCode) inject invalid whitespace characters when copy/pasting YAML from a browser policies: - name: my-first-policy description: | Adds a tag to a virtual machines resource: azure.vm filters: - type: value key: name value: my_vm_name actions: - type: tag tag: Hello value: World","title":"Write your first policy {#azure_write-policy}"},{"location":"azure/gettingstarted/#run-your-policy-azure_run-policy","text":"Choose one of the supported authentication mechanisms , and either log in to Azure CLI or set environment variables as documented in azure_authentication {.interpreted-text role=\"ref\"}. custodian run --output-dir=. custodian.yml If successful, you should see output like the following on the command line: 2016-12-20 08:35:06,133: custodian.policy:INFO Running policy my-first-policy resource: azure.vm 2016-12-20 08:35:07,514: custodian.policy:INFO policy: my-first-policy resource:azure.vm has count:1 time:1.38 2016-12-20 08:35:08,188: custodian.policy:INFO policy: my-first-policy action: tag: 1 execution_time: 0.67 You should also find a new my-first-policy directory with a log and a resources.json file. See filters {.interpreted-text role=\"ref\"} for more information on the features of the Value filter used in this sample.","title":"Run your policy {#azure_run-policy}"},{"location":"azure/gettingstarted/#optional-run-your-policy-with-azure-monitoring-monitor-azure-cc","text":"Cloud Custodian policies can emit logs and metrics to Application Insights when the policy executes. Please refer to the azure_monitoring {.interpreted-text role=\"ref\"} section for further details.","title":"(Optional) Run your policy with Azure Monitoring {#monitor-azure-cc}"},{"location":"azure/gettingstarted/#view-policy-results-azure_view_policy_reults","text":"The resources.json file shows you the raw data that results from your policy after filtering. This file can help you understand the fields available for your resources while developing your policy.","title":"View policy results {#azure_view_policy_reults}"},{"location":"azure/gettingstarted/#custodian-report","text":"Custodian has a report feature that allows the resources.json file to be viewed more concisely. By default, this will output data in a CSV format, but report also provides other output formats such as grid that are more digestable. When run, the result will look like this: +------------+------------+-----------------+-------------------------------------+ | name | location | resourceGroup | properties.hardwareProfile.vmSize | +============+============+=================+=====================================+ | my_vm_name | westus | my_vm_rg | Standard_D2_v2 | +------------+------------+-----------------+-------------------------------------+ The fields produced by custodian report vary by resource (i.e. properties.hardwareProfile.vmSize); however, you can add additional fields to your report by using the --field parameter. For example, if you want to see a list of tags on this resource: custodian report --output-dir=. --format grid --field tags=tags custodian.yml Result: +------------+------------+-----------------+-------------------------------------+----------------------------+ | name | location | resourceGroup | properties.hardwareProfile.vmSize | tagHeader | +============+============+=================+=====================================+============================+ | my_vm_name | westus | my_vm_rg | Standard_D2_v2 | {'custodian-tagged': True} | +------------+------------+-----------------+-------------------------------------+----------------------------+ The field parameter has the format --field header=field where header is the name of the column header in the report, and field is the JMESPath of a specific field to include in the output. All available fields for a resource can be found in the resources.json file.","title":"Custodian Report"},{"location":"azure/gettingstarted/#next-steps","text":"Notify users of policy violations using a Logic App <azure_examples_notifications_logic_app> {.interpreted-text role=\"ref\"} More example policies <azure_examples> {.interpreted-text role=\"ref\"}","title":"Next Steps"},{"location":"azure/gettingstarted.rst/","text":"Getting Started {#azure_gettingstarted} Write your first policy {#azure_write-policy} Cloud Custodian is a stateless rules engine that filters Azure resources and takes actions on based on policies that you define. Cloud Custodian policies are expressed in YAML and include the following: The type of resource to run the policy against Filters to narrow down the set of resources Actions to take on the filtered set of resources Our first policy filters to a VM of a specific name, then adds the tag Hello: World . Create a file named custodian.yml with the following content. Update my_vm_name to match an existing VM. Note: Some text editors (VSCode) inject invalid whitespace characters when copy/pasting YAML from a browser policies: - name: my-first-policy description: | Adds a tag to a virtual machines resource: azure.vm filters: - type: value key: name value: my_vm_name actions: - type: tag tag: Hello value: World Run your policy {#azure_run-policy} Choose one of the supported authentication mechanisms , and either log in to Azure CLI or set environment variables as documented in azure_authentication {.interpreted-text role=\"ref\"}. custodian run --output-dir=. custodian.yml If successful, you should see output like the following on the command line: 2016-12-20 08:35:06,133: custodian.policy:INFO Running policy my-first-policy resource: azure.vm 2016-12-20 08:35:07,514: custodian.policy:INFO policy: my-first-policy resource:azure.vm has count:1 time:1.38 2016-12-20 08:35:08,188: custodian.policy:INFO policy: my-first-policy action: tag: 1 execution_time: 0.67 You should also find a new my-first-policy directory with a log and a resources.json file. See filters {.interpreted-text role=\"ref\"} for more information on the features of the Value filter used in this sample. (Optional) Run your policy with Azure Monitoring {#monitor-azure-cc} Cloud Custodian policies can emit logs and metrics to Application Insights when the policy executes. Please refer to the azure_monitoring {.interpreted-text role=\"ref\"} section for further details. View policy results {#azure_view_policy_reults} The resources.json file shows you the raw data that results from your policy after filtering. This file can help you understand the fields available for your resources while developing your policy. Custodian Report Custodian has a report feature that allows the resources.json file to be viewed more concisely. By default, this will output data in a CSV format, but report also provides other output formats such as grid that are more digestable. When run, the result will look like this: +------------+------------+-----------------+-------------------------------------+ | name | location | resourceGroup | properties.hardwareProfile.vmSize | +============+============+=================+=====================================+ | my_vm_name | westus | my_vm_rg | Standard_D2_v2 | +------------+------------+-----------------+-------------------------------------+ The fields produced by custodian report vary by resource (i.e. properties.hardwareProfile.vmSize); however, you can add additional fields to your report by using the --field parameter. For example, if you want to see a list of tags on this resource: custodian report --output-dir=. --format grid --field tags=tags custodian.yml Result: +------------+------------+-----------------+-------------------------------------+----------------------------+ | name | location | resourceGroup | properties.hardwareProfile.vmSize | tagHeader | +============+============+=================+=====================================+============================+ | my_vm_name | westus | my_vm_rg | Standard_D2_v2 | {'custodian-tagged': True} | +------------+------------+-----------------+-------------------------------------+----------------------------+ The field parameter has the format --field header=field where header is the name of the column header in the report, and field is the JMESPath of a specific field to include in the output. All available fields for a resource can be found in the resources.json file. Next Steps Notify users of policy violations using a Logic App <azure_examples_notifications_logic_app> {.interpreted-text role=\"ref\"} More example policies <azure_examples> {.interpreted-text role=\"ref\"}","title":"Gettingstarted.rst"},{"location":"azure/gettingstarted.rst/#getting-started-azure_gettingstarted","text":"","title":"Getting Started {#azure_gettingstarted}"},{"location":"azure/gettingstarted.rst/#write-your-first-policy-azure_write-policy","text":"Cloud Custodian is a stateless rules engine that filters Azure resources and takes actions on based on policies that you define. Cloud Custodian policies are expressed in YAML and include the following: The type of resource to run the policy against Filters to narrow down the set of resources Actions to take on the filtered set of resources Our first policy filters to a VM of a specific name, then adds the tag Hello: World . Create a file named custodian.yml with the following content. Update my_vm_name to match an existing VM. Note: Some text editors (VSCode) inject invalid whitespace characters when copy/pasting YAML from a browser policies: - name: my-first-policy description: | Adds a tag to a virtual machines resource: azure.vm filters: - type: value key: name value: my_vm_name actions: - type: tag tag: Hello value: World","title":"Write your first policy {#azure_write-policy}"},{"location":"azure/gettingstarted.rst/#run-your-policy-azure_run-policy","text":"Choose one of the supported authentication mechanisms , and either log in to Azure CLI or set environment variables as documented in azure_authentication {.interpreted-text role=\"ref\"}. custodian run --output-dir=. custodian.yml If successful, you should see output like the following on the command line: 2016-12-20 08:35:06,133: custodian.policy:INFO Running policy my-first-policy resource: azure.vm 2016-12-20 08:35:07,514: custodian.policy:INFO policy: my-first-policy resource:azure.vm has count:1 time:1.38 2016-12-20 08:35:08,188: custodian.policy:INFO policy: my-first-policy action: tag: 1 execution_time: 0.67 You should also find a new my-first-policy directory with a log and a resources.json file. See filters {.interpreted-text role=\"ref\"} for more information on the features of the Value filter used in this sample.","title":"Run your policy {#azure_run-policy}"},{"location":"azure/gettingstarted.rst/#optional-run-your-policy-with-azure-monitoring-monitor-azure-cc","text":"Cloud Custodian policies can emit logs and metrics to Application Insights when the policy executes. Please refer to the azure_monitoring {.interpreted-text role=\"ref\"} section for further details.","title":"(Optional) Run your policy with Azure Monitoring {#monitor-azure-cc}"},{"location":"azure/gettingstarted.rst/#view-policy-results-azure_view_policy_reults","text":"The resources.json file shows you the raw data that results from your policy after filtering. This file can help you understand the fields available for your resources while developing your policy.","title":"View policy results {#azure_view_policy_reults}"},{"location":"azure/gettingstarted.rst/#custodian-report","text":"Custodian has a report feature that allows the resources.json file to be viewed more concisely. By default, this will output data in a CSV format, but report also provides other output formats such as grid that are more digestable. When run, the result will look like this: +------------+------------+-----------------+-------------------------------------+ | name | location | resourceGroup | properties.hardwareProfile.vmSize | +============+============+=================+=====================================+ | my_vm_name | westus | my_vm_rg | Standard_D2_v2 | +------------+------------+-----------------+-------------------------------------+ The fields produced by custodian report vary by resource (i.e. properties.hardwareProfile.vmSize); however, you can add additional fields to your report by using the --field parameter. For example, if you want to see a list of tags on this resource: custodian report --output-dir=. --format grid --field tags=tags custodian.yml Result: +------------+------------+-----------------+-------------------------------------+----------------------------+ | name | location | resourceGroup | properties.hardwareProfile.vmSize | tagHeader | +============+============+=================+=====================================+============================+ | my_vm_name | westus | my_vm_rg | Standard_D2_v2 | {'custodian-tagged': True} | +------------+------------+-----------------+-------------------------------------+----------------------------+ The field parameter has the format --field header=field where header is the name of the column header in the report, and field is the JMESPath of a specific field to include in the output. All available fields for a resource can be found in the resources.json file.","title":"Custodian Report"},{"location":"azure/gettingstarted.rst/#next-steps","text":"Notify users of policy violations using a Logic App <azure_examples_notifications_logic_app> {.interpreted-text role=\"ref\"} More example policies <azure_examples> {.interpreted-text role=\"ref\"}","title":"Next Steps"},{"location":"azure/advanced/","text":"Advanced Usage Topics for advanced usage of the Azure provider ::: {.toctree maxdepth=\"2\" titlesonly=\"\" glob=\"\"} multiplesubs azurepolicy contribute :::","title":"Index"},{"location":"azure/advanced/#advanced-usage","text":"Topics for advanced usage of the Azure provider ::: {.toctree maxdepth=\"2\" titlesonly=\"\" glob=\"\"} multiplesubs azurepolicy contribute :::","title":"Advanced Usage"},{"location":"azure/advanced/azurepolicy/","text":"Azure Policy Comparison {#azure_azurepolicy} Cloud Custodian and Azure Policy have significant overlap in scenarios they can accomplish with regard to compliance implementations. These areas of overlap can make it unclear to new users which tool is appropriate for a specific task. Both tools apply rules to resources, and have some ability to take actions, but they do it in very different ways which are complimentary rather than redundant. Azure Policy is reliable and efficient for building a custom validation layer on deployments to prevent deviation from customer defined rules. There is minimal extensibility possible and it is not a general purpose Azure rules engine. Cloud Custodian can not prevent deployments, but rather runs periodically or based on events in the subscription. It has many purpose-built Filters and Actions that help with common scenarios like configuring Firewalls, identifying expensive resources, and notifying users about violations. Rules can do anything the Azure SDK can do, and you can implement custom ones in Python if you need to. When reviewing your requirements, we recommend first identifying the requirements that can be implemented via Azure Policy. Custodian can then be used to implement the remaining requirements. Custodian is also frequently used to add a second layer of protection or mitigation actions to requirements covered by Azure Policy. In the event that you prefer to manage everything through Cloud Custodian for consistency in a multi-cloud environment you can take advantage of the Custodian support for managing Azure Policy with Custodian filters and actions. This way your Custodian rules can actually ensure Azure Policy is configured correctly. Examples In Azure Policy we can require users to include an owner tag on every Virtual Machine. If they create any deployment without one the Portal or API will return an error code. The users will manually update their ARM templates or use the Tag UI in the Azure Portal during resource creation. Here is what that Azure Policy might look like: { \"properties\": { \"displayName\": \"Enforce tag and its value on resource groups\", \"description\": \"Enforces a required tag and its value on resource groups.\", \"mode\": \"All\", \"parameters\": { \"tagName\": { \"type\": \"String\", \"metadata\": { \"description\": \"Name of the tag, such as costCenter\" } } }, \"policyRule\": { \"if\": { \"allOf\": [ { \"field\": \"type\", \"equals\": \"Microsoft.Compute/virtualMachine\" }, { \"not\": { \"field\": \"[concat('tags[',parameters('tagName'), ']')]\", \"exists\": \"true\" } } ] }, \"then\": { \"effect\": \"deny\" } } } } In Custodian we can deploy a Custodian Policy which is triggered by Virtual Machine creation events and automatically finds the identity of the creator and writes the tag without any required user action. policies: - name: azure-auto-tag-creator mode: type: azure-event-grid events: ['VmWrite'] resource: azure.vm description: Tag all new VMs with the 'Creator Email' tag. actions: - type: auto-tag-user tag: CreatorEmail A simple example of a non-compliance related rule might be cleaning up orphaned resources. This is out-of-scope for Azure Policy because it is not a deployment property filter. With Cloud Custodian a policy to find and delete unused Network Interfaces would look like this: policies: - name: orphaned-nic resource: azure.networkinterface filters: - type: value key: properties.virtualMachine value: null actions: - type: delete","title":"Azurepolicy"},{"location":"azure/advanced/azurepolicy/#azure-policy-comparison-azure_azurepolicy","text":"Cloud Custodian and Azure Policy have significant overlap in scenarios they can accomplish with regard to compliance implementations. These areas of overlap can make it unclear to new users which tool is appropriate for a specific task. Both tools apply rules to resources, and have some ability to take actions, but they do it in very different ways which are complimentary rather than redundant. Azure Policy is reliable and efficient for building a custom validation layer on deployments to prevent deviation from customer defined rules. There is minimal extensibility possible and it is not a general purpose Azure rules engine. Cloud Custodian can not prevent deployments, but rather runs periodically or based on events in the subscription. It has many purpose-built Filters and Actions that help with common scenarios like configuring Firewalls, identifying expensive resources, and notifying users about violations. Rules can do anything the Azure SDK can do, and you can implement custom ones in Python if you need to. When reviewing your requirements, we recommend first identifying the requirements that can be implemented via Azure Policy. Custodian can then be used to implement the remaining requirements. Custodian is also frequently used to add a second layer of protection or mitigation actions to requirements covered by Azure Policy. In the event that you prefer to manage everything through Cloud Custodian for consistency in a multi-cloud environment you can take advantage of the Custodian support for managing Azure Policy with Custodian filters and actions. This way your Custodian rules can actually ensure Azure Policy is configured correctly.","title":"Azure Policy Comparison {#azure_azurepolicy}"},{"location":"azure/advanced/azurepolicy/#examples","text":"In Azure Policy we can require users to include an owner tag on every Virtual Machine. If they create any deployment without one the Portal or API will return an error code. The users will manually update their ARM templates or use the Tag UI in the Azure Portal during resource creation. Here is what that Azure Policy might look like: { \"properties\": { \"displayName\": \"Enforce tag and its value on resource groups\", \"description\": \"Enforces a required tag and its value on resource groups.\", \"mode\": \"All\", \"parameters\": { \"tagName\": { \"type\": \"String\", \"metadata\": { \"description\": \"Name of the tag, such as costCenter\" } } }, \"policyRule\": { \"if\": { \"allOf\": [ { \"field\": \"type\", \"equals\": \"Microsoft.Compute/virtualMachine\" }, { \"not\": { \"field\": \"[concat('tags[',parameters('tagName'), ']')]\", \"exists\": \"true\" } } ] }, \"then\": { \"effect\": \"deny\" } } } } In Custodian we can deploy a Custodian Policy which is triggered by Virtual Machine creation events and automatically finds the identity of the creator and writes the tag without any required user action. policies: - name: azure-auto-tag-creator mode: type: azure-event-grid events: ['VmWrite'] resource: azure.vm description: Tag all new VMs with the 'Creator Email' tag. actions: - type: auto-tag-user tag: CreatorEmail A simple example of a non-compliance related rule might be cleaning up orphaned resources. This is out-of-scope for Azure Policy because it is not a deployment property filter. With Cloud Custodian a policy to find and delete unused Network Interfaces would look like this: policies: - name: orphaned-nic resource: azure.networkinterface filters: - type: value key: properties.virtualMachine value: null actions: - type: delete","title":"Examples"},{"location":"azure/advanced/contribute/","text":"Developer Guide {#azure_contribute} The c7n developer install includes c7n_azure. A shortcut for creating a virtual env for development is available in the makefile: $ make install $ source bin/activate This creates a virtual env in your enlistment and installs all packages as editable. You can also simply do [pip install -r requirements-dev.txt]{.title-ref} to install all dev/test dependencies. Adding New Azure Resources Install Azure Dependencies Custodian interfaces with ARM resources using Azure\\'s SDKs. Install the resources SDK in setup.py . install_requires=[\"azure-mgmt-authorization\", ... \"azure-mgmt-containerregistry\", ... Create New Azure Resource Create your new Azure Resource. service : The Azure SDK dependency added in step 1. client : Client class name of the Azure Resource SDK of the resource you added. enum_spec : Is a tuple of (enum_operation, list_operation, extra_args). The resource SDK client will have a list of operations this resource has. : Place the name of the property as the enum_operation. Next, put [list]{.title-ref} as the operations. default_report_fields : Fields that are used for running custodian report . from c7n_azure.provider import resources from c7n_azure.resources.arm import ArmResourceManager @resources.register('containerregistry') class ContainerRegistry(ArmResourceManager): class resource_type(ArmResourceManager.resource_type): service = 'azure.mgmt.containerregistry' client = 'ContainerRegistryManagementClient' enum_spec = ('registries', 'list', None) default_report_fields = ( 'name', 'location', 'resourceGroup', 'sku.name ) Load New Azure Resource Once the required dependencies are installed and created the new Azure Resource, custodian will load all registered resources. Import the resource in entry.py . import c7n_azure.resources.container_registry Testing Tests for c7n_azure run automatically with other Custodian tests. See Testing for Developers <developer-tests> {.interpreted-text role=\"ref\"} for information on how to run Tox. If you\\'d like to run tests at the command line or in your IDE then reference [tox.ini]{.title-ref} to see the required environment variables and command lines for running [pytest]{.title-ref}. Test framework c7n_azure uses [VCR.py]{.title-ref} for tests. This framework is used for tests in the official Azure Python SDK. VCRpy documentation can be found here: VCR.py documentation . ARM templates To ensure VCR cassettes can be easily re-recorded, there are ARM templates to deploy Azure tests infrastructure. These templates will allow you to provision real Azure resources appropriate for recreating the VCR cassettes used by the unit tests. They will let you run the unit tests against real resources. ARM templates and helper scripts can be found in [tools/c7n_azure/tests/templates]{.title-ref} folder. There are two scripts [provision.sh]{.title-ref} and [cleanup.sh]{.title-ref} to provision and delete resources. These scripts will provision or delete all ARM templates ([.json files]{.title-ref}) in this directory using resource groups named after the template files ([test_\\<filename>]{.title-ref}). This scripts use Azure CLI, so you need to [az login]{.title-ref} and [az account set -s \\'subscription name\\']{.title-ref} first. You can optionally pass a list of file names without extension to the scripts to act only on those templates: provision.sh vm storage cleanup.sh storage or do everything provision.sh If test method requires real infrastructure, please decorate this method with the ARM template file name to ensure this test can automatically create required infrastructure if needed. @arm_template('template.json') def test_template(self): Cassettes [AzureVCRBaseTest]{.title-ref} attempts to automatically obscure keys and other secrets in cassettes and replace subscription ids, but it is required to verify cassettes don\\'t contain any sensitive information before submitting. For long standing operations cassette can be modified to reduce test execution time (in case recorded cassette contains some responses with Retry-After headers or Azure SDK waits until resource is provisioned). Running tests You can use [tox]{.title-ref} to run all tests or instead you can use [pytest]{.title-ref} and run only Azure tests (or only specific set of tests). Running recorded tests still requires some authentication, it is possible to use fake data for authorization token and subscription id. export AZURE_ACCESS_TOKEN=fake_token export AZURE_SUBSCRIPTION_ID=ea42f556-5106-4743-99b0-c129bfa71a47 pytest tools/c7n_azure/tests","title":"Contribute"},{"location":"azure/advanced/contribute/#developer-guide-azure_contribute","text":"The c7n developer install includes c7n_azure. A shortcut for creating a virtual env for development is available in the makefile: $ make install $ source bin/activate This creates a virtual env in your enlistment and installs all packages as editable. You can also simply do [pip install -r requirements-dev.txt]{.title-ref} to install all dev/test dependencies.","title":"Developer Guide {#azure_contribute}"},{"location":"azure/advanced/contribute/#adding-new-azure-resources","text":"","title":"Adding New Azure Resources"},{"location":"azure/advanced/contribute/#install-azure-dependencies","text":"Custodian interfaces with ARM resources using Azure\\'s SDKs. Install the resources SDK in setup.py . install_requires=[\"azure-mgmt-authorization\", ... \"azure-mgmt-containerregistry\", ...","title":"Install Azure Dependencies"},{"location":"azure/advanced/contribute/#create-new-azure-resource","text":"Create your new Azure Resource. service : The Azure SDK dependency added in step 1. client : Client class name of the Azure Resource SDK of the resource you added. enum_spec : Is a tuple of (enum_operation, list_operation, extra_args). The resource SDK client will have a list of operations this resource has. : Place the name of the property as the enum_operation. Next, put [list]{.title-ref} as the operations. default_report_fields : Fields that are used for running custodian report . from c7n_azure.provider import resources from c7n_azure.resources.arm import ArmResourceManager @resources.register('containerregistry') class ContainerRegistry(ArmResourceManager): class resource_type(ArmResourceManager.resource_type): service = 'azure.mgmt.containerregistry' client = 'ContainerRegistryManagementClient' enum_spec = ('registries', 'list', None) default_report_fields = ( 'name', 'location', 'resourceGroup', 'sku.name )","title":"Create New Azure Resource"},{"location":"azure/advanced/contribute/#load-new-azure-resource","text":"Once the required dependencies are installed and created the new Azure Resource, custodian will load all registered resources. Import the resource in entry.py . import c7n_azure.resources.container_registry","title":"Load New Azure Resource"},{"location":"azure/advanced/contribute/#testing","text":"Tests for c7n_azure run automatically with other Custodian tests. See Testing for Developers <developer-tests> {.interpreted-text role=\"ref\"} for information on how to run Tox. If you\\'d like to run tests at the command line or in your IDE then reference [tox.ini]{.title-ref} to see the required environment variables and command lines for running [pytest]{.title-ref}.","title":"Testing"},{"location":"azure/advanced/contribute/#test-framework","text":"c7n_azure uses [VCR.py]{.title-ref} for tests. This framework is used for tests in the official Azure Python SDK. VCRpy documentation can be found here: VCR.py documentation .","title":"Test framework"},{"location":"azure/advanced/contribute/#arm-templates","text":"To ensure VCR cassettes can be easily re-recorded, there are ARM templates to deploy Azure tests infrastructure. These templates will allow you to provision real Azure resources appropriate for recreating the VCR cassettes used by the unit tests. They will let you run the unit tests against real resources. ARM templates and helper scripts can be found in [tools/c7n_azure/tests/templates]{.title-ref} folder. There are two scripts [provision.sh]{.title-ref} and [cleanup.sh]{.title-ref} to provision and delete resources. These scripts will provision or delete all ARM templates ([.json files]{.title-ref}) in this directory using resource groups named after the template files ([test_\\<filename>]{.title-ref}). This scripts use Azure CLI, so you need to [az login]{.title-ref} and [az account set -s \\'subscription name\\']{.title-ref} first. You can optionally pass a list of file names without extension to the scripts to act only on those templates: provision.sh vm storage cleanup.sh storage or do everything provision.sh If test method requires real infrastructure, please decorate this method with the ARM template file name to ensure this test can automatically create required infrastructure if needed. @arm_template('template.json') def test_template(self):","title":"ARM templates"},{"location":"azure/advanced/contribute/#cassettes","text":"[AzureVCRBaseTest]{.title-ref} attempts to automatically obscure keys and other secrets in cassettes and replace subscription ids, but it is required to verify cassettes don\\'t contain any sensitive information before submitting. For long standing operations cassette can be modified to reduce test execution time (in case recorded cassette contains some responses with Retry-After headers or Azure SDK waits until resource is provisioned).","title":"Cassettes"},{"location":"azure/advanced/contribute/#running-tests","text":"You can use [tox]{.title-ref} to run all tests or instead you can use [pytest]{.title-ref} and run only Azure tests (or only specific set of tests). Running recorded tests still requires some authentication, it is possible to use fake data for authorization token and subscription id. export AZURE_ACCESS_TOKEN=fake_token export AZURE_SUBSCRIPTION_ID=ea42f556-5106-4743-99b0-c129bfa71a47 pytest tools/c7n_azure/tests","title":"Running tests"},{"location":"azure/advanced/multiplesubs/","text":"Running against multiple subscriptions {#azure_multiplesubs} The C7N-Org tool supports running policies against multiple subscriptions. See the C7N-Org Readme for more information. If you\\'re using an Azure Service Principal for executing c7n-org you\\'ll need to ensure that the principal has access to multiple subscriptions. For instructions on creating a Service Principal and granting access across subscriptions, see azure_authentication {.interpreted-text role=\"ref\"} Note : There are pending issues with running C7N-Org on Windows. It may be required to use the --debug flag when running on Windows.","title":"Multiplesubs"},{"location":"azure/advanced/multiplesubs/#running-against-multiple-subscriptions-azure_multiplesubs","text":"The C7N-Org tool supports running policies against multiple subscriptions. See the C7N-Org Readme for more information. If you\\'re using an Azure Service Principal for executing c7n-org you\\'ll need to ensure that the principal has access to multiple subscriptions. For instructions on creating a Service Principal and granting access across subscriptions, see azure_authentication {.interpreted-text role=\"ref\"} Note : There are pending issues with running C7N-Org on Windows. It may be required to use the --debug flag when running on Windows.","title":"Running against multiple subscriptions {#azure_multiplesubs}"},{"location":"azure/configuration/","text":"Configuring Azure Policies {#azure_configuration} Understand your options for authentication, hosting strategies, and monitoring the results of policies. ::: {.toctree maxdepth=\"2\" titlesonly=\"\" glob=\"\"} authentication monitoring ::: Hosting Options The Azure provider for Cloud Custodian can be hosted by Azure Functions or in a containerized environment like ACI or AKS. Both hosting options have periodic and event based execution modes. For a quick and inexpensive start to running custodian policies, Azure Functions are a good hosting strategy. The Azure Container Host requires more up-front configuration, but can make running a large number of policies against multiple subscriptions more maintainable. ::: {.toctree maxdepth=\"2\" titlesonly=\"\" glob=\"\"} functionshosting containerhosting ./acitutorial.rst ./helmtutorial.rst :::","title":"Index"},{"location":"azure/configuration/#configuring-azure-policies-azure_configuration","text":"Understand your options for authentication, hosting strategies, and monitoring the results of policies. ::: {.toctree maxdepth=\"2\" titlesonly=\"\" glob=\"\"} authentication monitoring :::","title":"Configuring Azure Policies {#azure_configuration}"},{"location":"azure/configuration/#hosting-options","text":"The Azure provider for Cloud Custodian can be hosted by Azure Functions or in a containerized environment like ACI or AKS. Both hosting options have periodic and event based execution modes. For a quick and inexpensive start to running custodian policies, Azure Functions are a good hosting strategy. The Azure Container Host requires more up-front configuration, but can make running a large number of policies against multiple subscriptions more maintainable. ::: {.toctree maxdepth=\"2\" titlesonly=\"\" glob=\"\"} functionshosting containerhosting ./acitutorial.rst ./helmtutorial.rst :::","title":"Hosting Options"},{"location":"azure/configuration/acitutorial/","text":"Tutorial - ACI Deployment {#azure_configuration_acitutorial} This is a step-by-step tutorial for deploying the Azure Container Host in ACI that runs custodian policies that are uploaded to an Azure Storage Account. This tutorial makes use of the Azure CLI , so make sure that it is installed and you are logged in to your subscription. 1. Create a Resource Group This will hold all of the Azure resources created in this tutorial. You could use an existing resource group too. az group create --name c7n-aci-tutorial --location westus2 2. Create a Storage Account This storage account will hold all of the blobs and queues used by the container host. You could also use an existing resource here or multiple storage accounts. We will also create two blob containers. One will host the uploaded policies, and the other will store the custodian output files of running policies. az storage account create --resource-group c7n-aci-tutorial --name c7nstorage account_key=$(az storage account keys list --account-name c7nstorage --query \"[0].value\" --output tsv) az storage container create --account-name c7nstorage --account-key $account_key --name c7n-aci-policies az storage container create --account-name c7nstorage --account-key $account_key --name c7n-aci-logs 3. Create a Managed Identity This identity will be used with MSI on the ACI instance. You could also use an existing identity. az identity create --resource-group c7n-aci-tutorial --name c7n-aci This identity will need the proper permissions to interact with the storage account and any other azure resources that the Azure Container Host interacts with. This includes the resources that policies will run against. The simplest way to grant these permissions is to make the identity a Contributor on the target subscription and a Storage Blob Data Contributor and Storage Queue Data Contributor on the storage account created above. The <Target Subscription Resource Id> should be in the format /subscriptions/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx . The <Storage Account Resource Id> should be in the format /subscriptions/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx/resourceGroups/c7n-aci-tutorial/providers/Microsoft.Storage/storageAccounts/c7nstorage , and is available in the json output from creating the storage account in step 2. az role assignment create --assignee <Managed Identity Client Id> \\ --role \"Contributor\" --scope <Target Subscription Resource Id> az role assignment create --assignee <Managed Identity Client Id> \\ --role \"Storage Blob Data Contributor\" --scope <Storage Account Resource Id> az role assignment create --assignee <Managed Identity Client Id> \\ --role \"Storage Queue Data Contributor\" --scope <Storage Account Resource Id> 4. Create an Application Insights Instance We will create an Application Insights instance to gather logs and telemetry generated by the running container host. # You may need to add the application-insights extension az extension add -n application-insights az monitor app-insights component create --resource-group c7n-aci-tutorial --app c7n-aci-insights --location westus2 5. Create the ACI Container Host Now all of the required Azure resources should exist, and we can deploy the Container Host ARM template. Create a parameters file called c7n-aci-tutorial.json . See the Container Host Documentation<azure_containerhosting> {.interpreted-text role=\"ref\"} for information on filling out the environment variables. { \"$schema\": \"https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#\", \"contentVersion\": \"1.0.0.0\", \"parameters\": { \"aci_name\": { \"value\": \"cloud-custodian\" }, \"user_assigned_identity_name\": { \"value\": \"c7n-aci\" }, \"azure_subscription_id\": { \"value\": \"<Target Subscription ID>\" }, \"azure_container_queue_name\": { \"value\": \"c7n-aci-events\" }, \"azure_container_policy_uri\": { \"value\": \"<Policy Storage Container URI>\" }, \"azure_container_storage_resource_id\": { \"value\": \"<Storage Account Resource ID>\" }, \"azure_container_log_group\": { \"value\": \"azure://<Instrumentation Key>\" }, \"azure_container_metrics\": { \"value\": \"azure://<Instrumentation Key>\" }, \"azure_container_output_dir\": { \"value\": \"<Log Storage Container URI>\" } } } And deploy with this command az group deployment create \\ --resource-group c7n-aci-tutorial \\ --template-file tools/ops/azure/container-host/aci/aci-template.json \\ --parameters @c7n-aci-tutorial.json Once this deployment succeeds, the Azure Container Host should be running! You can see the logs with the following command: az container logs --resource-group c7n-aci-tutorial --name cloud-custodian --follow 6. Upload a Custodian Policy Finally, create a custodian policy called find-c7nstorage.yaml . This policy will just find the storage account we made earlier. We\\'ll set the mode to run every minute for easier testing. policies: - name: find-c7nstorage resource: azure.storage mode: type: container-periodic schedule: \"* * * * *\" # Run every minute as an example filters: - type: value key: name op: eq value: c7nstorage Upload this file to the policy storage container. Within a few minutes, the container host should pick it up and begin executing it. az storage blob upload --account-name c7nstorage --account-key $account_key \\ --container-name c7n-aci-policies --file find-c7nstorage.yaml --name find-c7nstorage.yaml","title":"Acitutorial"},{"location":"azure/configuration/acitutorial/#tutorial-aci-deployment-azure_configuration_acitutorial","text":"This is a step-by-step tutorial for deploying the Azure Container Host in ACI that runs custodian policies that are uploaded to an Azure Storage Account. This tutorial makes use of the Azure CLI , so make sure that it is installed and you are logged in to your subscription.","title":"Tutorial - ACI Deployment {#azure_configuration_acitutorial}"},{"location":"azure/configuration/acitutorial/#1-create-a-resource-group","text":"This will hold all of the Azure resources created in this tutorial. You could use an existing resource group too. az group create --name c7n-aci-tutorial --location westus2","title":"1. Create a Resource Group"},{"location":"azure/configuration/acitutorial/#2-create-a-storage-account","text":"This storage account will hold all of the blobs and queues used by the container host. You could also use an existing resource here or multiple storage accounts. We will also create two blob containers. One will host the uploaded policies, and the other will store the custodian output files of running policies. az storage account create --resource-group c7n-aci-tutorial --name c7nstorage account_key=$(az storage account keys list --account-name c7nstorage --query \"[0].value\" --output tsv) az storage container create --account-name c7nstorage --account-key $account_key --name c7n-aci-policies az storage container create --account-name c7nstorage --account-key $account_key --name c7n-aci-logs","title":"2. Create a Storage Account"},{"location":"azure/configuration/acitutorial/#3-create-a-managed-identity","text":"This identity will be used with MSI on the ACI instance. You could also use an existing identity. az identity create --resource-group c7n-aci-tutorial --name c7n-aci This identity will need the proper permissions to interact with the storage account and any other azure resources that the Azure Container Host interacts with. This includes the resources that policies will run against. The simplest way to grant these permissions is to make the identity a Contributor on the target subscription and a Storage Blob Data Contributor and Storage Queue Data Contributor on the storage account created above. The <Target Subscription Resource Id> should be in the format /subscriptions/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx . The <Storage Account Resource Id> should be in the format /subscriptions/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx/resourceGroups/c7n-aci-tutorial/providers/Microsoft.Storage/storageAccounts/c7nstorage , and is available in the json output from creating the storage account in step 2. az role assignment create --assignee <Managed Identity Client Id> \\ --role \"Contributor\" --scope <Target Subscription Resource Id> az role assignment create --assignee <Managed Identity Client Id> \\ --role \"Storage Blob Data Contributor\" --scope <Storage Account Resource Id> az role assignment create --assignee <Managed Identity Client Id> \\ --role \"Storage Queue Data Contributor\" --scope <Storage Account Resource Id>","title":"3. Create a Managed Identity"},{"location":"azure/configuration/acitutorial/#4-create-an-application-insights-instance","text":"We will create an Application Insights instance to gather logs and telemetry generated by the running container host. # You may need to add the application-insights extension az extension add -n application-insights az monitor app-insights component create --resource-group c7n-aci-tutorial --app c7n-aci-insights --location westus2","title":"4. Create an Application Insights Instance"},{"location":"azure/configuration/acitutorial/#5-create-the-aci-container-host","text":"Now all of the required Azure resources should exist, and we can deploy the Container Host ARM template. Create a parameters file called c7n-aci-tutorial.json . See the Container Host Documentation<azure_containerhosting> {.interpreted-text role=\"ref\"} for information on filling out the environment variables. { \"$schema\": \"https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#\", \"contentVersion\": \"1.0.0.0\", \"parameters\": { \"aci_name\": { \"value\": \"cloud-custodian\" }, \"user_assigned_identity_name\": { \"value\": \"c7n-aci\" }, \"azure_subscription_id\": { \"value\": \"<Target Subscription ID>\" }, \"azure_container_queue_name\": { \"value\": \"c7n-aci-events\" }, \"azure_container_policy_uri\": { \"value\": \"<Policy Storage Container URI>\" }, \"azure_container_storage_resource_id\": { \"value\": \"<Storage Account Resource ID>\" }, \"azure_container_log_group\": { \"value\": \"azure://<Instrumentation Key>\" }, \"azure_container_metrics\": { \"value\": \"azure://<Instrumentation Key>\" }, \"azure_container_output_dir\": { \"value\": \"<Log Storage Container URI>\" } } } And deploy with this command az group deployment create \\ --resource-group c7n-aci-tutorial \\ --template-file tools/ops/azure/container-host/aci/aci-template.json \\ --parameters @c7n-aci-tutorial.json Once this deployment succeeds, the Azure Container Host should be running! You can see the logs with the following command: az container logs --resource-group c7n-aci-tutorial --name cloud-custodian --follow","title":"5. Create the ACI Container Host"},{"location":"azure/configuration/acitutorial/#6-upload-a-custodian-policy","text":"Finally, create a custodian policy called find-c7nstorage.yaml . This policy will just find the storage account we made earlier. We\\'ll set the mode to run every minute for easier testing. policies: - name: find-c7nstorage resource: azure.storage mode: type: container-periodic schedule: \"* * * * *\" # Run every minute as an example filters: - type: value key: name op: eq value: c7nstorage Upload this file to the policy storage container. Within a few minutes, the container host should pick it up and begin executing it. az storage blob upload --account-name c7nstorage --account-key $account_key \\ --container-name c7n-aci-policies --file find-c7nstorage.yaml --name find-c7nstorage.yaml","title":"6. Upload a Custodian Policy"},{"location":"azure/configuration/authentication/","text":"Authentication & Access {#azure_authentication} Custodian supports four distinct authentication types, including Azure CLI integration, Service Principal, MSI, and raw tokens. Azure CLI If none of the following environment variables are set, Custodian will attempt to pull credentials and the default subscription from Azure CLI. This requires that you have already run az login and selected your subscription in Azure CLI. Service Principal A Service Principal can be provided by setting the following environment variables. AZURE_TENANT_ID AZURE_SUBSCRIPTION_ID AZURE_CLIENT_ID AZURE_CLIENT_SECRET You can create a Service Principal with Azure CLI or via the Azure Portal. The Service Principal requires the Contributor role to be assigned. Azure Portal Follow instructions to create and assign required permissions to the new Service Principal: How to create Service Principal Azure CLI # select correct subscription az account set -s \"my subscription name\" # create Service Principal az ad sp create-for-rbac --name <name> --password <password> This will yield something like: { \"appId\": appid, \"displayName\": name, \"name\": name, \"password\": password, \"tenant\": guid } Map the Service Principal to environment variables for Custodian: AZURE_TENANT_ID=tenant AZURE_SUBSCRIPTION_ID=subscriptionId AZURE_CLIENT_ID=appId AZURE_CLIENT_SECRET=password The Service Principal already has Contributor role. c7n-org If you\\'re using a Service Principal across subscriptions with [c7n-org]{.title-ref}, you\\'ll need to grant it access to each of the subscriptions. To grant access, see: instructions Access Token Passing access tokens directly is useful for integration or fake test authentication. For fake test authentication, environment variables should be configured: AZURE_ACCESS_TOKEN=fake_token AZURE_SUBSCRIPTION_ID=ea42f556-5106-4743-99b0-c129bfa71a47 You will also find this configuration in tox.ini. Managed Service Identity For more information about MSI, see Azure Documentation . If [AZURE_USE_MSI]{.title-ref} is set to any value, Custodian will attempt to use MSI. If [AZURE_CLIENT_ID]{.title-ref} is not set, Custodian will use the System Identity. If [AZURE_CLIENT_ID]{.title-ref} is set, Custodian will use the User Identity which matches the client id. You must set [AZURE_SUBSCRIPTION_ID]{.title-ref}. AZURE_USE_MSI=1 AZURE_SUBSCRIPTION_ID=subscriptionId AZURE_CLIENT_ID=clientId Azure Key Vault Integration If you run Custodian inside Azure VM, AKS, ACI or Azure Functions, you can leverage Azure Key Vault to store Service Principal credentials. You can store the json-formatted authentication file as a Key Vault secret. It will be used to extend your local authentication options. Best practice is to configure Subscription ID and Tenant ID locally and store the Service Principal credentials in Azure Key Vault. Cloud Custodian leverages Managed Service Identity or User Assigned Identity to access Key Vault and retrieve the extended configuration. The following example shows the secret stored in a Key Vault: {\"client_id\": \"<your-sp-id>\", \"client_secret\": \"<your-sp-password>\"} To enable Key Vault integration support, populate AZURE_KEYVAULT_SECRET environment variable with Secret Identifier. If you use User Assigned Identity, provide AZURE_KEYVAULT_CLIENT_ID with your UAI Service Principal ID. AZURE_KEYVAULT_SECRET=https://<vault_name>.vault.azure.net/secrets/<secret_name>/<optional_secret_version> AZURE_KEYVAULT_CLIENT_ID=<UAI_SP_ID> You can find more information on how to create Azure Key Vault Secrets in this quickstart: link . Azure Storage access If your Service Principal will be writing logs to storage or leveraging queues for mailer you should assign Storage roles, either at the subscription level or resource group/storage account level. Note: you cannot leverage Azure Storage functionality if you use Access Token for authentication. [Blob Data Contributor]{.title-ref} [Queue Data Contributor]{.title-ref} More details about Azure Storage access rights: Azure Documents Azure Cloud Offerings Cloud Custodian supports four different Cloud offerings: 1. Default Azure Public Cloud ([AzureCloud]{.title-ref}) 2. Azure China Cloud ([AzureChinaCloud]{.title-ref}) 3. Azure German Cloud ([AzureGermanCloud]{.title-ref}) 4. Azure US Government ([AzureUSGovernment]{.title-ref}) To target these Clouds, pass the cloud name in the --region flag custodian run -s . --region=AzureChinaCloud my-policy.yml Note : All Custodian features may not be available in every Cloud.","title":"Authentication"},{"location":"azure/configuration/authentication/#authentication-access-azure_authentication","text":"Custodian supports four distinct authentication types, including Azure CLI integration, Service Principal, MSI, and raw tokens.","title":"Authentication &amp; Access {#azure_authentication}"},{"location":"azure/configuration/authentication/#azure-cli","text":"If none of the following environment variables are set, Custodian will attempt to pull credentials and the default subscription from Azure CLI. This requires that you have already run az login and selected your subscription in Azure CLI.","title":"Azure CLI"},{"location":"azure/configuration/authentication/#service-principal","text":"A Service Principal can be provided by setting the following environment variables. AZURE_TENANT_ID AZURE_SUBSCRIPTION_ID AZURE_CLIENT_ID AZURE_CLIENT_SECRET You can create a Service Principal with Azure CLI or via the Azure Portal. The Service Principal requires the Contributor role to be assigned.","title":"Service Principal"},{"location":"azure/configuration/authentication/#azure-portal","text":"Follow instructions to create and assign required permissions to the new Service Principal: How to create Service Principal","title":"Azure Portal"},{"location":"azure/configuration/authentication/#azure-cli_1","text":"# select correct subscription az account set -s \"my subscription name\" # create Service Principal az ad sp create-for-rbac --name <name> --password <password> This will yield something like: { \"appId\": appid, \"displayName\": name, \"name\": name, \"password\": password, \"tenant\": guid } Map the Service Principal to environment variables for Custodian: AZURE_TENANT_ID=tenant AZURE_SUBSCRIPTION_ID=subscriptionId AZURE_CLIENT_ID=appId AZURE_CLIENT_SECRET=password The Service Principal already has Contributor role.","title":"Azure CLI"},{"location":"azure/configuration/authentication/#c7n-org","text":"If you\\'re using a Service Principal across subscriptions with [c7n-org]{.title-ref}, you\\'ll need to grant it access to each of the subscriptions. To grant access, see: instructions","title":"c7n-org"},{"location":"azure/configuration/authentication/#access-token","text":"Passing access tokens directly is useful for integration or fake test authentication. For fake test authentication, environment variables should be configured: AZURE_ACCESS_TOKEN=fake_token AZURE_SUBSCRIPTION_ID=ea42f556-5106-4743-99b0-c129bfa71a47 You will also find this configuration in tox.ini.","title":"Access Token"},{"location":"azure/configuration/authentication/#managed-service-identity","text":"For more information about MSI, see Azure Documentation . If [AZURE_USE_MSI]{.title-ref} is set to any value, Custodian will attempt to use MSI. If [AZURE_CLIENT_ID]{.title-ref} is not set, Custodian will use the System Identity. If [AZURE_CLIENT_ID]{.title-ref} is set, Custodian will use the User Identity which matches the client id. You must set [AZURE_SUBSCRIPTION_ID]{.title-ref}. AZURE_USE_MSI=1 AZURE_SUBSCRIPTION_ID=subscriptionId AZURE_CLIENT_ID=clientId","title":"Managed Service Identity"},{"location":"azure/configuration/authentication/#azure-key-vault-integration","text":"If you run Custodian inside Azure VM, AKS, ACI or Azure Functions, you can leverage Azure Key Vault to store Service Principal credentials. You can store the json-formatted authentication file as a Key Vault secret. It will be used to extend your local authentication options. Best practice is to configure Subscription ID and Tenant ID locally and store the Service Principal credentials in Azure Key Vault. Cloud Custodian leverages Managed Service Identity or User Assigned Identity to access Key Vault and retrieve the extended configuration. The following example shows the secret stored in a Key Vault: {\"client_id\": \"<your-sp-id>\", \"client_secret\": \"<your-sp-password>\"} To enable Key Vault integration support, populate AZURE_KEYVAULT_SECRET environment variable with Secret Identifier. If you use User Assigned Identity, provide AZURE_KEYVAULT_CLIENT_ID with your UAI Service Principal ID. AZURE_KEYVAULT_SECRET=https://<vault_name>.vault.azure.net/secrets/<secret_name>/<optional_secret_version> AZURE_KEYVAULT_CLIENT_ID=<UAI_SP_ID> You can find more information on how to create Azure Key Vault Secrets in this quickstart: link .","title":"Azure Key Vault Integration"},{"location":"azure/configuration/authentication/#azure-storage-access","text":"If your Service Principal will be writing logs to storage or leveraging queues for mailer you should assign Storage roles, either at the subscription level or resource group/storage account level. Note: you cannot leverage Azure Storage functionality if you use Access Token for authentication. [Blob Data Contributor]{.title-ref} [Queue Data Contributor]{.title-ref} More details about Azure Storage access rights: Azure Documents","title":"Azure Storage access"},{"location":"azure/configuration/authentication/#azure-cloud-offerings","text":"Cloud Custodian supports four different Cloud offerings: 1. Default Azure Public Cloud ([AzureCloud]{.title-ref}) 2. Azure China Cloud ([AzureChinaCloud]{.title-ref}) 3. Azure German Cloud ([AzureGermanCloud]{.title-ref}) 4. Azure US Government ([AzureUSGovernment]{.title-ref}) To target these Clouds, pass the cloud name in the --region flag custodian run -s . --region=AzureChinaCloud my-policy.yml Note : All Custodian features may not be available in every Cloud.","title":"Azure Cloud Offerings"},{"location":"azure/configuration/containerhosting/","text":"Azure Container Hosting {#azure_containerhosting} The Azure Container Host is an alternate execution mode for the cloud custodian azure provider. Running the Azure Container Host is done with the official custodian docker image . See the ACI <azure_configuration_acitutorial> {.interpreted-text role=\"ref\"} and Kubernetes <azure_configuration_helmtutorial> {.interpreted-text role=\"ref\"} deployment tutorials to get started running the Azure Container Host. Overview The Azure Container Host will periodically scan azure blob storage for a set of custodian policies to execute in either a periodic or event based mode against a target subscription. For periodic policies, the container host will execute the policy on the cron schedule that is provided. For event based policies, the container host maintains an azure queue that subscribes to events in the target azure subscription. Once the Azure Container Host is deployed, any policies uploaded to blob storage are automatically loaded and running against an Azure Subscription. This makes it very easy to manage and run a large number of policies. It is also possible to configure a set of container hosts to each monitor an Azure Subscription. This can be useful for monitoring a Management Group or other collections of subscriptions. These Container Hosts could be managed with any container orchestration, but we provide the tooling and a tutorial <azure_configuration_helmtutorial> {.interpreted-text role=\"ref\"} for deploying container hosts inside Kubernetes with a Helm chart. In this diagram, each Container Host is reading and writing to the same policy and monitoring resources, but they could each be configured to interact with their own Storage Accounts or Application Insights instances. Supported Policy Modes The container host will only run policies with one of the following modes specified. Otherwise, the policy will be ignored. Periodic Periodic policies must specify a mode with type container-periodic and a cron schedule. This schedule can specify when the policy should run. For example: once every hour, on midnight on every weekday, or once a month. policies: - name: run-every-day-at-midnight resource: azure.resourcegroup mode: type: container-periodic schedule: '0 0 * * *' Event Based Event based policies must specify a mode with the type container-event and a set of events that will trigger the execution. For example: after a new resource group is created. policies: - name: run-on-new-resource-group resource: azure.resourcegroup mode: type: container-event events: - resourceProvider: Microsoft.Resources/subscriptions/resourceGroups event: write Configuration Configuration for the container host is provided as environment variables. There are several environment variables specific to the container host: Variable Name Required Description AZURE_CONTAINER_POLICY_URI required The URL to the azure blob container to load custodian policies from. AZURE_CONTAINER_STORAGE_RESOURCE_ID required The resource id of the storage account to hold the event queue. AZURE_CONTAINER_QUEUE_NAME The name of the event queue that the container host will listen on. If this does not exist, it will be created. Defaults to the target subscripition id. AZURE_CONTAINER_LOG_GROUP The application insights to send log output to. In the format: azure://<instrumentation_key_guid> . AZURE_CONTAINER_METRICS The application insights to send metrics output to. In the format: azure://<instrumentation_key_guid> . AZURE_CONTAINER_OUTPUT_DIR The URL of the storage account blob container to send log output to. In the format: azure://<storage_account_name>.blob.core.windows.net/<blob_container_name> . In addition to the above environment variables, authentication must be provided to the container host. See azure_authentication {.interpreted-text role=\"ref\"} for authenticating the container host with an azure identity. Once an identity has been established, it will need the following roles in azure: Reader and Storage Blob Data Contributor on the Storage Account that holds the policy files. Contributor and Storage Queue Message Processor on the Storage Account that the event queue will live in. Any other roles that are needed to run the policies that the container host will run. For example, if there is a policy that filters the azure.vm resource, the Reader role will be required for the VMs that are in the container host\\'s target subscription. Running Locally The container host can be run locally with python -m c7n_azure.container_host.host . You will need to provide all of the same configuration specified above through either environment variables or CLI options. Run python -m c7n_azure.container_host.host --help for more information. Deployment Options For quick deployments, we provide tooling for 2 methods of deploying the Azure Container Host: ACI <azure_configuration_acitutorial> {.interpreted-text role=\"ref\"}, and Kubernetes with a Helm chart <azure_configuration_helmtutorial> {.interpreted-text role=\"ref\"}.","title":"Containerhosting"},{"location":"azure/configuration/containerhosting/#azure-container-hosting-azure_containerhosting","text":"The Azure Container Host is an alternate execution mode for the cloud custodian azure provider. Running the Azure Container Host is done with the official custodian docker image . See the ACI <azure_configuration_acitutorial> {.interpreted-text role=\"ref\"} and Kubernetes <azure_configuration_helmtutorial> {.interpreted-text role=\"ref\"} deployment tutorials to get started running the Azure Container Host.","title":"Azure Container Hosting {#azure_containerhosting}"},{"location":"azure/configuration/containerhosting/#overview","text":"The Azure Container Host will periodically scan azure blob storage for a set of custodian policies to execute in either a periodic or event based mode against a target subscription. For periodic policies, the container host will execute the policy on the cron schedule that is provided. For event based policies, the container host maintains an azure queue that subscribes to events in the target azure subscription. Once the Azure Container Host is deployed, any policies uploaded to blob storage are automatically loaded and running against an Azure Subscription. This makes it very easy to manage and run a large number of policies. It is also possible to configure a set of container hosts to each monitor an Azure Subscription. This can be useful for monitoring a Management Group or other collections of subscriptions. These Container Hosts could be managed with any container orchestration, but we provide the tooling and a tutorial <azure_configuration_helmtutorial> {.interpreted-text role=\"ref\"} for deploying container hosts inside Kubernetes with a Helm chart. In this diagram, each Container Host is reading and writing to the same policy and monitoring resources, but they could each be configured to interact with their own Storage Accounts or Application Insights instances.","title":"Overview"},{"location":"azure/configuration/containerhosting/#supported-policy-modes","text":"The container host will only run policies with one of the following modes specified. Otherwise, the policy will be ignored.","title":"Supported Policy Modes"},{"location":"azure/configuration/containerhosting/#periodic","text":"Periodic policies must specify a mode with type container-periodic and a cron schedule. This schedule can specify when the policy should run. For example: once every hour, on midnight on every weekday, or once a month. policies: - name: run-every-day-at-midnight resource: azure.resourcegroup mode: type: container-periodic schedule: '0 0 * * *'","title":"Periodic"},{"location":"azure/configuration/containerhosting/#event-based","text":"Event based policies must specify a mode with the type container-event and a set of events that will trigger the execution. For example: after a new resource group is created. policies: - name: run-on-new-resource-group resource: azure.resourcegroup mode: type: container-event events: - resourceProvider: Microsoft.Resources/subscriptions/resourceGroups event: write","title":"Event Based"},{"location":"azure/configuration/containerhosting/#configuration","text":"Configuration for the container host is provided as environment variables. There are several environment variables specific to the container host: Variable Name Required Description AZURE_CONTAINER_POLICY_URI required The URL to the azure blob container to load custodian policies from. AZURE_CONTAINER_STORAGE_RESOURCE_ID required The resource id of the storage account to hold the event queue. AZURE_CONTAINER_QUEUE_NAME The name of the event queue that the container host will listen on. If this does not exist, it will be created. Defaults to the target subscripition id. AZURE_CONTAINER_LOG_GROUP The application insights to send log output to. In the format: azure://<instrumentation_key_guid> . AZURE_CONTAINER_METRICS The application insights to send metrics output to. In the format: azure://<instrumentation_key_guid> . AZURE_CONTAINER_OUTPUT_DIR The URL of the storage account blob container to send log output to. In the format: azure://<storage_account_name>.blob.core.windows.net/<blob_container_name> . In addition to the above environment variables, authentication must be provided to the container host. See azure_authentication {.interpreted-text role=\"ref\"} for authenticating the container host with an azure identity. Once an identity has been established, it will need the following roles in azure: Reader and Storage Blob Data Contributor on the Storage Account that holds the policy files. Contributor and Storage Queue Message Processor on the Storage Account that the event queue will live in. Any other roles that are needed to run the policies that the container host will run. For example, if there is a policy that filters the azure.vm resource, the Reader role will be required for the VMs that are in the container host\\'s target subscription.","title":"Configuration"},{"location":"azure/configuration/containerhosting/#running-locally","text":"The container host can be run locally with python -m c7n_azure.container_host.host . You will need to provide all of the same configuration specified above through either environment variables or CLI options. Run python -m c7n_azure.container_host.host --help for more information.","title":"Running Locally"},{"location":"azure/configuration/containerhosting/#deployment-options","text":"For quick deployments, we provide tooling for 2 methods of deploying the Azure Container Host: ACI <azure_configuration_acitutorial> {.interpreted-text role=\"ref\"}, and Kubernetes with a Helm chart <azure_configuration_helmtutorial> {.interpreted-text role=\"ref\"}.","title":"Deployment Options"},{"location":"azure/configuration/functionshosting/","text":"Azure Functions Hosting {#azure_functionshosting} Overview The Azure provider supports deploying policies into Azure Functions to run them inexpensively in your subscription. Currently, you can deploy timer triggered functions (azure-periodic) or Event Grid triggered functions (azure-event-grid). The first deployment to an Azure Function will create the following resources in your Azure subscription: Resource Group: Holds all the resources Azure Storage: Serves as the backend data store for the Functions Application Insights: Provides logging and metric tracking for the Functions App Service Plan: A Linux based consumption plan using the V2 runtime to support Python Functions Function App: The Function that executes the given policy Successive policy deployments will only create a new Function App for the policy, because the rest of the infrastructure can be shared. Note: Python 3.6 or higher is required to deploy a policy to an Azure Function. Azure Modes Custodian can run in numerous modes. The default mode is pull. pull: : Default mode, which executes the policy locally to where Custodian is run. azure-periodic: : Creates a timer triggered Azure Function to run the policy in Custodian. The timing is executed based on a user defined cron interval , such as every 15 minutes or every hour on weekdays. azure-event-grid: : Creates Event Grid triggered Azure Functions to run the policy in Custodian. This mode allows you to apply your policies when events occur. See Azure Event Grid for more details. Provision Options The following Azure resources are required to support an Azure Function. If they do not exist, Custodian will create them as it creates the Azure Function for the policy. Storage (shared across functions) Application Insights (shared across functions) Application Service Plan (shared across functions with optional default auto scale rule) Application Service (per function) Functions can be deployed in either a dedicated Application Service Plan (Basic, Standard or Premium) or in a Consumption plan. More details on the different hosting models offered by Azure Functions can be found in the Azure Functions documentation . By default, Custodian policies are run using the Consumption hosting model. (i.e. skuTier=dynamic) Note: Linux Consumption plans are not available in all regions. You will get an error when applying the policy if you use an unsupported location. You can enable auto scaling for your dedicated App Service Plan. The default auto scaling allows you to specify the minimum and maximum number of VMs underlying the Functions. The App Service Plan will be scaled up if the average RAM usage was more than 80% in the past 10 minutes. This option is disabled by default. The same shared resources from above can be used to service multiple Functions. This is done by specifying the resources names in the policy deployment definition, or by using the default names every time. When deploying a new policy using existing infrastructure, only the new Function will be created. The default set of parameters for Azure Storage, Application Insights, and Application Service Plan will deploy the function successfully. To customize the deployment, the defaults can be overwritten by setting the provision-options object on mode . The following keys are supported, with their default values shown: servicePlan : - name (default: cloud-custodian) - location (default: East US) - resourceGroupName (default: cloud-custodian) - skuTier (default: Dynamic) \\# consumption - skuName (default: Y1) - autoScale (optional): : - enabled (default: False) - minCapacity (default: 1) - maxCapacity (default: 1) - defaultCapacity (default: 1) storageAccount : - name (default: custodian + sha256(resourceGroupName+subscription_id)[:8]) - location (default: servicePlan location) - resourceGroupName (default: servicePlan resource group) appInsights : - name (default: servicePlan resource group) - location (default: servicePlan location) - resourceGroupName (default: servicePlan name) The location allows you to choose the region to deploy the resource group and resources that will be provisioned. Application Insights has 20 available locations and thus may not always be in the same region as the other resources. For details, see Application Insights availability by region . If the specified resources already exist in the subscription, discovered by resource group and resource name, Custodian will not change the existing resource regardless of the parameters set by the policy. If a resource does not exist, it will be provisioned using the provided configuration. You can provide resource IDs to specify existing infrastructure, rather than matching resource group and resource name. Please see the third example below for the correct formatting. Custodian verifies that the resources defined by the given IDs exist before creating the Function. If the resource is missing, Custodian will return an error. The following example shows how to deploy a policy to a timer-triggered Function that runs every hour. The defaults are accepted for Storage and Application Insights, and custom values are provided for the Service Plan. This policy deploys a dedicated Basic B1 App Service Plan with the default auto scaling turned on. Based on the RAM consumption in the underlying VMs, the App Service Plan will be backed by 1-3 VMs. policies: - name: stopped-vm mode: type: azure-periodic schedule: '0 0 * * * *' provision-options: servicePlan: name: functionshost skuTier: Basic skuName: B1 autoScale: enabled: true minCapacity: 1 maxCapacity: 3 defaultCapacity: 1 resource: azure.vm filters: - type: instance-view key: statuses[].code op: not-in value_type: swap value: \"PowerState/running\" The following example shows how to set the name, size and location of all three components of the supporting infrastructure: policies: - name: stopped-vm mode: type: azure-periodic schedule: '0 0 * * * *' provision-options: servicePlan: name: functionshost location: East US skuTier: Standard skuName: S1 appInsights: location: East US storageAccount: name: sampleaccount location: East US resource: azure.vm filters: - type: instance-view key: statuses[].code op: not-in value_type: swap value: \"PowerState/running\" The final example shows how to use resource ids to specify existing infrastructure: policies: - name: stopped-vm mode: type: azure-periodic schedule: '0 0 * * * *' provision-options: servicePlan: /subscriptions/<subscription_id>/resourceGroups/cloud-custodian/providers/Microsoft.Web/serverFarms/existingResource appInsights: /subscriptions/<subscription_id>/resourceGroups/cloud-custodian/providers/microsoft.insights/components/existingResource storageAccount: /subscriptions/<subscription_id>/resourceGroups/cloud-custodian/providers/Microsoft.Storage/storageAccounts/existingResource resource: azure.vm filters: - type: instance-view key: statuses[].code op: not-in value_type: swap value: \"PowerState/running\" Authentication Options Custodian function policies support three different authentications modes. User Assigned Identities Managed System Identities Service Principal Credentials (embedded) Its highly recommended to utilize User Assigned Identities, like Managed System Identities they provide for dynamic automatically rotated credentials, but they also allow for simplicity of managing role assignments to a smaller population of IAM resources, instead of one per policy function. policies: - name: stopped-vm mode: type: azure-periodic schedule: '0 0 * * * *' provision-options: identity: type: UserAssigned id: my-custodian-identity resource: azure.vm The identity id can be provided as the user assigned identity\\'s name or the id, it will be resolved and verified as the policy is provisioned. Using a Managed System Identity results in the creation of an identity per policy function, which then needs subsequent role assignments before the policy will be able to successfully execute. policies: - name: stopped-vm mode: type: azure-periodic schedule: '0 0 * * * *' provision-options: identity: type: SystemAssigned resource: azure.vm Execution Options Execution options are not required, but allow you to override defaults that would normally be provided on the command line in non-serverless scenarios. Common properties are: output_dir cache_period dryrun metrics The default output directory for an Azure Function is /tmp/<random_uuid> . The following example shows how to save the output of the policy to an Azure Storage Account instead of in the default Function location. policies: - name: stopped-vm mode: type: azure-periodic schedule: '0 0 * * * *' provision-options: servicePlan: name: functionshost execution-options: output_dir: azure://yourstorageaccount.blob.core.windows.net/custodian metrics: azure://<resource_group_name>/<app_insights_name> resource: azure.vm filters: - type: instance-view key: statuses[].code op: not-in value_type: swap value: \"PowerState/running\" More details on Blob Storage output can be found at azure_bloboutput {.interpreted-text role=\"ref\"} Event Grid Functions Currently, Event Grid Functions are only supported at the subscription level. You can set the function to be triggered by write and/or delete events. When an Event Grid Function is deployed, Custodian creates an Event Grid Subscription to trigger the new Function when any event occurs in the Subscription. Once triggered, Custodian only executes the policy if the event was caused by the resource provider and event type specified in the policy. In order to subscribe to an event, you need to provide the resource provider and the action, or provide the string of one of the shortcuts . For a list of all of the resource providers and their actions, see Azure Resource Manager resource provider options . The following example shows an Event Grid Function that runs when a value is written to Key Vault. policies: - name: tag-key-vault-creator resource: azure.keyvault mode: type: azure-event-grid events: - resourceProvider: Microsoft.KeyVault/vaults event: write filters: - \"tag:CreatorEmail\": null actions: - type: auto-tag-user tag: CreatorEmail Management Groups Support You can deploy Azure Functions targeting all subscriptions that are part of a specified Management Group. The following variable allows you to specify Management Group name: AZURE_FUNCTION_MANAGEMENT_GROUP_NAME It can be used with Function specific Service Principal credentials described in the previous section. The Management Group environment variable has the highest priority, so [AZURE_FUNCTION_SUBSCRIPTION_ID]{.title-ref} will be ignored. Timer triggered functions When the Management Groups option is used with periodic mode, Cloud Custodian deploys a single Azure Function App with multiple Azure Functions following the single-subscription-per-function rule. Event triggered functions When the Management Groups option is used with event mode, Cloud Custodian deploys a single Azure Function. It creates an Event Grid subscription for each Subscription in the Management Group delivering events to a single Azure Storage Queue. Permissions The Service Principal used at the Functions runtime is required to have an appropriate level of permissions in each target subscription. The Service Principal used to provision Azure Functions is required to have an appropriate level of permissions to access Management Groups. If the Service Principal doesn\\'t have [MG Reader]{.title-ref} permissions in any child subscription, these subscriptions won\\'t be a part of the Cloud Custodian Azure Function deployment process.","title":"Functionshosting"},{"location":"azure/configuration/functionshosting/#azure-functions-hosting-azure_functionshosting","text":"","title":"Azure Functions Hosting {#azure_functionshosting}"},{"location":"azure/configuration/functionshosting/#overview","text":"The Azure provider supports deploying policies into Azure Functions to run them inexpensively in your subscription. Currently, you can deploy timer triggered functions (azure-periodic) or Event Grid triggered functions (azure-event-grid). The first deployment to an Azure Function will create the following resources in your Azure subscription: Resource Group: Holds all the resources Azure Storage: Serves as the backend data store for the Functions Application Insights: Provides logging and metric tracking for the Functions App Service Plan: A Linux based consumption plan using the V2 runtime to support Python Functions Function App: The Function that executes the given policy Successive policy deployments will only create a new Function App for the policy, because the rest of the infrastructure can be shared. Note: Python 3.6 or higher is required to deploy a policy to an Azure Function.","title":"Overview"},{"location":"azure/configuration/functionshosting/#azure-modes","text":"Custodian can run in numerous modes. The default mode is pull. pull: : Default mode, which executes the policy locally to where Custodian is run. azure-periodic: : Creates a timer triggered Azure Function to run the policy in Custodian. The timing is executed based on a user defined cron interval , such as every 15 minutes or every hour on weekdays. azure-event-grid: : Creates Event Grid triggered Azure Functions to run the policy in Custodian. This mode allows you to apply your policies when events occur. See Azure Event Grid for more details.","title":"Azure Modes"},{"location":"azure/configuration/functionshosting/#provision-options","text":"The following Azure resources are required to support an Azure Function. If they do not exist, Custodian will create them as it creates the Azure Function for the policy. Storage (shared across functions) Application Insights (shared across functions) Application Service Plan (shared across functions with optional default auto scale rule) Application Service (per function) Functions can be deployed in either a dedicated Application Service Plan (Basic, Standard or Premium) or in a Consumption plan. More details on the different hosting models offered by Azure Functions can be found in the Azure Functions documentation . By default, Custodian policies are run using the Consumption hosting model. (i.e. skuTier=dynamic) Note: Linux Consumption plans are not available in all regions. You will get an error when applying the policy if you use an unsupported location. You can enable auto scaling for your dedicated App Service Plan. The default auto scaling allows you to specify the minimum and maximum number of VMs underlying the Functions. The App Service Plan will be scaled up if the average RAM usage was more than 80% in the past 10 minutes. This option is disabled by default. The same shared resources from above can be used to service multiple Functions. This is done by specifying the resources names in the policy deployment definition, or by using the default names every time. When deploying a new policy using existing infrastructure, only the new Function will be created. The default set of parameters for Azure Storage, Application Insights, and Application Service Plan will deploy the function successfully. To customize the deployment, the defaults can be overwritten by setting the provision-options object on mode . The following keys are supported, with their default values shown: servicePlan : - name (default: cloud-custodian) - location (default: East US) - resourceGroupName (default: cloud-custodian) - skuTier (default: Dynamic) \\# consumption - skuName (default: Y1) - autoScale (optional): : - enabled (default: False) - minCapacity (default: 1) - maxCapacity (default: 1) - defaultCapacity (default: 1) storageAccount : - name (default: custodian + sha256(resourceGroupName+subscription_id)[:8]) - location (default: servicePlan location) - resourceGroupName (default: servicePlan resource group) appInsights : - name (default: servicePlan resource group) - location (default: servicePlan location) - resourceGroupName (default: servicePlan name) The location allows you to choose the region to deploy the resource group and resources that will be provisioned. Application Insights has 20 available locations and thus may not always be in the same region as the other resources. For details, see Application Insights availability by region . If the specified resources already exist in the subscription, discovered by resource group and resource name, Custodian will not change the existing resource regardless of the parameters set by the policy. If a resource does not exist, it will be provisioned using the provided configuration. You can provide resource IDs to specify existing infrastructure, rather than matching resource group and resource name. Please see the third example below for the correct formatting. Custodian verifies that the resources defined by the given IDs exist before creating the Function. If the resource is missing, Custodian will return an error. The following example shows how to deploy a policy to a timer-triggered Function that runs every hour. The defaults are accepted for Storage and Application Insights, and custom values are provided for the Service Plan. This policy deploys a dedicated Basic B1 App Service Plan with the default auto scaling turned on. Based on the RAM consumption in the underlying VMs, the App Service Plan will be backed by 1-3 VMs. policies: - name: stopped-vm mode: type: azure-periodic schedule: '0 0 * * * *' provision-options: servicePlan: name: functionshost skuTier: Basic skuName: B1 autoScale: enabled: true minCapacity: 1 maxCapacity: 3 defaultCapacity: 1 resource: azure.vm filters: - type: instance-view key: statuses[].code op: not-in value_type: swap value: \"PowerState/running\" The following example shows how to set the name, size and location of all three components of the supporting infrastructure: policies: - name: stopped-vm mode: type: azure-periodic schedule: '0 0 * * * *' provision-options: servicePlan: name: functionshost location: East US skuTier: Standard skuName: S1 appInsights: location: East US storageAccount: name: sampleaccount location: East US resource: azure.vm filters: - type: instance-view key: statuses[].code op: not-in value_type: swap value: \"PowerState/running\" The final example shows how to use resource ids to specify existing infrastructure: policies: - name: stopped-vm mode: type: azure-periodic schedule: '0 0 * * * *' provision-options: servicePlan: /subscriptions/<subscription_id>/resourceGroups/cloud-custodian/providers/Microsoft.Web/serverFarms/existingResource appInsights: /subscriptions/<subscription_id>/resourceGroups/cloud-custodian/providers/microsoft.insights/components/existingResource storageAccount: /subscriptions/<subscription_id>/resourceGroups/cloud-custodian/providers/Microsoft.Storage/storageAccounts/existingResource resource: azure.vm filters: - type: instance-view key: statuses[].code op: not-in value_type: swap value: \"PowerState/running\"","title":"Provision Options"},{"location":"azure/configuration/functionshosting/#authentication-options","text":"Custodian function policies support three different authentications modes. User Assigned Identities Managed System Identities Service Principal Credentials (embedded) Its highly recommended to utilize User Assigned Identities, like Managed System Identities they provide for dynamic automatically rotated credentials, but they also allow for simplicity of managing role assignments to a smaller population of IAM resources, instead of one per policy function. policies: - name: stopped-vm mode: type: azure-periodic schedule: '0 0 * * * *' provision-options: identity: type: UserAssigned id: my-custodian-identity resource: azure.vm The identity id can be provided as the user assigned identity\\'s name or the id, it will be resolved and verified as the policy is provisioned. Using a Managed System Identity results in the creation of an identity per policy function, which then needs subsequent role assignments before the policy will be able to successfully execute. policies: - name: stopped-vm mode: type: azure-periodic schedule: '0 0 * * * *' provision-options: identity: type: SystemAssigned resource: azure.vm","title":"Authentication Options"},{"location":"azure/configuration/functionshosting/#execution-options","text":"Execution options are not required, but allow you to override defaults that would normally be provided on the command line in non-serverless scenarios. Common properties are: output_dir cache_period dryrun metrics The default output directory for an Azure Function is /tmp/<random_uuid> . The following example shows how to save the output of the policy to an Azure Storage Account instead of in the default Function location. policies: - name: stopped-vm mode: type: azure-periodic schedule: '0 0 * * * *' provision-options: servicePlan: name: functionshost execution-options: output_dir: azure://yourstorageaccount.blob.core.windows.net/custodian metrics: azure://<resource_group_name>/<app_insights_name> resource: azure.vm filters: - type: instance-view key: statuses[].code op: not-in value_type: swap value: \"PowerState/running\" More details on Blob Storage output can be found at azure_bloboutput {.interpreted-text role=\"ref\"}","title":"Execution Options"},{"location":"azure/configuration/functionshosting/#event-grid-functions","text":"Currently, Event Grid Functions are only supported at the subscription level. You can set the function to be triggered by write and/or delete events. When an Event Grid Function is deployed, Custodian creates an Event Grid Subscription to trigger the new Function when any event occurs in the Subscription. Once triggered, Custodian only executes the policy if the event was caused by the resource provider and event type specified in the policy. In order to subscribe to an event, you need to provide the resource provider and the action, or provide the string of one of the shortcuts . For a list of all of the resource providers and their actions, see Azure Resource Manager resource provider options . The following example shows an Event Grid Function that runs when a value is written to Key Vault. policies: - name: tag-key-vault-creator resource: azure.keyvault mode: type: azure-event-grid events: - resourceProvider: Microsoft.KeyVault/vaults event: write filters: - \"tag:CreatorEmail\": null actions: - type: auto-tag-user tag: CreatorEmail","title":"Event Grid Functions"},{"location":"azure/configuration/functionshosting/#management-groups-support","text":"You can deploy Azure Functions targeting all subscriptions that are part of a specified Management Group. The following variable allows you to specify Management Group name: AZURE_FUNCTION_MANAGEMENT_GROUP_NAME It can be used with Function specific Service Principal credentials described in the previous section. The Management Group environment variable has the highest priority, so [AZURE_FUNCTION_SUBSCRIPTION_ID]{.title-ref} will be ignored.","title":"Management Groups Support"},{"location":"azure/configuration/functionshosting/#timer-triggered-functions","text":"When the Management Groups option is used with periodic mode, Cloud Custodian deploys a single Azure Function App with multiple Azure Functions following the single-subscription-per-function rule.","title":"Timer triggered functions"},{"location":"azure/configuration/functionshosting/#event-triggered-functions","text":"When the Management Groups option is used with event mode, Cloud Custodian deploys a single Azure Function. It creates an Event Grid subscription for each Subscription in the Management Group delivering events to a single Azure Storage Queue.","title":"Event triggered functions"},{"location":"azure/configuration/functionshosting/#permissions","text":"The Service Principal used at the Functions runtime is required to have an appropriate level of permissions in each target subscription. The Service Principal used to provision Azure Functions is required to have an appropriate level of permissions to access Management Groups. If the Service Principal doesn\\'t have [MG Reader]{.title-ref} permissions in any child subscription, these subscriptions won\\'t be a part of the Cloud Custodian Azure Function deployment process.","title":"Permissions"},{"location":"azure/configuration/helmtutorial/","text":"Tutorial - Helm Deployment {#azure_configuration_helmtutorial} This is a step-by-step tutorial for deploying the Azure Container Host in Azure Kubernetes Service (AKS) with the provided helm chart and deployment script. This tutorial makes use of the Azure CLI and helm . Make sure that both are installed and you are logged in to your subscription. 1. Create a Resource Group This will hold all of the Azure resources created in this tutorial. You could use an existing resource group too. az group create --name c7n-helm-tutorial --location westus2 2. Create a Storage Account This storage account will hold all of the blobs and queues used by the container host. You could also use an existing resource here or multiple storage accounts. We will also create two blob containers. One will host the uploaded policies, and the other will store the custodian output files of running policies. az storage account create --resource-group c7n-helm-tutorial --name c7nstorage account_key=$(az storage account keys list --account-name c7nstorage --query \"[0].value\" --output tsv) az storage container create --account-name c7nstorage --account-key $account_key --name c7n-helm-policies az storage container create --account-name c7nstorage --account-key $account_key --name c7n-helm-logs 3. Create a Service Principal This Service Principal will be used as the identity for the container host. You could also use an existing Service Principal. Remember to save the password to use as the client secret. az ad sp create-for-rbac --name c7n-helm This Service Principal will need the proper permissions to interact with the storage account and any other azure resources that the Azure Container Host interacts with. This includes the resources that policies will run against. The simplest way to grant these permissions is to make the identity a Contributor on the target subscription and a Storage Blob Data Contributor and Storage Queue Data Contributor on the storage account created above. az role assignment create --assignee <Service Principal Client Id> \\ --role \"Contributor\" --scope <Target Subscription Resource Id> az role assignment create --assignee <Service Principal Client Id> \\ --role \"Storage Blob Data Contributor\" --scope <Storage Account Resource Id> az role assignment create --assignee <Service Principal Client Id> \\ --role \"Storage Queue Data Contributor\" --scope <Storage Account Resource Id> 4. Create an Application Insights Instance We will create an Application Insights instance to gather logs and telemetry generated by the running container host. # You may need to add the application-insights extension az extension add -n application-insights az monitor app-insights component create --resource-group c7n-helm-tutorial --app c7n-helm-insights --location westus2 5. Create an AKS Cluster and Install Tiller This AKS cluster will be used to host the pods running the Container Host. Tiller will also need to be installed with the proper service account. The provided node count and vm size will create a small cluster and should be adjusted to fit your needs outside this tutorial. az aks create --resource-group c7n-helm-tutorial --name c7n-helm \\ --node-count 1 --node-vm-size Standard_B2s \\ --service-principal <Service Principal ID> --client-secret <Client Secret> az aks get-credentials --resource-group c7n-helm-tutorial --name c7n-helm Once the cluster has been created, we can initialize helm and tiller. First, create the Service Account and Cluster Role Binding for Tiller # rbac-config.yaml apiVersion: v1 kind: ServiceAccount metadata: name: tiller namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: tiller roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: tiller namespace: kube-system kubectl apply -f rbac-config.yaml helm init --service-account tiller 6. Deploy the Helm Chart Now we are ready to deploy the helm chart and the container host. Create a file with the helm configuration values for our container host. See the Container Host Documentation<azure_containerhosting> {.interpreted-text role=\"ref\"} for information on filling out the environment variables. # helm-values.yaml defaultEnvironment: AZURE_TENANT_ID: \"<Azure Tenant ID>\" AZURE_CLIENT_ID: \"<Azure Client ID>\" AZURE_CONTAINER_POLICY_URI: \"<Azure Policy Container URI>\" AZURE_CONTAINER_STORAGE_RESOURCE_ID: \"<Storage Account Resource ID>\" AZURE_CONTAINER_METRICS: \"azure://<App Insights Instrumentation Key>\" AZURE_CONTAINER_LOG_GROUP: \"azure://<App Insights Instrumentation Key>\" AZURE_CONTAINER_OUTPUT_DIR: \"<Azure Logs Container URI>\" subscriptionHosts: - name: '<Subscription Name>' environment: AZURE_SUBSCRIPTION_ID: \"<Subscription ID>\" Then deploy the chart with the following command. The client secret should come from creating the Service Principal and must be provided in a base64 encoded format. helm upgrade --install --debug --force --wait \\ --namespace cloud-custodian --values helm-values.yaml \\ --set defaultSecretEnvironment.AZURE_CLIENT_SECRET=<Base 64 Azure Client Secret> \\ helm-tutorial tools/ops/azure/container-host/chart To verify that the pod is running: # Check if pod status is \"Running\" kubectl get pods --namespace cloud-custodian # Watch the logs for the pod kubectl logs <Pod Name> --namespace cloud-custodian --follow 7. Upload a Custodian Policy Finally, create a custodian policy called find-c7nstorage.yaml . This policy will just find the storage account we made earlier. We\\'ll set the mode to run every minute for easier testing. policies: - name: find-c7nstorage resource: azure.storage mode: type: container-periodic schedule: \"* * * * *\" # Run every minute as an example filters: - type: value key: name op: eq value: c7nstorage Upload this file to the policy storage container. Within a few minutes, the container host should pick it up and begin executing it. az storage blob upload --account-name c7nstorage --account-key $account_key \\ --container-name c7n-helm-policies --file find-c7nstorage.yaml --name find-c7nstorage.yaml","title":"Helmtutorial"},{"location":"azure/configuration/helmtutorial/#tutorial-helm-deployment-azure_configuration_helmtutorial","text":"This is a step-by-step tutorial for deploying the Azure Container Host in Azure Kubernetes Service (AKS) with the provided helm chart and deployment script. This tutorial makes use of the Azure CLI and helm . Make sure that both are installed and you are logged in to your subscription.","title":"Tutorial - Helm Deployment {#azure_configuration_helmtutorial}"},{"location":"azure/configuration/helmtutorial/#1-create-a-resource-group","text":"This will hold all of the Azure resources created in this tutorial. You could use an existing resource group too. az group create --name c7n-helm-tutorial --location westus2","title":"1. Create a Resource Group"},{"location":"azure/configuration/helmtutorial/#2-create-a-storage-account","text":"This storage account will hold all of the blobs and queues used by the container host. You could also use an existing resource here or multiple storage accounts. We will also create two blob containers. One will host the uploaded policies, and the other will store the custodian output files of running policies. az storage account create --resource-group c7n-helm-tutorial --name c7nstorage account_key=$(az storage account keys list --account-name c7nstorage --query \"[0].value\" --output tsv) az storage container create --account-name c7nstorage --account-key $account_key --name c7n-helm-policies az storage container create --account-name c7nstorage --account-key $account_key --name c7n-helm-logs","title":"2. Create a Storage Account"},{"location":"azure/configuration/helmtutorial/#3-create-a-service-principal","text":"This Service Principal will be used as the identity for the container host. You could also use an existing Service Principal. Remember to save the password to use as the client secret. az ad sp create-for-rbac --name c7n-helm This Service Principal will need the proper permissions to interact with the storage account and any other azure resources that the Azure Container Host interacts with. This includes the resources that policies will run against. The simplest way to grant these permissions is to make the identity a Contributor on the target subscription and a Storage Blob Data Contributor and Storage Queue Data Contributor on the storage account created above. az role assignment create --assignee <Service Principal Client Id> \\ --role \"Contributor\" --scope <Target Subscription Resource Id> az role assignment create --assignee <Service Principal Client Id> \\ --role \"Storage Blob Data Contributor\" --scope <Storage Account Resource Id> az role assignment create --assignee <Service Principal Client Id> \\ --role \"Storage Queue Data Contributor\" --scope <Storage Account Resource Id>","title":"3. Create a Service Principal"},{"location":"azure/configuration/helmtutorial/#4-create-an-application-insights-instance","text":"We will create an Application Insights instance to gather logs and telemetry generated by the running container host. # You may need to add the application-insights extension az extension add -n application-insights az monitor app-insights component create --resource-group c7n-helm-tutorial --app c7n-helm-insights --location westus2","title":"4. Create an Application Insights Instance"},{"location":"azure/configuration/helmtutorial/#5-create-an-aks-cluster-and-install-tiller","text":"This AKS cluster will be used to host the pods running the Container Host. Tiller will also need to be installed with the proper service account. The provided node count and vm size will create a small cluster and should be adjusted to fit your needs outside this tutorial. az aks create --resource-group c7n-helm-tutorial --name c7n-helm \\ --node-count 1 --node-vm-size Standard_B2s \\ --service-principal <Service Principal ID> --client-secret <Client Secret> az aks get-credentials --resource-group c7n-helm-tutorial --name c7n-helm Once the cluster has been created, we can initialize helm and tiller. First, create the Service Account and Cluster Role Binding for Tiller # rbac-config.yaml apiVersion: v1 kind: ServiceAccount metadata: name: tiller namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: tiller roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: tiller namespace: kube-system kubectl apply -f rbac-config.yaml helm init --service-account tiller","title":"5. Create an AKS Cluster and Install Tiller"},{"location":"azure/configuration/helmtutorial/#6-deploy-the-helm-chart","text":"Now we are ready to deploy the helm chart and the container host. Create a file with the helm configuration values for our container host. See the Container Host Documentation<azure_containerhosting> {.interpreted-text role=\"ref\"} for information on filling out the environment variables. # helm-values.yaml defaultEnvironment: AZURE_TENANT_ID: \"<Azure Tenant ID>\" AZURE_CLIENT_ID: \"<Azure Client ID>\" AZURE_CONTAINER_POLICY_URI: \"<Azure Policy Container URI>\" AZURE_CONTAINER_STORAGE_RESOURCE_ID: \"<Storage Account Resource ID>\" AZURE_CONTAINER_METRICS: \"azure://<App Insights Instrumentation Key>\" AZURE_CONTAINER_LOG_GROUP: \"azure://<App Insights Instrumentation Key>\" AZURE_CONTAINER_OUTPUT_DIR: \"<Azure Logs Container URI>\" subscriptionHosts: - name: '<Subscription Name>' environment: AZURE_SUBSCRIPTION_ID: \"<Subscription ID>\" Then deploy the chart with the following command. The client secret should come from creating the Service Principal and must be provided in a base64 encoded format. helm upgrade --install --debug --force --wait \\ --namespace cloud-custodian --values helm-values.yaml \\ --set defaultSecretEnvironment.AZURE_CLIENT_SECRET=<Base 64 Azure Client Secret> \\ helm-tutorial tools/ops/azure/container-host/chart To verify that the pod is running: # Check if pod status is \"Running\" kubectl get pods --namespace cloud-custodian # Watch the logs for the pod kubectl logs <Pod Name> --namespace cloud-custodian --follow","title":"6. Deploy the Helm Chart"},{"location":"azure/configuration/helmtutorial/#7-upload-a-custodian-policy","text":"Finally, create a custodian policy called find-c7nstorage.yaml . This policy will just find the storage account we made earlier. We\\'ll set the mode to run every minute for easier testing. policies: - name: find-c7nstorage resource: azure.storage mode: type: container-periodic schedule: \"* * * * *\" # Run every minute as an example filters: - type: value key: name op: eq value: c7nstorage Upload this file to the policy storage container. Within a few minutes, the container host should pick it up and begin executing it. az storage blob upload --account-name c7nstorage --account-key $account_key \\ --container-name c7n-helm-policies --file find-c7nstorage.yaml --name find-c7nstorage.yaml","title":"7. Upload a Custodian Policy"},{"location":"azure/configuration/monitoring/","text":"Logging, Metrics and Output {#azure_monitoring} Writing Custodian Logs to Azure App Insights Cloud Custodian can upload its logs to Application Insights. Each policy's log output contains the policy name , subscription id and execution id properties . These logs will be found under the trace source in Application Insights. Usage example using an instrumentation key: {.sh} custodian run -s <output_directory> -l azure://<instrumentation_key_guid> policy.yml Usage example using a resource name: {.sh} custodian run -s <output_directory> -l azure://<resource_group_name>/<app_insights_name> policy.yml Writing Custodian Metrics to Azure App Insights By default, Cloud Custodian will upload the following metrics in all modes: ResourceCount - the number of resources that matched the set of filters ActionTime - the time to execute the actions. In pull and azure-periodic mode, Cloud Custodian will also publish the following metric: ResourceTime - the time to query for and filter the resources, Additionally, some custom filters and actions may generate their own metrics. These metrics will be found under the customMetrics source in Application Insights. Usage example using an instrumentation key: {.sh} custodian run -s <output_directory> -m azure://<instrumentation_key_guid> policy.yml Usage example using a resource name: {.sh} custodian run -s <output_directory> -m azure://<resource_group_name>/<app_insights_name> policy.yml In azure-periodic and azure-event-grid modes, you can configure metrics under the execution-options . As above, you can provide an instrumentation key or a resource name. {.yaml} policies: - name: periodic-mode-logging-metrics resource: azure.storage mode: type: azure-periodic schedule: '0 0 * * * *' provision-options: servicePlan: name: cloud-custodian location: eastus resourceGroupName: cloud-custodian execution-options: metrics: azure://<instrumentation_key_guid> Writing Custodian Output to Azure Blob Storage {#azure_bloboutput} You can pass the URL of a blob storage container as the output path to Custodian. You must change the URL prefix from https to azure. By default, Custodian will add the policy name and date as the prefix to the blob. {.sh} custodian run -s azure://mystorage.blob.core.windows.net/logs mypolicy.yml In addition, you can use [pyformat]{.title-ref} syntax to format the output prefix. This example is the same structure as the default one. {.sh} custodian run -s azure://mystorage.blob.core.windows.net/logs/{policy_name}/{now:%Y/%m/%d/%H/} mypolicy.yml Use [{account_id}]{.title-ref} for Subscription ID. Authentication to Storage The account working with storage will require [Storage Blob Data Contributor]{.title-ref} on either the storage account or a higher scope.","title":"Monitoring"},{"location":"azure/configuration/monitoring/#logging-metrics-and-output-azure_monitoring","text":"","title":"Logging, Metrics and Output {#azure_monitoring}"},{"location":"azure/configuration/monitoring/#writing-custodian-logs-to-azure-app-insights","text":"Cloud Custodian can upload its logs to Application Insights. Each policy's log output contains the policy name , subscription id and execution id properties . These logs will be found under the trace source in Application Insights. Usage example using an instrumentation key: {.sh} custodian run -s <output_directory> -l azure://<instrumentation_key_guid> policy.yml Usage example using a resource name: {.sh} custodian run -s <output_directory> -l azure://<resource_group_name>/<app_insights_name> policy.yml","title":"Writing Custodian Logs to Azure App Insights"},{"location":"azure/configuration/monitoring/#writing-custodian-metrics-to-azure-app-insights","text":"By default, Cloud Custodian will upload the following metrics in all modes: ResourceCount - the number of resources that matched the set of filters ActionTime - the time to execute the actions. In pull and azure-periodic mode, Cloud Custodian will also publish the following metric: ResourceTime - the time to query for and filter the resources, Additionally, some custom filters and actions may generate their own metrics. These metrics will be found under the customMetrics source in Application Insights. Usage example using an instrumentation key: {.sh} custodian run -s <output_directory> -m azure://<instrumentation_key_guid> policy.yml Usage example using a resource name: {.sh} custodian run -s <output_directory> -m azure://<resource_group_name>/<app_insights_name> policy.yml In azure-periodic and azure-event-grid modes, you can configure metrics under the execution-options . As above, you can provide an instrumentation key or a resource name. {.yaml} policies: - name: periodic-mode-logging-metrics resource: azure.storage mode: type: azure-periodic schedule: '0 0 * * * *' provision-options: servicePlan: name: cloud-custodian location: eastus resourceGroupName: cloud-custodian execution-options: metrics: azure://<instrumentation_key_guid>","title":"Writing Custodian Metrics to Azure App Insights"},{"location":"azure/configuration/monitoring/#writing-custodian-output-to-azure-blob-storage-azure_bloboutput","text":"You can pass the URL of a blob storage container as the output path to Custodian. You must change the URL prefix from https to azure. By default, Custodian will add the policy name and date as the prefix to the blob. {.sh} custodian run -s azure://mystorage.blob.core.windows.net/logs mypolicy.yml In addition, you can use [pyformat]{.title-ref} syntax to format the output prefix. This example is the same structure as the default one. {.sh} custodian run -s azure://mystorage.blob.core.windows.net/logs/{policy_name}/{now:%Y/%m/%d/%H/} mypolicy.yml Use [{account_id}]{.title-ref} for Subscription ID.","title":"Writing Custodian Output to Azure Blob Storage {#azure_bloboutput}"},{"location":"azure/configuration/monitoring/#authentication-to-storage","text":"The account working with storage will require [Storage Blob Data Contributor]{.title-ref} on either the storage account or a higher scope.","title":"Authentication to Storage"},{"location":"azure/examples/","text":"Examples {#azure_examples} These use cases provide examples of specific policies. General ::: {.toctree maxdepth=\"2\" titlesonly=\"\" glob=\"\"} ./ general ::: Compute ::: {.toctree maxdepth=\"2\" titlesonly=\"\" glob=\"\"} ./ compute ::: Storage and Databases ::: {.toctree maxdepth=\"2\" titlesonly=\"\" glob=\"\"} ./ databases ::: Identity ::: {.toctree maxdepth=\"2\" titlesonly=\"\" glob=\"\"} ./ identity ::: Networking ::: {.toctree maxdepth=\"2\" titlesonly=\"\" glob=\"\"} ./ networking ::: Notifications ::: {.toctree maxdepth=\"1\" titlesonly=\"\" glob=\"\"} ./logicappnotifications/logicappnotification.rst ./ notifications :::","title":"Index"},{"location":"azure/examples/#examples-azure_examples","text":"These use cases provide examples of specific policies.","title":"Examples {#azure_examples}"},{"location":"azure/examples/#general","text":"::: {.toctree maxdepth=\"2\" titlesonly=\"\" glob=\"\"} ./ general :::","title":"General"},{"location":"azure/examples/#compute","text":"::: {.toctree maxdepth=\"2\" titlesonly=\"\" glob=\"\"} ./ compute :::","title":"Compute"},{"location":"azure/examples/#storage-and-databases","text":"::: {.toctree maxdepth=\"2\" titlesonly=\"\" glob=\"\"} ./ databases :::","title":"Storage and Databases"},{"location":"azure/examples/#identity","text":"::: {.toctree maxdepth=\"2\" titlesonly=\"\" glob=\"\"} ./ identity :::","title":"Identity"},{"location":"azure/examples/#networking","text":"::: {.toctree maxdepth=\"2\" titlesonly=\"\" glob=\"\"} ./ networking :::","title":"Networking"},{"location":"azure/examples/#notifications","text":"::: {.toctree maxdepth=\"1\" titlesonly=\"\" glob=\"\"} ./logicappnotifications/logicappnotification.rst ./ notifications :::","title":"Notifications"},{"location":"azure/examples/appservicecors-compute/","text":"App Services - Filter By CORS Configuration {#azure_examples_app_service_cors} Filter to select all Application Services (Web Apps and Functions) with a Cross-Origin Resource Sharing (CORS) configuration set to allow all origins. policies: - name: app-service-cors-policy description: | Get all wildcard CORS configurations resource: azure.webapp filters: - type: configuration key: cors.allowedOrigins value: '*' op: contains","title":"Appservicecors compute"},{"location":"azure/examples/appservicecors-compute/#app-services-filter-by-cors-configuration-azure_examples_app_service_cors","text":"Filter to select all Application Services (Web Apps and Functions) with a Cross-Origin Resource Sharing (CORS) configuration set to allow all origins. policies: - name: app-service-cors-policy description: | Get all wildcard CORS configurations resource: azure.webapp filters: - type: configuration key: cors.allowedOrigins value: '*' op: contains","title":"App Services - Filter By CORS Configuration {#azure_examples_app_service_cors}"},{"location":"azure/examples/appserviceplansresize-compute/","text":"App Service - Resize All Application Service Plans {#azure_examples_resize_app_service_plan} Count or Size can be provided individually or together. Add filters to resize specific Application Service Plans. Note: This will not resize consumption based plans. policies: - name: azure-resize-plan resource: azure.appserviceplan actions: - type: resize-plan size: F1 # F1, B1, B2, B3, S1, S2, S3, P1v2, P2v2, P3v2 count: 1","title":"Appserviceplansresize compute"},{"location":"azure/examples/appserviceplansresize-compute/#app-service-resize-all-application-service-plans-azure_examples_resize_app_service_plan","text":"Count or Size can be provided individually or together. Add filters to resize specific Application Service Plans. Note: This will not resize consumption based plans. policies: - name: azure-resize-plan resource: azure.appserviceplan actions: - type: resize-plan size: F1 # F1, B1, B2, B3, S1, S2, S3, P1v2, P2v2, P3v2 count: 1","title":"App Service - Resize All Application Service Plans {#azure_examples_resize_app_service_plan}"},{"location":"azure/examples/cosmosdb-collection-on-off-hours-databases/","text":"Cosmos DB Collections - Resize Throughput with On/Off Hours With Azure Cosmos DB, you pay for the throughput you provision. Sometimes it\\'s known that Cosmos DB will not be utilized during certain hours of the day. To save cost during those times, it\\'s useful for the higher throughput collections to be resized down to a lower throughput. Combining the following filters and actions will allow us to resize and restore the throughput state of Cosmos DB collections according to hours of the day: Filters: onhour : allows us to filter actions so they execute only during on hours. The filter can be applied using the parent filter for the Cosmos DB Account. offhour : allows us to filter actions so they execute only during off hours. The filter can be applied using the parent filter for the Cosmos DB Account. offer : allows us to filter collections with high throughputs (in this example, greater than 800) Actions: save-throughput-state : saves the current state of the collections in a tag replace-offer : resizes collections during off hours (in this example, down to 400) restore-throughput-state : restores the throughput state of the collections from the tag provided in the store-throughput-state action Note: The tag provided to save-throughput-state and restore-throughput-state must be the same. policies: - name: restore-collections-throughput-during-on-hours resource: azure.cosmosdb-collection filters: - type: parent filter: type: onhour default_tz: pt actions: - type: restore-throughput-state state-tag: on-hours-throughput - name: save-collections-throughput-and-resize-during-off-hours resource: azure.cosmosdb-collection filters: - type: parent filter: type: offhour default_tz: pt - type: offer key: content.offerThroughput op: gt value: 800 actions: - type: save-throughput-state state-tag: on-hours-throughput - type: replace-offer throughput: 400","title":"Cosmosdb collection on off hours databases"},{"location":"azure/examples/cosmosdb-collection-on-off-hours-databases/#cosmos-db-collections-resize-throughput-with-onoff-hours","text":"With Azure Cosmos DB, you pay for the throughput you provision. Sometimes it\\'s known that Cosmos DB will not be utilized during certain hours of the day. To save cost during those times, it\\'s useful for the higher throughput collections to be resized down to a lower throughput. Combining the following filters and actions will allow us to resize and restore the throughput state of Cosmos DB collections according to hours of the day: Filters: onhour : allows us to filter actions so they execute only during on hours. The filter can be applied using the parent filter for the Cosmos DB Account. offhour : allows us to filter actions so they execute only during off hours. The filter can be applied using the parent filter for the Cosmos DB Account. offer : allows us to filter collections with high throughputs (in this example, greater than 800) Actions: save-throughput-state : saves the current state of the collections in a tag replace-offer : resizes collections during off hours (in this example, down to 400) restore-throughput-state : restores the throughput state of the collections from the tag provided in the store-throughput-state action Note: The tag provided to save-throughput-state and restore-throughput-state must be the same. policies: - name: restore-collections-throughput-during-on-hours resource: azure.cosmosdb-collection filters: - type: parent filter: type: onhour default_tz: pt actions: - type: restore-throughput-state state-tag: on-hours-throughput - name: save-collections-throughput-and-resize-during-off-hours resource: azure.cosmosdb-collection filters: - type: parent filter: type: offhour default_tz: pt - type: offer key: content.offerThroughput op: gt value: 800 actions: - type: save-throughput-state state-tag: on-hours-throughput - type: replace-offer throughput: 400","title":"Cosmos DB Collections - Resize Throughput with On/Off Hours"},{"location":"azure/examples/emailnotify-notifications/","text":"Email - Send Users an Email Action to queue email after the mailer is configured. See c7n_mailer readme.md for more information. policies: - name: notify resource: azure.resourcegroup actions: - type: notify template: default subject: Hello World to: - someone@somewhere.com transport: type: asq queue: https://storagename.queue.core.windows.net/queuename","title":"Emailnotify notifications"},{"location":"azure/examples/emailnotify-notifications/#email-send-users-an-email","text":"Action to queue email after the mailer is configured. See c7n_mailer readme.md for more information. policies: - name: notify resource: azure.resourcegroup actions: - type: notify template: default subject: Hello World to: - someone@somewhere.com transport: type: asq queue: https://storagename.queue.core.windows.net/queuename","title":"Email - Send Users an Email"},{"location":"azure/examples/firewall-cosmosaction-networking/","text":"Firewall - Update CosmosDB Rules In this example we identify Cosmos DB accounts that either have no firewall configured or which have one configured which is allowing access outside of expected ranges. We then reconfigure that firewall to known-safe defaults which include a bypass for all of the Azure Cloud as well as additional space in our data center. Virtual network rules are not specified so they will not be modified. policies: - name: cosmos-firewall-enable description: | Find all incorrect firewalls and enable with a set of defaults resource: azure.cosmosdb filters: - or: - type: value key: properties.ipRangeFilter value: empty # The firewall is disabled - not: - type: firewall-rules only: # Should *only* allow access within the specified maximums here - 19.0.0.0/16 - 20.0.1.2 - ServiceTags.AzureCloud actions: - type: set-firewall-rules append: False bypass-rules: # Enable firewall and allow all Azure Cloud - AzureCloud - Portal ip-rules: # and some external IP space - 19.0.0.0/16 - 20.0.1.2","title":"Firewall cosmosaction networking"},{"location":"azure/examples/firewall-cosmosaction-networking/#firewall-update-cosmosdb-rules","text":"In this example we identify Cosmos DB accounts that either have no firewall configured or which have one configured which is allowing access outside of expected ranges. We then reconfigure that firewall to known-safe defaults which include a bypass for all of the Azure Cloud as well as additional space in our data center. Virtual network rules are not specified so they will not be modified. policies: - name: cosmos-firewall-enable description: | Find all incorrect firewalls and enable with a set of defaults resource: azure.cosmosdb filters: - or: - type: value key: properties.ipRangeFilter value: empty # The firewall is disabled - not: - type: firewall-rules only: # Should *only* allow access within the specified maximums here - 19.0.0.0/16 - 20.0.1.2 - ServiceTags.AzureCloud actions: - type: set-firewall-rules append: False bypass-rules: # Enable firewall and allow all Azure Cloud - AzureCloud - Portal ip-rules: # and some external IP space - 19.0.0.0/16 - 20.0.1.2","title":"Firewall - Update CosmosDB Rules"},{"location":"azure/examples/firewall-rulesfiltering-networking/","text":"Firewall - Filter Storage Accounts By Rules This example demonstrates a common filtering scenario where we would like to ensure all firewalls are configured to only allow access from IP addresses in our datacenter (or any block of IP space). Below we look at storage accounts and we identify any accounts where the firewall is not enabled or the firewall is enabled but it allows IP\\'s that are not within the specified IP space. The IP space is specified as an array and can contain a variety of formats as shown in the example. The only field used in the firewall rules filter returns any resources where the firewall only contains IP\\'s from the list provided. It need not contain all of them (or any of them). In this example we use the not modifier to find non-compliant resources. You could further extend this example by using the set-network-rules action to remediate the non-compliant resources. policies: - name: storage-only-allow-datacenter-ips description: | Find all storage accounts which permit access from any IP not in datacenter IP space resource: azure.storage filters: - or: - type: value key: properties.networkAcls.defaultAction value: 'Allow' - not: - type: firewall-rules only: - '8.8.8.8' - '10.0.0.0/16' - '20.0.0.0 - 20.10.0.0'","title":"Firewall rulesfiltering networking"},{"location":"azure/examples/firewall-rulesfiltering-networking/#firewall-filter-storage-accounts-by-rules","text":"This example demonstrates a common filtering scenario where we would like to ensure all firewalls are configured to only allow access from IP addresses in our datacenter (or any block of IP space). Below we look at storage accounts and we identify any accounts where the firewall is not enabled or the firewall is enabled but it allows IP\\'s that are not within the specified IP space. The IP space is specified as an array and can contain a variety of formats as shown in the example. The only field used in the firewall rules filter returns any resources where the firewall only contains IP\\'s from the list provided. It need not contain all of them (or any of them). In this example we use the not modifier to find non-compliant resources. You could further extend this example by using the set-network-rules action to remediate the non-compliant resources. policies: - name: storage-only-allow-datacenter-ips description: | Find all storage accounts which permit access from any IP not in datacenter IP space resource: azure.storage filters: - or: - type: value key: properties.networkAcls.defaultAction value: 'Allow' - not: - type: firewall-rules only: - '8.8.8.8' - '10.0.0.0/16' - '20.0.0.0 - 20.10.0.0'","title":"Firewall - Filter Storage Accounts By Rules"},{"location":"azure/examples/loadbalancerfilterbyfrotendip-networking/","text":"Load Balancer - Filter load balancer by front end public ip Filter to select load balancers with an ipv6 frontend public IP. policies: - name: loadbalancer-with-ipv6-frontend resource: azure.loadbalancer filters: - type: frontend-public-ip key: properties.publicIPAddressVersion op: in value_type: normalize value: \"ipv6\"","title":"Loadbalancerfilterbyfrotendip networking"},{"location":"azure/examples/loadbalancerfilterbyfrotendip-networking/#load-balancer-filter-load-balancer-by-front-end-public-ip","text":"Filter to select load balancers with an ipv6 frontend public IP. policies: - name: loadbalancer-with-ipv6-frontend resource: azure.loadbalancer filters: - type: frontend-public-ip key: properties.publicIPAddressVersion op: in value_type: normalize value: \"ipv6\"","title":"Load Balancer - Filter load balancer by front end public ip"},{"location":"azure/examples/metrics-general/","text":"Monitor - Filter resources by metrics from Azure Monitor Find VMs with an average Percentage CPU greater than or equal to 75% over the last 12 hours policies: - name: find-busy-vms description: Find VMs with avg cpu >= 75% over the last 12 hours resource: azure.vm filters: - type: metric metric: Percentage CPU aggregation: average op: ge threshold: 75 timeframe: 12 Find KeyVaults with more than 1000 API hits in the last hour policies: - name: keyvault-hits resource: azure.keyvault filters: - type: metric metric: ServiceApiHit aggregation: total op: gt threshold: 1000 timeframe: 1 Find SQL servers with less than 10% average DTU consumption over last 24 hours policies: - name: dtu-consumption resource: azure.sqlserver filters: - type: metric metric: dtu_consumption_percent aggregation: average op: lt threshold: 10 timeframe: 24 filter: \"DatabaseResourceId eq '*'\"","title":"Metrics general"},{"location":"azure/examples/metrics-general/#monitor-filter-resources-by-metrics-from-azure-monitor","text":"Find VMs with an average Percentage CPU greater than or equal to 75% over the last 12 hours policies: - name: find-busy-vms description: Find VMs with avg cpu >= 75% over the last 12 hours resource: azure.vm filters: - type: metric metric: Percentage CPU aggregation: average op: ge threshold: 75 timeframe: 12 Find KeyVaults with more than 1000 API hits in the last hour policies: - name: keyvault-hits resource: azure.keyvault filters: - type: metric metric: ServiceApiHit aggregation: total op: gt threshold: 1000 timeframe: 1 Find SQL servers with less than 10% average DTU consumption over last 24 hours policies: - name: dtu-consumption resource: azure.sqlserver filters: - type: metric metric: dtu_consumption_percent aggregation: average op: lt threshold: 10 timeframe: 24 filter: \"DatabaseResourceId eq '*'\"","title":"Monitor - Filter resources by metrics from Azure Monitor"},{"location":"azure/examples/networksecuritygroups-networking/","text":"Network Security Groups - Deny access to Network Security Group This policy will deny access to all ports that are NOT 22, 23 or 24 for all Network Security Groups For more examples see azure.networksecuritygroup {.interpreted-text role=\"ref\"} policies: - name: close-inbound-except-22-24 resource: azure.networksecuritygroup filters: - type: ingress exceptPorts: '22-24' match: 'any' access: 'Allow' actions: - type: close exceptPorts: '22-24' direction: 'Inbound'","title":"Networksecuritygroups networking"},{"location":"azure/examples/networksecuritygroups-networking/#network-security-groups-deny-access-to-network-security-group","text":"This policy will deny access to all ports that are NOT 22, 23 or 24 for all Network Security Groups For more examples see azure.networksecuritygroup {.interpreted-text role=\"ref\"} policies: - name: close-inbound-except-22-24 resource: azure.networksecuritygroup filters: - type: ingress exceptPorts: '22-24' match: 'any' access: 'Allow' actions: - type: close exceptPorts: '22-24' direction: 'Inbound'","title":"Network Security Groups - Deny access to Network Security Group"},{"location":"azure/examples/resourcegroupsdelayedoperation-general/","text":"Resource Groups - Delayed operations {#azure_example_delayedoperation} You can use the mark-for-op action and the marked-for-op filter to implement delayed actions, such as delete a resource if it remains non-compliant for a few days. This set of policies tags all empty resource groups with a special tag. If tagged group remains empty, it will be remove after 7 days. If the days field is omitted the empty resource groups will be deleted immediately. If resource group is no longer empty, tag is removed. policies: - name: rg-mark-empty-for-deletion description: | Find any empty resource groups and mark for deletion in 7 days resource: azure.resourcegroup filters: - \"tag:c7n_rg_empty\": absent - type: empty-group actions: - type: mark-for-op tag: c7n_rg_empty op: delete days: 7 - name: rg-unmark-if-not-empty resource: azure.resourcegroup description: | Remove the deletion tag from any resource group which now contain resources so it doesn't get deleted by the following policy filters: - \"tag:c7n_rg_empty\": not-null - not: - type: empty-group actions: - type: untag tags: ['c7n_rg_empty'] - name: rg-delete-empty resource: azure.resourcegroup description: | Delete any marked resource groups which are empty if it has been that way for 7 days or more. filters: - type: marked-for-op tag: c7n_rg_empty op: delete actions: - type: delete","title":"Resourcegroupsdelayedoperation general"},{"location":"azure/examples/resourcegroupsdelayedoperation-general/#resource-groups-delayed-operations-azure_example_delayedoperation","text":"You can use the mark-for-op action and the marked-for-op filter to implement delayed actions, such as delete a resource if it remains non-compliant for a few days. This set of policies tags all empty resource groups with a special tag. If tagged group remains empty, it will be remove after 7 days. If the days field is omitted the empty resource groups will be deleted immediately. If resource group is no longer empty, tag is removed. policies: - name: rg-mark-empty-for-deletion description: | Find any empty resource groups and mark for deletion in 7 days resource: azure.resourcegroup filters: - \"tag:c7n_rg_empty\": absent - type: empty-group actions: - type: mark-for-op tag: c7n_rg_empty op: delete days: 7 - name: rg-unmark-if-not-empty resource: azure.resourcegroup description: | Remove the deletion tag from any resource group which now contain resources so it doesn't get deleted by the following policy filters: - \"tag:c7n_rg_empty\": not-null - not: - type: empty-group actions: - type: untag tags: ['c7n_rg_empty'] - name: rg-delete-empty resource: azure.resourcegroup description: | Delete any marked resource groups which are empty if it has been that way for 7 days or more. filters: - type: marked-for-op tag: c7n_rg_empty op: delete actions: - type: delete","title":"Resource Groups - Delayed operations {#azure_example_delayedoperation}"},{"location":"azure/examples/resourcegroupsorphanresources-general-compute-networking/","text":"Resource Groups - Delete or report on orphan resources (NICs, Disks, Public IPs) {#azure_orphanresources} ::: {#azure_orphanresources-disk} Deletes all disks that are not being managed by a VM: ::: policies: - name: orphaned-disk resource: azure.disk filters: - type: value key: managedBy value: null actions: - type: delete ::: {#azure_orphanresources-nic} Gets all Network Interfaces that are not attached to any VMs: ::: policies: - name: orphaned-nic resource: azure.networkinterface filters: - type: value key: properties.virtualMachine value: null ::: {#azure_orphanresources-publicip} Queues an email with Public IPs that are not attached to any Network Interfaces. See c7n_mailer readme.md for more information on how to send an email. ::: policies: - name: orphaned-ip resource: azure.publicip filters: - type: value key: properties.ipConfiguration value: null actions: - type: notify template: default subject: Orphaned Public IP resource to: - someone@somewhere.com transport: type: asq queue: https://storagename.queue.core.windows.net/queuename","title":"Resourcegroupsorphanresources general compute networking"},{"location":"azure/examples/resourcegroupsorphanresources-general-compute-networking/#resource-groups-delete-or-report-on-orphan-resources-nics-disks-public-ips-azure_orphanresources","text":"::: {#azure_orphanresources-disk} Deletes all disks that are not being managed by a VM: ::: policies: - name: orphaned-disk resource: azure.disk filters: - type: value key: managedBy value: null actions: - type: delete ::: {#azure_orphanresources-nic} Gets all Network Interfaces that are not attached to any VMs: ::: policies: - name: orphaned-nic resource: azure.networkinterface filters: - type: value key: properties.virtualMachine value: null ::: {#azure_orphanresources-publicip} Queues an email with Public IPs that are not attached to any Network Interfaces. See c7n_mailer readme.md for more information on how to send an email. ::: policies: - name: orphaned-ip resource: azure.publicip filters: - type: value key: properties.ipConfiguration value: null actions: - type: notify template: default subject: Orphaned Public IP resource to: - someone@somewhere.com transport: type: asq queue: https://storagename.queue.core.windows.net/queuename","title":"Resource Groups - Delete or report on orphan resources (NICs, Disks, Public IPs) {#azure_orphanresources}"},{"location":"azure/examples/resourcegroupsremoveempty-general/","text":"Resource Groups - Remove empty Resource Groups Removes all empty resource groups from the subscription: policies: - name: rg-remove-empty description: | Removes any empty resource groups from subscription resource: azure.resourcegroup filters: - type: empty-group actions: - type: delete","title":"Resourcegroupsremoveempty general"},{"location":"azure/examples/resourcegroupsremoveempty-general/#resource-groups-remove-empty-resource-groups","text":"Removes all empty resource groups from the subscription: policies: - name: rg-remove-empty description: | Removes any empty resource groups from subscription resource: azure.resourcegroup filters: - type: empty-group actions: - type: delete","title":"Resource Groups - Remove empty Resource Groups"},{"location":"azure/examples/routetablewithsubnet-networking/","text":"Routes - Find route tables with a specific subnet {#azure_examples_routetable_with_subnet} Returns all route tables that route to the subnet named [subnetname]{.title-ref} in the virtual network named [vnetname]{.title-ref}. Since the relationship between route tables and subnets is one-to-many, either 0 or 1 route tables will be returned for a given subnet. policies: - name: routes-to-specific-subnet resource: azure.routetable filters: - type: value key: properties.subnets[?ends_with(id, 'vnetname/subnets/subnetname')] | [0] value: not-null","title":"Routetablewithsubnet networking"},{"location":"azure/examples/routetablewithsubnet-networking/#routes-find-route-tables-with-a-specific-subnet-azure_examples_routetable_with_subnet","text":"Returns all route tables that route to the subnet named [subnetname]{.title-ref} in the virtual network named [vnetname]{.title-ref}. Since the relationship between route tables and subnets is one-to-many, either 0 or 1 route tables will be returned for a given subnet. policies: - name: routes-to-specific-subnet resource: azure.routetable filters: - type: value key: properties.subnets[?ends_with(id, 'vnetname/subnets/subnetname')] | [0] value: not-null","title":"Routes - Find route tables with a specific subnet {#azure_examples_routetable_with_subnet}"},{"location":"azure/examples/sqldatabasebackupretention-databases/","text":"SQL - Find databases with specific retention options {#azure_examples_sqldatabasebackupretention} Find SQL Databases with a short term backup retention less than 14 days. policies: - name: short-term-backup-retention resource: azure.sqldatabase filters: - type: short-term-backup-retention op: lt retention-period-days: 14 Find SQL Databases with a monthly long term backup retention period more than one year policies: - name: long-term-backup-retention resource: azure.sqldatabase filters: - type: long-term-backup-retention backup-type: monthly op: gt retention-period: 1 retention-period-units: year","title":"Sqldatabasebackupretention databases"},{"location":"azure/examples/sqldatabasebackupretention-databases/#sql-find-databases-with-specific-retention-options-azure_examples_sqldatabasebackupretention","text":"Find SQL Databases with a short term backup retention less than 14 days. policies: - name: short-term-backup-retention resource: azure.sqldatabase filters: - type: short-term-backup-retention op: lt retention-period-days: 14 Find SQL Databases with a monthly long term backup retention period more than one year policies: - name: long-term-backup-retention resource: azure.sqldatabase filters: - type: long-term-backup-retention backup-type: monthly op: gt retention-period: 1 retention-period-units: year","title":"SQL - Find databases with specific retention options {#azure_examples_sqldatabasebackupretention}"},{"location":"azure/examples/sqldatabaseupdateretentionpolicies-databases/","text":"SQL - Update SQL Database retention policies {#azure_examples_sqldatabaseupdateretentionpolicies} Update any SQL Database short term retentions to at least 7 days policies: - name: update-short-term-backup-retention-policy resource: azure.sqldatabase filters: - type: short-term-backup-retention op: lt retention-period-days: 7 actions: - type: update-short-term-backup-retention retention-period-days: 7 Enforce a 1 month maximum retention for weekly backups on all SQL Databases policies: - name: update-long-term-backup-retention-policy resource: azure.sqldatabase filters: - type: long-term-backup-retention backup-type: weekly op: gt retention-period: 1 retention-period-units: months actions: - type: update-long-term-backup-retention backup-type: weekly retention-period: 1 retention-period-units: months","title":"Sqldatabaseupdateretentionpolicies databases"},{"location":"azure/examples/sqldatabaseupdateretentionpolicies-databases/#sql-update-sql-database-retention-policies-azure_examples_sqldatabaseupdateretentionpolicies","text":"Update any SQL Database short term retentions to at least 7 days policies: - name: update-short-term-backup-retention-policy resource: azure.sqldatabase filters: - type: short-term-backup-retention op: lt retention-period-days: 7 actions: - type: update-short-term-backup-retention retention-period-days: 7 Enforce a 1 month maximum retention for weekly backups on all SQL Databases policies: - name: update-long-term-backup-retention-policy resource: azure.sqldatabase filters: - type: long-term-backup-retention backup-type: weekly op: gt retention-period: 1 retention-period-units: months actions: - type: update-long-term-backup-retention backup-type: weekly retention-period: 1 retention-period-units: months","title":"SQL - Update SQL Database retention policies {#azure_examples_sqldatabaseupdateretentionpolicies}"},{"location":"azure/examples/sqldatabasewithpremiumsku-databases/","text":"SQL - Find all SQL Databases with Premium SKU {#azure_examples_sqldatabasewithpremiumsku} Find all SQL databases with Premium SKU. policies: - name: sqldatabase-with-premium-sku resource: azure.sqldatabase filters: - type: value key: sku.tier op: eq value: Premium","title":"Sqldatabasewithpremiumsku databases"},{"location":"azure/examples/sqldatabasewithpremiumsku-databases/#sql-find-all-sql-databases-with-premium-sku-azure_examples_sqldatabasewithpremiumsku","text":"Find all SQL databases with Premium SKU. policies: - name: sqldatabase-with-premium-sku resource: azure.sqldatabase filters: - type: value key: sku.tier op: eq value: Premium","title":"SQL - Find all SQL Databases with Premium SKU {#azure_examples_sqldatabasewithpremiumsku}"},{"location":"azure/examples/storageaddfirewallrulestostorage-databases-networking/","text":"Storage - Add storage firewall rules {#azure_examples_add_firewall_rules_to_storage} Finds storage accounts with no ip rules and modifies them to: - Deny public access - Enable Logging and Metrics access to the database - Allow access from the locations specified by ip-rules and virtual-network-rules policies: - name: add-storage-firewall description: | Find storage accounts without open ip list and restrict them. resource: azure.storage filters: - type: value key: properties.networkAcls.ipRules value_type: size op: eq value: 0 actions: - type: set-firewall-rules default-action: Deny bypass: [Logging, Metrics] ip-rules: - 11.12.13.14 - 21.22.23.24 virtual-network-rules: - virtual-network-resource-id: /subscriptions/12345678-1234-1234-1234-123456789012/resourceGroups/rg1/providers/Microsoft.Network/virtualNetworks/vnet1/subnets/subnet1 - virtual-network-resource-id: /subscriptions/12345678-1234-1234-1234-123456789012/resourceGroups/rg1/providers/Microsoft.Network/virtualNetworks/vnet2/subnets/subnet2 Finds storage account with no ip rules and modifies it to: - Enable Logging and Metrics access to the database - Allow access from the location specified by ip-rules Note: Because this policy leaves the storage account open to access by all ip addresses, the new rules will have no effect on the performance until the default-action is set to Deny. policies: - name: add-inactive-storage-firewall description: | Find storage accounts without open ip list and add some rules. The rules will be stored as inactive and can be activated later. resource: azure.storage filters: - type: value key: properties.networkAcls.ipRules value_type: size op: eq value: 0 actions: - type: set-firewall-rules default-action: Allow bypass: [Logging, Metrics] ip-rules: - 11.12.13.14 - 21.22.23.24","title":"Storageaddfirewallrulestostorage databases networking"},{"location":"azure/examples/storageaddfirewallrulestostorage-databases-networking/#storage-add-storage-firewall-rules-azure_examples_add_firewall_rules_to_storage","text":"Finds storage accounts with no ip rules and modifies them to: - Deny public access - Enable Logging and Metrics access to the database - Allow access from the locations specified by ip-rules and virtual-network-rules policies: - name: add-storage-firewall description: | Find storage accounts without open ip list and restrict them. resource: azure.storage filters: - type: value key: properties.networkAcls.ipRules value_type: size op: eq value: 0 actions: - type: set-firewall-rules default-action: Deny bypass: [Logging, Metrics] ip-rules: - 11.12.13.14 - 21.22.23.24 virtual-network-rules: - virtual-network-resource-id: /subscriptions/12345678-1234-1234-1234-123456789012/resourceGroups/rg1/providers/Microsoft.Network/virtualNetworks/vnet1/subnets/subnet1 - virtual-network-resource-id: /subscriptions/12345678-1234-1234-1234-123456789012/resourceGroups/rg1/providers/Microsoft.Network/virtualNetworks/vnet2/subnets/subnet2 Finds storage account with no ip rules and modifies it to: - Enable Logging and Metrics access to the database - Allow access from the location specified by ip-rules Note: Because this policy leaves the storage account open to access by all ip addresses, the new rules will have no effect on the performance until the default-action is set to Deny. policies: - name: add-inactive-storage-firewall description: | Find storage accounts without open ip list and add some rules. The rules will be stored as inactive and can be activated later. resource: azure.storage filters: - type: value key: properties.networkAcls.ipRules value_type: size op: eq value: 0 actions: - type: set-firewall-rules default-action: Allow bypass: [Logging, Metrics] ip-rules: - 11.12.13.14 - 21.22.23.24","title":"Storage - Add storage firewall rules {#azure_examples_add_firewall_rules_to_storage}"},{"location":"azure/examples/storageblockpublicaccess-networking-databases/","text":"Storage - Block public access Restricts access to storage accounts with specified ip rules to only the ips specified: policies: - name: storage-block-public-access description: | Blocks public access to storage accounts with defined IP access rules. resource: azure.storage filters: - type: value key: properties.networkAcls.ipRules value_type: size op: ne value: 0 actions: - type: set-firewall-rules default-action: Deny ip-rules: []","title":"Storageblockpublicaccess networking databases"},{"location":"azure/examples/storageblockpublicaccess-networking-databases/#storage-block-public-access","text":"Restricts access to storage accounts with specified ip rules to only the ips specified: policies: - name: storage-block-public-access description: | Blocks public access to storage accounts with defined IP access rules. resource: azure.storage filters: - type: value key: properties.networkAcls.ipRules value_type: size op: ne value: 0 actions: - type: set-firewall-rules default-action: Deny ip-rules: []","title":"Storage - Block public access"},{"location":"azure/examples/storagecontainerevent-databases/","text":"Storage - Monitor newly created Containers for public access {#azure_examples_storage_container_event} Deploy an Azure Function to monitor real-time Blob Storage Container events. - Filter incoming container events on the publicAccess property. - Provides a way to act quickly on any changes to existing containers or creation of new containers. - Add your own actions to notify or mitigate as needed. policies: - name: storage_container_public_access_event description: 'Identity containers with public access' mode: type: azure-event-grid events: - StorageContainerWrite provision-options: identity: type: UserAssigned id: custodian_identity execution-options: output_dir: azure://<storage_account>.blob.core.windows.net/custodian resource: azure.storage-container filters: - type: value key: properties.publicAccess op: not-equal value: None # Possible values: Blob, Container, None","title":"Storagecontainerevent databases"},{"location":"azure/examples/storagecontainerevent-databases/#storage-monitor-newly-created-containers-for-public-access-azure_examples_storage_container_event","text":"Deploy an Azure Function to monitor real-time Blob Storage Container events. - Filter incoming container events on the publicAccess property. - Provides a way to act quickly on any changes to existing containers or creation of new containers. - Add your own actions to notify or mitigate as needed. policies: - name: storage_container_public_access_event description: 'Identity containers with public access' mode: type: azure-event-grid events: - StorageContainerWrite provision-options: identity: type: UserAssigned id: custodian_identity execution-options: output_dir: azure://<storage_account>.blob.core.windows.net/custodian resource: azure.storage-container filters: - type: value key: properties.publicAccess op: not-equal value: None # Possible values: Blob, Container, None","title":"Storage - Monitor newly created Containers for public access {#azure_examples_storage_container_event}"},{"location":"azure/examples/tagadd-general-compute/","text":"Tags - Add tag to Virtual Machines Add the tag [TagName]{.title-ref} with value [TagValue]{.title-ref} to all VMs in the subscription policies: - name: tag-add description: | Adds a tag to all virtual machines resource: azure.vm actions: - type: tag tag: TagName value: TagValue","title":"Tagadd general compute"},{"location":"azure/examples/tagadd-general-compute/#tags-add-tag-to-virtual-machines","text":"Add the tag [TagName]{.title-ref} with value [TagValue]{.title-ref} to all VMs in the subscription policies: - name: tag-add description: | Adds a tag to all virtual machines resource: azure.vm actions: - type: tag tag: TagName value: TagValue","title":"Tags - Add tag to Virtual Machines"},{"location":"azure/examples/tagautotagusers-general-identity/","text":"Tags - Automatically tag the creator of a resource or resource group {#azure_examples_autotagusers} General notes: It is strongly recommended to always use resource-type filter when using armresource . Tag operation is allowed only for ARM resources supported by Cloud Custodian, error message is logged for all unknown and unsupported resources. In non-event mode (periodic function or local execution), the action will look in Azure Activity Logs up to 90 days prior to execution to find the user email (default). It will attempt to identify the first user or Service Principal that performed the \\'write\\' operation for each resource. In event mode, the action will use the data from the event to determine the user or principal that performed the action. The days option is not applicable in this mode. Note : Resource Groups aren\\'t a part of armresource type, so tags needs to be added separately. Note : Service Principals do not have email addresses and their tag values will be the application id. Tag all resource groups with the email address of the creator. The email address is taken from the oldest user to \\'write\\' the resource group within the last 10 days. policies: - name: azure-auto-tag-creator-resource-groups resource: azure.resourcegroup description: | Tag all existing resource groups with the 'CreatorEmail' tag; looking up to 10 days prior. actions: - type: auto-tag-user tag: CreatorEmail days: 10 Tag all virtual networks, storage accounts and virtual machines with their creator email, limited to 10 days of logs to identify the creator. policies: - name: azure-auto-tag-creator-resources resource: azure.armresource description: | Tag all arm resources of the VMs, VNETs and Storage accounts with the 'Creator Email' tag; looking up to 10 days prior. filters: - type: resource-type values: - Microsoft.Network/virtualNetworks - Microsoft.Storage/storageAccounts - Microsoft.Compute/virtualMachines actions: - type: auto-tag-user tag: CreatorEmail days: 10 Event grid triggered policy to tag all virtual machines and storage accounts with the creator email. The creator email is contained in the event message, so days is not needed. policies: - name: azure-auto-tag-creator mode: type: azure-event-grid events: ['VmWrite', 'StorageWrite'] resource: azure.armresource description: | Tag all new VMs and StorageAccounts with the 'Creator Email' tag. Note: 'resource-type' filter is not required because policy is not triggered by other resources. actions: - type: auto-tag-user tag: CreatorEmail","title":"Tagautotagusers general identity"},{"location":"azure/examples/tagautotagusers-general-identity/#tags-automatically-tag-the-creator-of-a-resource-or-resource-group-azure_examples_autotagusers","text":"General notes: It is strongly recommended to always use resource-type filter when using armresource . Tag operation is allowed only for ARM resources supported by Cloud Custodian, error message is logged for all unknown and unsupported resources. In non-event mode (periodic function or local execution), the action will look in Azure Activity Logs up to 90 days prior to execution to find the user email (default). It will attempt to identify the first user or Service Principal that performed the \\'write\\' operation for each resource. In event mode, the action will use the data from the event to determine the user or principal that performed the action. The days option is not applicable in this mode. Note : Resource Groups aren\\'t a part of armresource type, so tags needs to be added separately. Note : Service Principals do not have email addresses and their tag values will be the application id. Tag all resource groups with the email address of the creator. The email address is taken from the oldest user to \\'write\\' the resource group within the last 10 days. policies: - name: azure-auto-tag-creator-resource-groups resource: azure.resourcegroup description: | Tag all existing resource groups with the 'CreatorEmail' tag; looking up to 10 days prior. actions: - type: auto-tag-user tag: CreatorEmail days: 10 Tag all virtual networks, storage accounts and virtual machines with their creator email, limited to 10 days of logs to identify the creator. policies: - name: azure-auto-tag-creator-resources resource: azure.armresource description: | Tag all arm resources of the VMs, VNETs and Storage accounts with the 'Creator Email' tag; looking up to 10 days prior. filters: - type: resource-type values: - Microsoft.Network/virtualNetworks - Microsoft.Storage/storageAccounts - Microsoft.Compute/virtualMachines actions: - type: auto-tag-user tag: CreatorEmail days: 10 Event grid triggered policy to tag all virtual machines and storage accounts with the creator email. The creator email is contained in the event message, so days is not needed. policies: - name: azure-auto-tag-creator mode: type: azure-event-grid events: ['VmWrite', 'StorageWrite'] resource: azure.armresource description: | Tag all new VMs and StorageAccounts with the 'Creator Email' tag. Note: 'resource-type' filter is not required because policy is not triggered by other resources. actions: - type: auto-tag-user tag: CreatorEmail","title":"Tags - Automatically tag the creator of a resource or resource group {#azure_examples_autotagusers}"},{"location":"azure/examples/tagremove-general-compute/","text":"Tags - Remove tag From Virtual Machines Remove the tags [TagName]{.title-ref} and [TagName2]{.title-ref} from all VMs in the subscription policies: - name: tag-remove description: | Removes tags from all virtual machines resource: azure.vm actions: - type: untag tags: ['TagName', 'TagName2']","title":"Tagremove general compute"},{"location":"azure/examples/tagremove-general-compute/#tags-remove-tag-from-virtual-machines","text":"Remove the tags [TagName]{.title-ref} and [TagName2]{.title-ref} from all VMs in the subscription policies: - name: tag-remove description: | Removes tags from all virtual machines resource: azure.vm actions: - type: untag tags: ['TagName', 'TagName2']","title":"Tags - Remove tag From Virtual Machines"},{"location":"azure/examples/tagtrim-general-compute/","text":"Tags - Trim tags From Virtual Machines Azure Resources and Resource Groups have a limit of 15 tags. This action can be used to remove enough tags to make the desired amount of space while preserving a given set of tags. Setting space to 0 will remove all tags not listed to preserve. This example limits the number of tags on all virtual machines to 12, while making sure to keep [TagName1]{.title-ref} and [TagName2]{.title-ref} preserved. policies: - name: tag-trim description: | Trims tags from resources to make additional space resource: azure.vm actions: - type: tag-trim preserve: ['TagName1', 'TagName2'] space: 3","title":"Tagtrim general compute"},{"location":"azure/examples/tagtrim-general-compute/#tags-trim-tags-from-virtual-machines","text":"Azure Resources and Resource Groups have a limit of 15 tags. This action can be used to remove enough tags to make the desired amount of space while preserving a given set of tags. Setting space to 0 will remove all tags not listed to preserve. This example limits the number of tags on all virtual machines to 12, while making sure to keep [TagName1]{.title-ref} and [TagName2]{.title-ref} preserved. policies: - name: tag-trim description: | Trims tags from resources to make additional space resource: azure.vm actions: - type: tag-trim preserve: ['TagName1', 'TagName2'] space: 3","title":"Tags - Trim tags From Virtual Machines"},{"location":"azure/examples/teamsnewresourcegroup-general-notifications/","text":"Resource Group - Generate a Teams Message on Create {#azure_examples_teams_new_resource_group} This policy will send a notification to a Microsoft Teams channel when a new resource group is created in the subscription. There is a few minute delay between the resource group being created and the notification appearing. In order to target the correct Teams channel, you will need to insert the custom incoming webhook for the channel. If you do not have a webhook set up, please see Setting up a custom incoming webhook . We are using an actionable message card to provide a link to the new resource group. Note: linking to portal pages within Azure is not supported by all web browsers. To run the policy, you must replace <your_webhook_here> with the correct url. For more information on how the event grid function works please see azure_functionshosting {.interpreted-text role=\"ref\"} policies: - name: notify-new-resource-group description: | Generates a Teams notification when a new resource group is created resource: azure.resourcegroup mode: type: azure-event-grid events: [{ resourceProvider: Microsoft.Resources/subscriptions/resourceGroups, event: write }] actions: - type: webhook url: <your_webhook_here> batch: false body: > { \"@context\": `https://schema.org/extensions`, \"@type\": `MessageCard`, \"themeColor\": `0072C6`, \"title\": `New Resource Group Created`, \"text\": join('', [`A new resource group has been created in subscription `, account_id, `.\\n\\nResource Group Name: `, resource.name, `\\n\\nResource Group Location: `, resource.location]) \"potentialAction\": [ { \"@type\": `OpenUri`, \"name\": `Open In Portal`, \"targets\": [ { \"os\": `default`, \"uri\": join('',[`https://ms.portal.azure.com/#@microsoft.onmicrosoft.com/resource`, resource.id, `/overview`]) } ] } ] }","title":"Teamsnewresourcegroup general notifications"},{"location":"azure/examples/teamsnewresourcegroup-general-notifications/#resource-group-generate-a-teams-message-on-create-azure_examples_teams_new_resource_group","text":"This policy will send a notification to a Microsoft Teams channel when a new resource group is created in the subscription. There is a few minute delay between the resource group being created and the notification appearing. In order to target the correct Teams channel, you will need to insert the custom incoming webhook for the channel. If you do not have a webhook set up, please see Setting up a custom incoming webhook . We are using an actionable message card to provide a link to the new resource group. Note: linking to portal pages within Azure is not supported by all web browsers. To run the policy, you must replace <your_webhook_here> with the correct url. For more information on how the event grid function works please see azure_functionshosting {.interpreted-text role=\"ref\"} policies: - name: notify-new-resource-group description: | Generates a Teams notification when a new resource group is created resource: azure.resourcegroup mode: type: azure-event-grid events: [{ resourceProvider: Microsoft.Resources/subscriptions/resourceGroups, event: write }] actions: - type: webhook url: <your_webhook_here> batch: false body: > { \"@context\": `https://schema.org/extensions`, \"@type\": `MessageCard`, \"themeColor\": `0072C6`, \"title\": `New Resource Group Created`, \"text\": join('', [`A new resource group has been created in subscription `, account_id, `.\\n\\nResource Group Name: `, resource.name, `\\n\\nResource Group Location: `, resource.location]) \"potentialAction\": [ { \"@type\": `OpenUri`, \"name\": `Open In Portal`, \"targets\": [ { \"os\": `default`, \"uri\": join('',[`https://ms.portal.azure.com/#@microsoft.onmicrosoft.com/resource`, resource.id, `/overview`]) } ] } ] }","title":"Resource Group - Generate a Teams Message on Create {#azure_examples_teams_new_resource_group}"},{"location":"azure/examples/virtualmachinesstoppedvm-compute/","text":"Virtual Machines - Find Stopped Virtual Machines Filter to select all virtual machines that are not running: policies: - name: stopped-vm resource: azure.vm filters: - type: instance-view key: statuses[].code op: not-in value_type: swap value: \"PowerState/running\"","title":"Virtualmachinesstoppedvm compute"},{"location":"azure/examples/virtualmachinesstoppedvm-compute/#virtual-machines-find-stopped-virtual-machines","text":"Filter to select all virtual machines that are not running: policies: - name: stopped-vm resource: azure.vm filters: - type: instance-view key: statuses[].code op: not-in value_type: swap value: \"PowerState/running\"","title":"Virtual Machines - Find Stopped Virtual Machines"},{"location":"azure/examples/virtualmachineswithpublicip-compute-networking/","text":"Virtual Machines - Find Virtual Machines with public IP address {#azure_examples_vm_with_public_ips} Filter to select all virtual machines with a public ip address policies: - name: vms-with-public-ip resource: azure.vm filters: - type: network-interface key: 'properties.ipConfigurations[].properties.publicIPAddress.id' value: not-null","title":"Virtualmachineswithpublicip compute networking"},{"location":"azure/examples/virtualmachineswithpublicip-compute-networking/#virtual-machines-find-virtual-machines-with-public-ip-address-azure_examples_vm_with_public_ips","text":"Filter to select all virtual machines with a public ip address policies: - name: vms-with-public-ip resource: azure.vm filters: - type: network-interface key: 'properties.ipConfigurations[].properties.publicIPAddress.id' value: not-null","title":"Virtual Machines - Find Virtual Machines with public IP address {#azure_examples_vm_with_public_ips}"},{"location":"azure/examples/logicappnotifications/logicappnotification/","text":"Email - Use Azure Logic Apps to notify users of policy violations {#azure_examples_notifications_logic_app} Azure Logic Apps are a great option to create simple scalable workflows in Azure, such as sending an email or posting to Microsoft Teams. In this example, we will create a workflow that is invoked by Custodian\\'s logic-app <Azure.common.actions.logic-app> {.interpreted-text role=\"ref\"} action. Create and configure Azure Logic App We will now walk through the steps to configure a Logic Apps workflow that will consist of an [Request]{.title-ref} trigger and an [Outlook send email]{.title-ref} action. Create a new Azure Logic App From the main Azure menu, choose Create a resource \u2500\u25b6 Integration \u2500\u25b6 Logic App. Configure your logic app as shown below. For this example, let\\'s name the Logic App custodian-notifications . After you\\'re done, hit Create . After the Logic App is created, go to the resource and select Blank Logic App under Templates . Create Request trigger Under the Built-in tab, select Request . Select When a HTTP request is received . This will setup a HTTP trigger for your Logic App. Hitting the Save button will generate the URL that will be used to invoke your Logic App. Create the Outlook Send Email action Click the + New Step to create an action to send an email. In the search box, search for Send an email (V2) . Under the Actions tab, select the Office 365 Outlook action. Sign into an Office 365 account to create a connection for your Logic App. This account will be used to send emails once the workflow is invoked. Next, customize the email template that will be sent. Go to into the Code View mode by hitting the Code View Button . Inside the definition property, update the actions property to look like the following: \"actions\": { \"Send_an_email_(V2)\": { \"inputs\": { \"body\": { \"Body\": \"<p><span style=\\\"font-size: 16px\\\"><strong>Policy Name: </strong></span>@{triggerBody()['PolicyName']}<br>\\n<span style=\\\"font-size: 16px\\\"><strong>Policy Description:</strong></span><strong> </strong>@{triggerBody()['PolicyDescription']}<br>\\n<strong><br>\\n</strong><span style=\\\"font-size: 16px\\\"><strong>Resource</strong></span><strong><br>\\n--------------<br>\\nName: </strong>@{triggerBody()['Resource']['Name']}<br>\\n<strong>Location: </strong>@{triggerBody()['Resource']['Location']}<br>\\n<strong>Owner: </strong>@{triggerBody()['Resource']['Owner']}<br>\\n<strong>VmSize: </strong>@{triggerBody()['Resource']['VmSize']}<br>\\n<br>\\n</p>\", \"Subject\": \"Cloud Custodian Policy: @{triggerBody()['PolicyName']}\", \"To\": \"@{triggerBody()['Resource']['Owner']}\" }, \"host\": { \"connection\": { \"name\": \"@parameters('$connections')['office365']['connectionId']\" } }, \"method\": \"post\", \"path\": \"/v2/Mail\" }, \"runAfter\": {}, \"type\": \"ApiConnection\" } } Return back to the Designer mode. The template now appears in the Send an email (V2) action. Save the workflow. We now have a Logic App workflow that can be invoked via a HTTP request to send an email with an Office 365 account. Author Cloud Custodian policy Let\\'s start with an existing policy such as the Find Virtual Machines with Public IP address <azure_examples_vm_with_public_ips> {.interpreted-text role=\"ref\"} policy. This policy will find all Azure Virtual Machines that have public IPs attached to them. policies: - name: find-vms-with-public-ips description: | VMs should not have public-ips attached to them. resource: azure.vm filters: - type: network-interface key: properties.ipConfigurations[].properties.publicIPAddress.id value: not-null Add Logic App action Add a logic-app <Azure.common.actions.logic-app> {.interpreted-text role=\"ref\"} action to the policy to invoke the newly created Logic App. This action expects a CreatorEmail tag to be added on each resource. This can be done by Custodian using the auto-tag-user <azure_examples_autotagusers> {.interpreted-text role=\"ref\"} policy. actions: - type: logic-app resource-group: cloud-custodian # \u2500\u25b6 This is the resource group where you created your Logic App logic-app-name: custodian-notifications # \u2500\u25b6 This is the name of your Logic App batch: false # \u2500\u25b6 We want to invoke the Logic App for each resource that violates our policy body: > # \u2500\u25b6 We will select specific properties of our resource that can be used in our Logic App { PolicyName: policy.name, PolicyDescription: policy.description, Resource: resource. { Name: name, Location: location, Owner: tags.CreatorEmail, # \u2500\u25b6 The CreatorEmail tag on the resource will be recipient of the email. VmSize: properties.hardwareProfile.vmSize } } Final updated policy policies: - name: find-vms-with-public-ips description: | VMs should not have public-ips attached to them. resource: azure.vm filters: - type: network-interface key: properties.ipConfigurations[].properties.publicIPAddress.id value: not-null actions: - type: logic-app resource-group: cloud-custodian logic-app-name: custodian-notifications batch: false body: > { PolicyName: policy.name, PolicyDescription: policy.description, Resource: resource. { Name: name, Location: location, Owner: tags.CreatorEmail, VmSize: properties.hardwareProfile.vmSize } } Test the policy Policy violating virtual machine Find a Virtual Machine with a Public IP Address , which is a violation of the find-vms-with-public-ips <azure_examples_vm_with_public_ips> {.interpreted-text role=\"ref\"} policy. Next, either manually add a CreatorEmail tag to it or execute the auto-tag-user <azure_examples_autotagusers> {.interpreted-text role=\"ref\"} policy targeting [Virtual Machine]{.title-ref} resources. Email sent from logic app When we execute the policy above, it will invoke the logic-app action, which will result in the following email to be sent:","title":"Logicappnotification"},{"location":"azure/examples/logicappnotifications/logicappnotification/#email-use-azure-logic-apps-to-notify-users-of-policy-violations-azure_examples_notifications_logic_app","text":"Azure Logic Apps are a great option to create simple scalable workflows in Azure, such as sending an email or posting to Microsoft Teams. In this example, we will create a workflow that is invoked by Custodian\\'s logic-app <Azure.common.actions.logic-app> {.interpreted-text role=\"ref\"} action.","title":"Email - Use Azure Logic Apps to notify users of policy violations {#azure_examples_notifications_logic_app}"},{"location":"azure/examples/logicappnotifications/logicappnotification/#create-and-configure-azure-logic-app","text":"We will now walk through the steps to configure a Logic Apps workflow that will consist of an [Request]{.title-ref} trigger and an [Outlook send email]{.title-ref} action.","title":"Create and configure Azure Logic App"},{"location":"azure/examples/logicappnotifications/logicappnotification/#create-a-new-azure-logic-app","text":"From the main Azure menu, choose Create a resource \u2500\u25b6 Integration \u2500\u25b6 Logic App. Configure your logic app as shown below. For this example, let\\'s name the Logic App custodian-notifications . After you\\'re done, hit Create . After the Logic App is created, go to the resource and select Blank Logic App under Templates .","title":"Create a new Azure Logic App"},{"location":"azure/examples/logicappnotifications/logicappnotification/#create-request-trigger","text":"Under the Built-in tab, select Request . Select When a HTTP request is received . This will setup a HTTP trigger for your Logic App. Hitting the Save button will generate the URL that will be used to invoke your Logic App.","title":"Create Request trigger"},{"location":"azure/examples/logicappnotifications/logicappnotification/#create-the-outlook-send-email-action","text":"Click the + New Step to create an action to send an email. In the search box, search for Send an email (V2) . Under the Actions tab, select the Office 365 Outlook action. Sign into an Office 365 account to create a connection for your Logic App. This account will be used to send emails once the workflow is invoked. Next, customize the email template that will be sent. Go to into the Code View mode by hitting the Code View Button . Inside the definition property, update the actions property to look like the following: \"actions\": { \"Send_an_email_(V2)\": { \"inputs\": { \"body\": { \"Body\": \"<p><span style=\\\"font-size: 16px\\\"><strong>Policy Name: </strong></span>@{triggerBody()['PolicyName']}<br>\\n<span style=\\\"font-size: 16px\\\"><strong>Policy Description:</strong></span><strong> </strong>@{triggerBody()['PolicyDescription']}<br>\\n<strong><br>\\n</strong><span style=\\\"font-size: 16px\\\"><strong>Resource</strong></span><strong><br>\\n--------------<br>\\nName: </strong>@{triggerBody()['Resource']['Name']}<br>\\n<strong>Location: </strong>@{triggerBody()['Resource']['Location']}<br>\\n<strong>Owner: </strong>@{triggerBody()['Resource']['Owner']}<br>\\n<strong>VmSize: </strong>@{triggerBody()['Resource']['VmSize']}<br>\\n<br>\\n</p>\", \"Subject\": \"Cloud Custodian Policy: @{triggerBody()['PolicyName']}\", \"To\": \"@{triggerBody()['Resource']['Owner']}\" }, \"host\": { \"connection\": { \"name\": \"@parameters('$connections')['office365']['connectionId']\" } }, \"method\": \"post\", \"path\": \"/v2/Mail\" }, \"runAfter\": {}, \"type\": \"ApiConnection\" } } Return back to the Designer mode. The template now appears in the Send an email (V2) action. Save the workflow. We now have a Logic App workflow that can be invoked via a HTTP request to send an email with an Office 365 account.","title":"Create the Outlook Send Email action"},{"location":"azure/examples/logicappnotifications/logicappnotification/#author-cloud-custodian-policy","text":"Let\\'s start with an existing policy such as the Find Virtual Machines with Public IP address <azure_examples_vm_with_public_ips> {.interpreted-text role=\"ref\"} policy. This policy will find all Azure Virtual Machines that have public IPs attached to them. policies: - name: find-vms-with-public-ips description: | VMs should not have public-ips attached to them. resource: azure.vm filters: - type: network-interface key: properties.ipConfigurations[].properties.publicIPAddress.id value: not-null","title":"Author Cloud Custodian policy"},{"location":"azure/examples/logicappnotifications/logicappnotification/#add-logic-app-action","text":"Add a logic-app <Azure.common.actions.logic-app> {.interpreted-text role=\"ref\"} action to the policy to invoke the newly created Logic App. This action expects a CreatorEmail tag to be added on each resource. This can be done by Custodian using the auto-tag-user <azure_examples_autotagusers> {.interpreted-text role=\"ref\"} policy. actions: - type: logic-app resource-group: cloud-custodian # \u2500\u25b6 This is the resource group where you created your Logic App logic-app-name: custodian-notifications # \u2500\u25b6 This is the name of your Logic App batch: false # \u2500\u25b6 We want to invoke the Logic App for each resource that violates our policy body: > # \u2500\u25b6 We will select specific properties of our resource that can be used in our Logic App { PolicyName: policy.name, PolicyDescription: policy.description, Resource: resource. { Name: name, Location: location, Owner: tags.CreatorEmail, # \u2500\u25b6 The CreatorEmail tag on the resource will be recipient of the email. VmSize: properties.hardwareProfile.vmSize } }","title":"Add Logic App action"},{"location":"azure/examples/logicappnotifications/logicappnotification/#final-updated-policy","text":"policies: - name: find-vms-with-public-ips description: | VMs should not have public-ips attached to them. resource: azure.vm filters: - type: network-interface key: properties.ipConfigurations[].properties.publicIPAddress.id value: not-null actions: - type: logic-app resource-group: cloud-custodian logic-app-name: custodian-notifications batch: false body: > { PolicyName: policy.name, PolicyDescription: policy.description, Resource: resource. { Name: name, Location: location, Owner: tags.CreatorEmail, VmSize: properties.hardwareProfile.vmSize } }","title":"Final updated policy"},{"location":"azure/examples/logicappnotifications/logicappnotification/#test-the-policy","text":"","title":"Test the policy"},{"location":"azure/examples/logicappnotifications/logicappnotification/#policy-violating-virtual-machine","text":"Find a Virtual Machine with a Public IP Address , which is a violation of the find-vms-with-public-ips <azure_examples_vm_with_public_ips> {.interpreted-text role=\"ref\"} policy. Next, either manually add a CreatorEmail tag to it or execute the auto-tag-user <azure_examples_autotagusers> {.interpreted-text role=\"ref\"} policy targeting [Virtual Machine]{.title-ref} resources.","title":"Policy violating virtual machine"},{"location":"azure/examples/logicappnotifications/logicappnotification/#email-sent-from-logic-app","text":"When we execute the policy above, it will invoke the logic-app action, which will result in the following email to be sent:","title":"Email sent from logic app"},{"location":"developer/","text":"Developer Guide {#developer} This section is for developers who are contributing to custodian. developer-installing {.interpreted-text role=\"ref\"} developer-tests {.interpreted-text role=\"ref\"} developer-packaging {.interpreted-text role=\"ref\"} developer-documentation {.interpreted-text role=\"ref\"}","title":"Index"},{"location":"developer/#developer-guide-developer","text":"This section is for developers who are contributing to custodian. developer-installing {.interpreted-text role=\"ref\"} developer-tests {.interpreted-text role=\"ref\"} developer-packaging {.interpreted-text role=\"ref\"} developer-documentation {.interpreted-text role=\"ref\"}","title":"Developer Guide {#developer}"},{"location":"developer/documentation/","text":"Documentation For Developers {#developer-documentation} Cloud Custodian makes every effort to provide comprehensive documentation. Any new features you add should be documented. The documentation is built using sphinx . The documentation is written using reStructured Text ( rst ). The sphinx documentation contains a useful introduction to rst syntax. Find the Documentation The root of the documentation is located in the docs directory. Within the documentation, topics are organized according to the following main areas: Overview <../index> {.interpreted-text role=\"doc\"} Quickstart <quickstart> {.interpreted-text role=\"ref\"} AWS <aws-gettingstarted> {.interpreted-text role=\"ref\"} Azure <azure_gettingstarted> {.interpreted-text role=\"ref\"} GCP <gcp_gettingstarted> {.interpreted-text role=\"ref\"} Developer <developer> {.interpreted-text role=\"ref\"} In addition, the api documentation will be built from docstrings on classes and methods in source code. The rst files for these may be found in the generated subdirectory. Edit the Documentation In most cases, documentation edits will be made in docstrings on source code. Docstrings should be written following the principles of pep 257 . Within docstrings, rst directives allow for highlighting code examples: class AutoTagUser(EventAction): \"\"\"Tag a resource with the user who created/modified it. .. code-block:: yaml policies: - name: ec2-auto-tag-ownercontact resource: ec2 description: | Triggered when a new EC2 Instance is launched. Checks to see if it's missing the OwnerContact tag. If missing it gets created with the value of the ID of whomever called the RunInstances API mode: type: cloudtrail role: arn:aws:iam::123456789000:role/custodian-auto-tagger events: - RunInstances filters: - tag:OwnerContact: absent actions: - type: auto-tag-user tag: OwnerContact There's a number of caveats to usage. Resources which don't include tagging as part of their api may have some delay before automation kicks in to create a tag. Real world delay may be several minutes, with worst case into hours[0]. This creates a race condition between auto tagging and automation. In practice this window is on the order of a fraction of a second, as we fetch the resource and evaluate the presence of the tag before attempting to tag it. References CloudTrail User https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-event-reference-user-identity.html \"\"\" Render the Documentation In general, you should use tox to build the documentation: tox -e docs This command will clean previously built files and rebuild the entire documentation tree. When developing, you may prefer to build only those files you have edited. To do so, use the following command: make -f docs/Makefile.sphinx html You can also build documentation via the provided tox dockerfile. You will need to build and run from the root of your source enlistment each time you edit documentation files: docker build -t tox_linux --build-arg TOX_ENV=docs . -f tools/dev/docker_tox_linux/Dockerfile docker run -v 'pwd'/docs/build:/src/docs/build -it tox_linux","title":"Documentation"},{"location":"developer/documentation/#documentation-for-developers-developer-documentation","text":"Cloud Custodian makes every effort to provide comprehensive documentation. Any new features you add should be documented. The documentation is built using sphinx . The documentation is written using reStructured Text ( rst ). The sphinx documentation contains a useful introduction to rst syntax.","title":"Documentation For Developers {#developer-documentation}"},{"location":"developer/documentation/#find-the-documentation","text":"The root of the documentation is located in the docs directory. Within the documentation, topics are organized according to the following main areas: Overview <../index> {.interpreted-text role=\"doc\"} Quickstart <quickstart> {.interpreted-text role=\"ref\"} AWS <aws-gettingstarted> {.interpreted-text role=\"ref\"} Azure <azure_gettingstarted> {.interpreted-text role=\"ref\"} GCP <gcp_gettingstarted> {.interpreted-text role=\"ref\"} Developer <developer> {.interpreted-text role=\"ref\"} In addition, the api documentation will be built from docstrings on classes and methods in source code. The rst files for these may be found in the generated subdirectory.","title":"Find the Documentation"},{"location":"developer/documentation/#edit-the-documentation","text":"In most cases, documentation edits will be made in docstrings on source code. Docstrings should be written following the principles of pep 257 . Within docstrings, rst directives allow for highlighting code examples: class AutoTagUser(EventAction): \"\"\"Tag a resource with the user who created/modified it. .. code-block:: yaml policies: - name: ec2-auto-tag-ownercontact resource: ec2 description: | Triggered when a new EC2 Instance is launched. Checks to see if it's missing the OwnerContact tag. If missing it gets created with the value of the ID of whomever called the RunInstances API mode: type: cloudtrail role: arn:aws:iam::123456789000:role/custodian-auto-tagger events: - RunInstances filters: - tag:OwnerContact: absent actions: - type: auto-tag-user tag: OwnerContact There's a number of caveats to usage. Resources which don't include tagging as part of their api may have some delay before automation kicks in to create a tag. Real world delay may be several minutes, with worst case into hours[0]. This creates a race condition between auto tagging and automation. In practice this window is on the order of a fraction of a second, as we fetch the resource and evaluate the presence of the tag before attempting to tag it. References CloudTrail User https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-event-reference-user-identity.html \"\"\"","title":"Edit the Documentation"},{"location":"developer/documentation/#render-the-documentation","text":"In general, you should use tox to build the documentation: tox -e docs This command will clean previously built files and rebuild the entire documentation tree. When developing, you may prefer to build only those files you have edited. To do so, use the following command: make -f docs/Makefile.sphinx html You can also build documentation via the provided tox dockerfile. You will need to build and run from the root of your source enlistment each time you edit documentation files: docker build -t tox_linux --build-arg TOX_ENV=docs . -f tools/dev/docker_tox_linux/Dockerfile docker run -v 'pwd'/docs/build:/src/docs/build -it tox_linux","title":"Render the Documentation"},{"location":"developer/documentation.rst/","text":"Documentation For Developers {#developer-documentation} Cloud Custodian makes every effort to provide comprehensive documentation. Any new features you add should be documented. The documentation is built using sphinx . The documentation is written using reStructured Text ( rst ). The sphinx documentation contains a useful introduction to rst syntax. Find the Documentation The root of the documentation is located in the docs directory. Within the documentation, topics are organized according to the following main areas: Overview <../index> {.interpreted-text role=\"doc\"} Quickstart <quickstart> {.interpreted-text role=\"ref\"} AWS <aws-gettingstarted> {.interpreted-text role=\"ref\"} Azure <azure_gettingstarted> {.interpreted-text role=\"ref\"} GCP <gcp_gettingstarted> {.interpreted-text role=\"ref\"} Developer <developer> {.interpreted-text role=\"ref\"} In addition, the api documentation will be built from docstrings on classes and methods in source code. The rst files for these may be found in the generated subdirectory. Edit the Documentation In most cases, documentation edits will be made in docstrings on source code. Docstrings should be written following the principles of pep 257 . Within docstrings, rst directives allow for highlighting code examples: class AutoTagUser(EventAction): \"\"\"Tag a resource with the user who created/modified it. .. code-block:: yaml policies: - name: ec2-auto-tag-ownercontact resource: ec2 description: | Triggered when a new EC2 Instance is launched. Checks to see if it's missing the OwnerContact tag. If missing it gets created with the value of the ID of whomever called the RunInstances API mode: type: cloudtrail role: arn:aws:iam::123456789000:role/custodian-auto-tagger events: - RunInstances filters: - tag:OwnerContact: absent actions: - type: auto-tag-user tag: OwnerContact There's a number of caveats to usage. Resources which don't include tagging as part of their api may have some delay before automation kicks in to create a tag. Real world delay may be several minutes, with worst case into hours[0]. This creates a race condition between auto tagging and automation. In practice this window is on the order of a fraction of a second, as we fetch the resource and evaluate the presence of the tag before attempting to tag it. References CloudTrail User https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-event-reference-user-identity.html \"\"\" Render the Documentation In general, you should use tox to build the documentation: tox -e docs This command will clean previously built files and rebuild the entire documentation tree. When developing, you may prefer to build only those files you have edited. To do so, use the following command: make -f docs/Makefile.sphinx html You can also build documentation via the provided tox dockerfile. You will need to build and run from the root of your source enlistment each time you edit documentation files: docker build -t tox_linux --build-arg TOX_ENV=docs . -f tools/dev/docker_tox_linux/Dockerfile docker run -v 'pwd'/docs/build:/src/docs/build -it tox_linux","title":"Documentation.rst"},{"location":"developer/documentation.rst/#documentation-for-developers-developer-documentation","text":"Cloud Custodian makes every effort to provide comprehensive documentation. Any new features you add should be documented. The documentation is built using sphinx . The documentation is written using reStructured Text ( rst ). The sphinx documentation contains a useful introduction to rst syntax.","title":"Documentation For Developers {#developer-documentation}"},{"location":"developer/documentation.rst/#find-the-documentation","text":"The root of the documentation is located in the docs directory. Within the documentation, topics are organized according to the following main areas: Overview <../index> {.interpreted-text role=\"doc\"} Quickstart <quickstart> {.interpreted-text role=\"ref\"} AWS <aws-gettingstarted> {.interpreted-text role=\"ref\"} Azure <azure_gettingstarted> {.interpreted-text role=\"ref\"} GCP <gcp_gettingstarted> {.interpreted-text role=\"ref\"} Developer <developer> {.interpreted-text role=\"ref\"} In addition, the api documentation will be built from docstrings on classes and methods in source code. The rst files for these may be found in the generated subdirectory.","title":"Find the Documentation"},{"location":"developer/documentation.rst/#edit-the-documentation","text":"In most cases, documentation edits will be made in docstrings on source code. Docstrings should be written following the principles of pep 257 . Within docstrings, rst directives allow for highlighting code examples: class AutoTagUser(EventAction): \"\"\"Tag a resource with the user who created/modified it. .. code-block:: yaml policies: - name: ec2-auto-tag-ownercontact resource: ec2 description: | Triggered when a new EC2 Instance is launched. Checks to see if it's missing the OwnerContact tag. If missing it gets created with the value of the ID of whomever called the RunInstances API mode: type: cloudtrail role: arn:aws:iam::123456789000:role/custodian-auto-tagger events: - RunInstances filters: - tag:OwnerContact: absent actions: - type: auto-tag-user tag: OwnerContact There's a number of caveats to usage. Resources which don't include tagging as part of their api may have some delay before automation kicks in to create a tag. Real world delay may be several minutes, with worst case into hours[0]. This creates a race condition between auto tagging and automation. In practice this window is on the order of a fraction of a second, as we fetch the resource and evaluate the presence of the tag before attempting to tag it. References CloudTrail User https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-event-reference-user-identity.html \"\"\"","title":"Edit the Documentation"},{"location":"developer/documentation.rst/#render-the-documentation","text":"In general, you should use tox to build the documentation: tox -e docs This command will clean previously built files and rebuild the entire documentation tree. When developing, you may prefer to build only those files you have edited. To do so, use the following command: make -f docs/Makefile.sphinx html You can also build documentation via the provided tox dockerfile. You will need to build and run from the root of your source enlistment each time you edit documentation files: docker build -t tox_linux --build-arg TOX_ENV=docs . -f tools/dev/docker_tox_linux/Dockerfile docker run -v 'pwd'/docs/build:/src/docs/build -it tox_linux","title":"Render the Documentation"},{"location":"developer/index.rst/","text":"Developer Guide {#developer} This section is for developers who are contributing to custodian. developer-installing {.interpreted-text role=\"ref\"} developer-tests {.interpreted-text role=\"ref\"} developer-packaging {.interpreted-text role=\"ref\"} developer-documentation {.interpreted-text role=\"ref\"}","title":"Index.rst"},{"location":"developer/index.rst/#developer-guide-developer","text":"This section is for developers who are contributing to custodian. developer-installing {.interpreted-text role=\"ref\"} developer-tests {.interpreted-text role=\"ref\"} developer-packaging {.interpreted-text role=\"ref\"} developer-documentation {.interpreted-text role=\"ref\"}","title":"Developer Guide {#developer}"},{"location":"developer/installing/","text":"Installing for Developers {#developer-installing} Installing Prerequisites Cloud Custodian supports Python 3.6, 3.7, 3.8 and above. To develop the Custodian, you will need to have a make/C toolchain, Python3 and some basic Python tools. Install Python 3 You\\'ll need to have a Python 3 environment set up. You may have a preferred way of doing this. Here are instructions for a way to do it on Ubuntu and Mac OS X. On Ubuntu On most recent versions of Ubuntu, Python 3 is included by default. To get Python 3.8, first add the deadsnakes package repository: sudo add-apt-repository ppa:deadsnakes/ppa Next, install python3.8 and the development headers for it: sudo apt-get install python3.8 python3.8-dev Then, install pip : sudo apt-get install python3-pip When this is complete you should be able to check that you have pip properly installed: python3.8 -m pip --version pip 9.0.1 from /usr/lib/python3/dist-packages (python 3.8) (your exact version numbers will likely differ) On macOS with Homebrew brew install python3 Installing python3 will get you the latest version of Python 3 supported by Homebrew, currently Python 3.7. Basic Python Tools Once your Python installation is squared away, you will need to install tox : python3.7 -m pip install -U pip tox (note that we also updated pip in order to get the latest version) Installing Custodian First, clone the repository: git clone https://github.com/cloud-custodian/cloud-custodian.git cd cloud-custodian ::: {.note} ::: {.title} Note ::: If you have the intention to contribute to Cloud Custodian, it\\'s better to make a fork of the Cloud-Custodian repository first, and work inside your fork, so that you can push changes to your fork and make a pull request from there. Make the fork from the Github UI, then clone your fork instead of the main repository. git clone https://github.com/<your github account>/cloud-custodian.git To keep track of the changes to the original cloud-custodian repository, add a remote upstream repository in your fork: git remote add upstream https://github.com/cloud-custodian/cloud-custodian.git Then, to get the upstream changes and merge them into your fork: git fetch upstream git merge upstream/master ::: Now that the repository is set up, build the software with tox : tox Tox creates a sandboxed \\\"virtual environment\\\" (\\\"venv\\\") for each Python version, 3.6, 3.7, 3.8 These are stored in the .tox/ directory. It then runs the test suite under all versions of Python, per the tox.ini file. If tox is unable to find a Python executable on your system for one of the supported versions, it will fail for that environment. You can safely ignore these failures when developing locally. You can run the test suite in a single environment with the -e flag: tox -e py38 To access the executables installed in one or the other virtual environment, source the venv into your current shell, e.g.: source .tox/py37/bin/activate You should then have, e.g., the custodian command available: (py37)$ custodian -h You\\'ll also be able to invoke pytest directly with the arguments of your choosing, e.g.: (py37) $ pytest tests/test_s3.py -x -k replication Note that you\\'ll have to set up environment variables appropriately per the tox.ini for provider credentials. See below for the best way to do that. Installing in Your Own Virtual Environment Running directly from a tox sandbox, while very easy to set up, might not be the most comfortable way of working. You might want to create your own virtual environment and use that for running Custodian. This can be done using the venv module. It can be done right inside the cloned Cloud Custodian repository: python3 -m venv . The above command assumes the current directory is the Cloud Custodian checkout. Next, you\\'ll need to install all the development dependencies. Cloud Custodian uses poetry for packaging and dependency management. Poetry uses a custom installer, to be fully isolated from the rest of your system. For osx and linux, poetry recommends running this for installing: curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python - For windows powershell use this command: (Invoke-WebRequest -Uri https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py -UseBasicParsing).Content | python - Once poetry is installed, you can set up Cloud Custodian using the included Makefile: source bin/activate (cloud-custodian) $ make install-poetry ::: {.note} ::: {.title} Note ::: It\\'s important to activate the venv before running the installer, or poetry will create a venv for each dependency folder included in the Cloud Custodian repository. ::: Once this is done, poetry can be used to run the tests as well: (cloud-custodian) $ make test-poetry You could also use pytest to run the tests, but you will need to set up some environment variables to successfully run the full test suite. The best way to do that is to edit the test.env file in the root of the repository and \\\"source\\\" it, using the shell: source test.env In general, it\\'s best to use tox to run the full test suite, and use pytest to run specific tests that you are working on.","title":"Installing"},{"location":"developer/installing/#installing-for-developers-developer-installing","text":"","title":"Installing for Developers {#developer-installing}"},{"location":"developer/installing/#installing-prerequisites","text":"Cloud Custodian supports Python 3.6, 3.7, 3.8 and above. To develop the Custodian, you will need to have a make/C toolchain, Python3 and some basic Python tools.","title":"Installing Prerequisites"},{"location":"developer/installing/#install-python-3","text":"You\\'ll need to have a Python 3 environment set up. You may have a preferred way of doing this. Here are instructions for a way to do it on Ubuntu and Mac OS X.","title":"Install Python 3"},{"location":"developer/installing/#on-ubuntu","text":"On most recent versions of Ubuntu, Python 3 is included by default. To get Python 3.8, first add the deadsnakes package repository: sudo add-apt-repository ppa:deadsnakes/ppa Next, install python3.8 and the development headers for it: sudo apt-get install python3.8 python3.8-dev Then, install pip : sudo apt-get install python3-pip When this is complete you should be able to check that you have pip properly installed: python3.8 -m pip --version pip 9.0.1 from /usr/lib/python3/dist-packages (python 3.8) (your exact version numbers will likely differ)","title":"On Ubuntu"},{"location":"developer/installing/#on-macos-with-homebrew","text":"brew install python3 Installing python3 will get you the latest version of Python 3 supported by Homebrew, currently Python 3.7.","title":"On macOS with Homebrew"},{"location":"developer/installing/#basic-python-tools","text":"Once your Python installation is squared away, you will need to install tox : python3.7 -m pip install -U pip tox (note that we also updated pip in order to get the latest version)","title":"Basic Python Tools"},{"location":"developer/installing/#installing-custodian","text":"First, clone the repository: git clone https://github.com/cloud-custodian/cloud-custodian.git cd cloud-custodian ::: {.note} ::: {.title} Note ::: If you have the intention to contribute to Cloud Custodian, it\\'s better to make a fork of the Cloud-Custodian repository first, and work inside your fork, so that you can push changes to your fork and make a pull request from there. Make the fork from the Github UI, then clone your fork instead of the main repository. git clone https://github.com/<your github account>/cloud-custodian.git To keep track of the changes to the original cloud-custodian repository, add a remote upstream repository in your fork: git remote add upstream https://github.com/cloud-custodian/cloud-custodian.git Then, to get the upstream changes and merge them into your fork: git fetch upstream git merge upstream/master ::: Now that the repository is set up, build the software with tox : tox Tox creates a sandboxed \\\"virtual environment\\\" (\\\"venv\\\") for each Python version, 3.6, 3.7, 3.8 These are stored in the .tox/ directory. It then runs the test suite under all versions of Python, per the tox.ini file. If tox is unable to find a Python executable on your system for one of the supported versions, it will fail for that environment. You can safely ignore these failures when developing locally. You can run the test suite in a single environment with the -e flag: tox -e py38 To access the executables installed in one or the other virtual environment, source the venv into your current shell, e.g.: source .tox/py37/bin/activate You should then have, e.g., the custodian command available: (py37)$ custodian -h You\\'ll also be able to invoke pytest directly with the arguments of your choosing, e.g.: (py37) $ pytest tests/test_s3.py -x -k replication Note that you\\'ll have to set up environment variables appropriately per the tox.ini for provider credentials. See below for the best way to do that.","title":"Installing Custodian"},{"location":"developer/installing/#installing-in-your-own-virtual-environment","text":"Running directly from a tox sandbox, while very easy to set up, might not be the most comfortable way of working. You might want to create your own virtual environment and use that for running Custodian. This can be done using the venv module. It can be done right inside the cloned Cloud Custodian repository: python3 -m venv . The above command assumes the current directory is the Cloud Custodian checkout. Next, you\\'ll need to install all the development dependencies. Cloud Custodian uses poetry for packaging and dependency management. Poetry uses a custom installer, to be fully isolated from the rest of your system. For osx and linux, poetry recommends running this for installing: curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python - For windows powershell use this command: (Invoke-WebRequest -Uri https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py -UseBasicParsing).Content | python - Once poetry is installed, you can set up Cloud Custodian using the included Makefile: source bin/activate (cloud-custodian) $ make install-poetry ::: {.note} ::: {.title} Note ::: It\\'s important to activate the venv before running the installer, or poetry will create a venv for each dependency folder included in the Cloud Custodian repository. ::: Once this is done, poetry can be used to run the tests as well: (cloud-custodian) $ make test-poetry You could also use pytest to run the tests, but you will need to set up some environment variables to successfully run the full test suite. The best way to do that is to edit the test.env file in the root of the repository and \\\"source\\\" it, using the shell: source test.env In general, it\\'s best to use tox to run the full test suite, and use pytest to run specific tests that you are working on.","title":"Installing in Your Own Virtual Environment"},{"location":"developer/installing.rst/","text":"Installing for Developers {#developer-installing} Installing Prerequisites Cloud Custodian supports Python 3.6, 3.7, 3.8 and above. To develop the Custodian, you will need to have a make/C toolchain, Python3 and some basic Python tools. Install Python 3 You\\'ll need to have a Python 3 environment set up. You may have a preferred way of doing this. Here are instructions for a way to do it on Ubuntu and Mac OS X. On Ubuntu On most recent versions of Ubuntu, Python 3 is included by default. To get Python 3.8, first add the deadsnakes package repository: sudo add-apt-repository ppa:deadsnakes/ppa Next, install python3.8 and the development headers for it: sudo apt-get install python3.8 python3.8-dev Then, install pip : sudo apt-get install python3-pip When this is complete you should be able to check that you have pip properly installed: python3.8 -m pip --version pip 9.0.1 from /usr/lib/python3/dist-packages (python 3.8) (your exact version numbers will likely differ) On macOS with Homebrew brew install python3 Installing python3 will get you the latest version of Python 3 supported by Homebrew, currently Python 3.7. Basic Python Tools Once your Python installation is squared away, you will need to install tox : python3.7 -m pip install -U pip tox (note that we also updated pip in order to get the latest version) Installing Custodian First, clone the repository: git clone https://github.com/cloud-custodian/cloud-custodian.git cd cloud-custodian ::: {.note} ::: {.title} Note ::: If you have the intention to contribute to Cloud Custodian, it\\'s better to make a fork of the Cloud-Custodian repository first, and work inside your fork, so that you can push changes to your fork and make a pull request from there. Make the fork from the Github UI, then clone your fork instead of the main repository. git clone https://github.com/<your github account>/cloud-custodian.git To keep track of the changes to the original cloud-custodian repository, add a remote upstream repository in your fork: git remote add upstream https://github.com/cloud-custodian/cloud-custodian.git Then, to get the upstream changes and merge them into your fork: git fetch upstream git merge upstream/master ::: Now that the repository is set up, build the software with tox : tox Tox creates a sandboxed \\\"virtual environment\\\" (\\\"venv\\\") for each Python version, 3.6, 3.7, 3.8 These are stored in the .tox/ directory. It then runs the test suite under all versions of Python, per the tox.ini file. If tox is unable to find a Python executable on your system for one of the supported versions, it will fail for that environment. You can safely ignore these failures when developing locally. You can run the test suite in a single environment with the -e flag: tox -e py38 To access the executables installed in one or the other virtual environment, source the venv into your current shell, e.g.: source .tox/py37/bin/activate You should then have, e.g., the custodian command available: (py37)$ custodian -h You\\'ll also be able to invoke pytest directly with the arguments of your choosing, e.g.: (py37) $ pytest tests/test_s3.py -x -k replication Note that you\\'ll have to set up environment variables appropriately per the tox.ini for provider credentials. See below for the best way to do that. Installing in Your Own Virtual Environment Running directly from a tox sandbox, while very easy to set up, might not be the most comfortable way of working. You might want to create your own virtual environment and use that for running Custodian. This can be done using the venv module. It can be done right inside the cloned Cloud Custodian repository: python3 -m venv . The above command assumes the current directory is the Cloud Custodian checkout. Next, you\\'ll need to install all the development dependencies. Cloud Custodian uses poetry for packaging and dependency management. Poetry uses a custom installer, to be fully isolated from the rest of your system. For osx and linux, poetry recommends running this for installing: curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python - For windows powershell use this command: (Invoke-WebRequest -Uri https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py -UseBasicParsing).Content | python - Once poetry is installed, you can set up Cloud Custodian using the included Makefile: source bin/activate (cloud-custodian) $ make install-poetry ::: {.note} ::: {.title} Note ::: It\\'s important to activate the venv before running the installer, or poetry will create a venv for each dependency folder included in the Cloud Custodian repository. ::: Once this is done, poetry can be used to run the tests as well: (cloud-custodian) $ make test-poetry You could also use pytest to run the tests, but you will need to set up some environment variables to successfully run the full test suite. The best way to do that is to edit the test.env file in the root of the repository and \\\"source\\\" it, using the shell: source test.env In general, it\\'s best to use tox to run the full test suite, and use pytest to run specific tests that you are working on.","title":"Installing.rst"},{"location":"developer/installing.rst/#installing-for-developers-developer-installing","text":"","title":"Installing for Developers {#developer-installing}"},{"location":"developer/installing.rst/#installing-prerequisites","text":"Cloud Custodian supports Python 3.6, 3.7, 3.8 and above. To develop the Custodian, you will need to have a make/C toolchain, Python3 and some basic Python tools.","title":"Installing Prerequisites"},{"location":"developer/installing.rst/#install-python-3","text":"You\\'ll need to have a Python 3 environment set up. You may have a preferred way of doing this. Here are instructions for a way to do it on Ubuntu and Mac OS X.","title":"Install Python 3"},{"location":"developer/installing.rst/#on-ubuntu","text":"On most recent versions of Ubuntu, Python 3 is included by default. To get Python 3.8, first add the deadsnakes package repository: sudo add-apt-repository ppa:deadsnakes/ppa Next, install python3.8 and the development headers for it: sudo apt-get install python3.8 python3.8-dev Then, install pip : sudo apt-get install python3-pip When this is complete you should be able to check that you have pip properly installed: python3.8 -m pip --version pip 9.0.1 from /usr/lib/python3/dist-packages (python 3.8) (your exact version numbers will likely differ)","title":"On Ubuntu"},{"location":"developer/installing.rst/#on-macos-with-homebrew","text":"brew install python3 Installing python3 will get you the latest version of Python 3 supported by Homebrew, currently Python 3.7.","title":"On macOS with Homebrew"},{"location":"developer/installing.rst/#basic-python-tools","text":"Once your Python installation is squared away, you will need to install tox : python3.7 -m pip install -U pip tox (note that we also updated pip in order to get the latest version)","title":"Basic Python Tools"},{"location":"developer/installing.rst/#installing-custodian","text":"First, clone the repository: git clone https://github.com/cloud-custodian/cloud-custodian.git cd cloud-custodian ::: {.note} ::: {.title} Note ::: If you have the intention to contribute to Cloud Custodian, it\\'s better to make a fork of the Cloud-Custodian repository first, and work inside your fork, so that you can push changes to your fork and make a pull request from there. Make the fork from the Github UI, then clone your fork instead of the main repository. git clone https://github.com/<your github account>/cloud-custodian.git To keep track of the changes to the original cloud-custodian repository, add a remote upstream repository in your fork: git remote add upstream https://github.com/cloud-custodian/cloud-custodian.git Then, to get the upstream changes and merge them into your fork: git fetch upstream git merge upstream/master ::: Now that the repository is set up, build the software with tox : tox Tox creates a sandboxed \\\"virtual environment\\\" (\\\"venv\\\") for each Python version, 3.6, 3.7, 3.8 These are stored in the .tox/ directory. It then runs the test suite under all versions of Python, per the tox.ini file. If tox is unable to find a Python executable on your system for one of the supported versions, it will fail for that environment. You can safely ignore these failures when developing locally. You can run the test suite in a single environment with the -e flag: tox -e py38 To access the executables installed in one or the other virtual environment, source the venv into your current shell, e.g.: source .tox/py37/bin/activate You should then have, e.g., the custodian command available: (py37)$ custodian -h You\\'ll also be able to invoke pytest directly with the arguments of your choosing, e.g.: (py37) $ pytest tests/test_s3.py -x -k replication Note that you\\'ll have to set up environment variables appropriately per the tox.ini for provider credentials. See below for the best way to do that.","title":"Installing Custodian"},{"location":"developer/installing.rst/#installing-in-your-own-virtual-environment","text":"Running directly from a tox sandbox, while very easy to set up, might not be the most comfortable way of working. You might want to create your own virtual environment and use that for running Custodian. This can be done using the venv module. It can be done right inside the cloned Cloud Custodian repository: python3 -m venv . The above command assumes the current directory is the Cloud Custodian checkout. Next, you\\'ll need to install all the development dependencies. Cloud Custodian uses poetry for packaging and dependency management. Poetry uses a custom installer, to be fully isolated from the rest of your system. For osx and linux, poetry recommends running this for installing: curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python - For windows powershell use this command: (Invoke-WebRequest -Uri https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py -UseBasicParsing).Content | python - Once poetry is installed, you can set up Cloud Custodian using the included Makefile: source bin/activate (cloud-custodian) $ make install-poetry ::: {.note} ::: {.title} Note ::: It\\'s important to activate the venv before running the installer, or poetry will create a venv for each dependency folder included in the Cloud Custodian repository. ::: Once this is done, poetry can be used to run the tests as well: (cloud-custodian) $ make test-poetry You could also use pytest to run the tests, but you will need to set up some environment variables to successfully run the full test suite. The best way to do that is to edit the test.env file in the root of the repository and \\\"source\\\" it, using the shell: source test.env In general, it\\'s best to use tox to run the full test suite, and use pytest to run specific tests that you are working on.","title":"Installing in Your Own Virtual Environment"},{"location":"developer/packaging/","text":"Packaging Custodian {#developer-packaging} Custodian uses poetry https://python-poetry.org/ for managing dependencies and providing for repeatable installs. Its not typically required for developers as we maintain setuptools/pip/tox compatible environments, however familiarity is needed when making changes to the dependency graph (add/update/remove) dependencies, as all the setup.py/requirements files are generated artifacts. The reasoning around the move to poetry was that of needing better tooling to freeze the custodian dependency graph when publishing packages to pypi to ensure that releases would be repeatably installable at a future date in spite of changes to the underlying dependency graph, some perhaps not obeying semantic versioning principles. Additionally, with the growth of providers and other tools, we wanted better holistic management for release automation across the set of packages. After experimenting with a few tools in the ecosystem, including building our own, the maintainers settled on poetry as one that offered both a superior ux, was actively maintained, and had a reasonable python api for additional release management activities. Our additional tooling around poetry is to help automate management across the half-dozen custodian packages as well as to keep requirements and setup.py files in sync. We continue to use setuptools/pip in our CI infrastructure as it offers significant speed benefits [0]. To ensure the poetry install is exercised as part of CI, we do maintain the main docker image via poetry. Usage We maintain several makefile targets that can be used to front end poetry. [make install-poetry]{.title-ref} an alternative custodian installation method, assumes poetry is already installed. [make pkg-show-update]{.title-ref} show available updates to packages in poetry lockfiles. [make pkg-update]{.title-ref} attempts to update dependencies across the tree, should be followed by gen-requirements/gen-setup below. [make pkg-gen-requirements]{.title-ref} show available updates to packages in poetry lockfiles. [make pkg-gen-setup]{.title-ref} generate setup.py files from pyproject.toml this will carry over semver constraints. [make pkg-freeze-setup]{.title-ref} generate setup.py files from pyproject.toml with all dependencies frozen in setup.py. [make pkg-publish-wheel]{.title-ref} increments version, builds wheels, lints, and publishes build to testpypi via twine. The underlying script that provides additional poetry/packaging automation specific to custodian is in tools/dev/poetrypkg.py [0] poetry will call out to pip as a subprocess per package to control the exact versions installed, as pip does not have a public api. Caveats To maintain dependencies between packages within our repository, we specify all intra repo dependencies as dev dependencies with relative directory source paths. When when we generate setup.py files we do so sans any dev dependencies, which we resolve during generation to the latest version, frozen or semver compatible, per source directory development dependency. One interesting consequence of source directory dependencies in poetry is that they break any attempts to distribute/publish a package, even if they are [dev]{.title-ref} dependencies. This is because, per the pyproject.toml spec, poetry will be invoked during install by the build system. The invocation/installation of poetry as a build sys is transparently handled by pip, but simple resolution/parse of pyproject.toml dev dependencies will cause a poetry failure for a source distribution install, as installation of an sdist, is actually a wheel compilation. As a result of this publishing limitation we only publish wheels instead of sdists, which avoids the build system entirely, because a wheel is an extractable installation container/format file.","title":"Packaging"},{"location":"developer/packaging/#packaging-custodian-developer-packaging","text":"Custodian uses poetry https://python-poetry.org/ for managing dependencies and providing for repeatable installs. Its not typically required for developers as we maintain setuptools/pip/tox compatible environments, however familiarity is needed when making changes to the dependency graph (add/update/remove) dependencies, as all the setup.py/requirements files are generated artifacts. The reasoning around the move to poetry was that of needing better tooling to freeze the custodian dependency graph when publishing packages to pypi to ensure that releases would be repeatably installable at a future date in spite of changes to the underlying dependency graph, some perhaps not obeying semantic versioning principles. Additionally, with the growth of providers and other tools, we wanted better holistic management for release automation across the set of packages. After experimenting with a few tools in the ecosystem, including building our own, the maintainers settled on poetry as one that offered both a superior ux, was actively maintained, and had a reasonable python api for additional release management activities. Our additional tooling around poetry is to help automate management across the half-dozen custodian packages as well as to keep requirements and setup.py files in sync. We continue to use setuptools/pip in our CI infrastructure as it offers significant speed benefits [0]. To ensure the poetry install is exercised as part of CI, we do maintain the main docker image via poetry.","title":"Packaging Custodian {#developer-packaging}"},{"location":"developer/packaging/#usage","text":"We maintain several makefile targets that can be used to front end poetry. [make install-poetry]{.title-ref} an alternative custodian installation method, assumes poetry is already installed. [make pkg-show-update]{.title-ref} show available updates to packages in poetry lockfiles. [make pkg-update]{.title-ref} attempts to update dependencies across the tree, should be followed by gen-requirements/gen-setup below. [make pkg-gen-requirements]{.title-ref} show available updates to packages in poetry lockfiles. [make pkg-gen-setup]{.title-ref} generate setup.py files from pyproject.toml this will carry over semver constraints. [make pkg-freeze-setup]{.title-ref} generate setup.py files from pyproject.toml with all dependencies frozen in setup.py. [make pkg-publish-wheel]{.title-ref} increments version, builds wheels, lints, and publishes build to testpypi via twine. The underlying script that provides additional poetry/packaging automation specific to custodian is in tools/dev/poetrypkg.py [0] poetry will call out to pip as a subprocess per package to control the exact versions installed, as pip does not have a public api.","title":"Usage"},{"location":"developer/packaging/#caveats","text":"To maintain dependencies between packages within our repository, we specify all intra repo dependencies as dev dependencies with relative directory source paths. When when we generate setup.py files we do so sans any dev dependencies, which we resolve during generation to the latest version, frozen or semver compatible, per source directory development dependency. One interesting consequence of source directory dependencies in poetry is that they break any attempts to distribute/publish a package, even if they are [dev]{.title-ref} dependencies. This is because, per the pyproject.toml spec, poetry will be invoked during install by the build system. The invocation/installation of poetry as a build sys is transparently handled by pip, but simple resolution/parse of pyproject.toml dev dependencies will cause a poetry failure for a source distribution install, as installation of an sdist, is actually a wheel compilation. As a result of this publishing limitation we only publish wheels instead of sdists, which avoids the build system entirely, because a wheel is an extractable installation container/format file.","title":"Caveats"},{"location":"developer/packaging.rst/","text":"Packaging Custodian {#developer-packaging} Custodian uses poetry https://python-poetry.org/ for managing dependencies and providing for repeatable installs. Its not typically required for developers as we maintain setuptools/pip/tox compatible environments, however familiarity is needed when making changes to the dependency graph (add/update/remove) dependencies, as all the setup.py/requirements files are generated artifacts. The reasoning around the move to poetry was that of needing better tooling to freeze the custodian dependency graph when publishing packages to pypi to ensure that releases would be repeatably installable at a future date in spite of changes to the underlying dependency graph, some perhaps not obeying semantic versioning principles. Additionally, with the growth of providers and other tools, we wanted better holistic management for release automation across the set of packages. After experimenting with a few tools in the ecosystem, including building our own, the maintainers settled on poetry as one that offered both a superior ux, was actively maintained, and had a reasonable python api for additional release management activities. Our additional tooling around poetry is to help automate management across the half-dozen custodian packages as well as to keep requirements and setup.py files in sync. We continue to use setuptools/pip in our CI infrastructure as it offers significant speed benefits [0]. To ensure the poetry install is exercised as part of CI, we do maintain the main docker image via poetry. Usage We maintain several makefile targets that can be used to front end poetry. [make install-poetry]{.title-ref} an alternative custodian installation method, assumes poetry is already installed. [make pkg-show-update]{.title-ref} show available updates to packages in poetry lockfiles. [make pkg-update]{.title-ref} attempts to update dependencies across the tree, should be followed by gen-requirements/gen-setup below. [make pkg-gen-requirements]{.title-ref} show available updates to packages in poetry lockfiles. [make pkg-gen-setup]{.title-ref} generate setup.py files from pyproject.toml this will carry over semver constraints. [make pkg-freeze-setup]{.title-ref} generate setup.py files from pyproject.toml with all dependencies frozen in setup.py. [make pkg-publish-wheel]{.title-ref} increments version, builds wheels, lints, and publishes build to testpypi via twine. The underlying script that provides additional poetry/packaging automation specific to custodian is in tools/dev/poetrypkg.py [0] poetry will call out to pip as a subprocess per package to control the exact versions installed, as pip does not have a public api. Caveats To maintain dependencies between packages within our repository, we specify all intra repo dependencies as dev dependencies with relative directory source paths. When when we generate setup.py files we do so sans any dev dependencies, which we resolve during generation to the latest version, frozen or semver compatible, per source directory development dependency. One interesting consequence of source directory dependencies in poetry is that they break any attempts to distribute/publish a package, even if they are [dev]{.title-ref} dependencies. This is because, per the pyproject.toml spec, poetry will be invoked during install by the build system. The invocation/installation of poetry as a build sys is transparently handled by pip, but simple resolution/parse of pyproject.toml dev dependencies will cause a poetry failure for a source distribution install, as installation of an sdist, is actually a wheel compilation. As a result of this publishing limitation we only publish wheels instead of sdists, which avoids the build system entirely, because a wheel is an extractable installation container/format file.","title":"Packaging.rst"},{"location":"developer/packaging.rst/#packaging-custodian-developer-packaging","text":"Custodian uses poetry https://python-poetry.org/ for managing dependencies and providing for repeatable installs. Its not typically required for developers as we maintain setuptools/pip/tox compatible environments, however familiarity is needed when making changes to the dependency graph (add/update/remove) dependencies, as all the setup.py/requirements files are generated artifacts. The reasoning around the move to poetry was that of needing better tooling to freeze the custodian dependency graph when publishing packages to pypi to ensure that releases would be repeatably installable at a future date in spite of changes to the underlying dependency graph, some perhaps not obeying semantic versioning principles. Additionally, with the growth of providers and other tools, we wanted better holistic management for release automation across the set of packages. After experimenting with a few tools in the ecosystem, including building our own, the maintainers settled on poetry as one that offered both a superior ux, was actively maintained, and had a reasonable python api for additional release management activities. Our additional tooling around poetry is to help automate management across the half-dozen custodian packages as well as to keep requirements and setup.py files in sync. We continue to use setuptools/pip in our CI infrastructure as it offers significant speed benefits [0]. To ensure the poetry install is exercised as part of CI, we do maintain the main docker image via poetry.","title":"Packaging Custodian {#developer-packaging}"},{"location":"developer/packaging.rst/#usage","text":"We maintain several makefile targets that can be used to front end poetry. [make install-poetry]{.title-ref} an alternative custodian installation method, assumes poetry is already installed. [make pkg-show-update]{.title-ref} show available updates to packages in poetry lockfiles. [make pkg-update]{.title-ref} attempts to update dependencies across the tree, should be followed by gen-requirements/gen-setup below. [make pkg-gen-requirements]{.title-ref} show available updates to packages in poetry lockfiles. [make pkg-gen-setup]{.title-ref} generate setup.py files from pyproject.toml this will carry over semver constraints. [make pkg-freeze-setup]{.title-ref} generate setup.py files from pyproject.toml with all dependencies frozen in setup.py. [make pkg-publish-wheel]{.title-ref} increments version, builds wheels, lints, and publishes build to testpypi via twine. The underlying script that provides additional poetry/packaging automation specific to custodian is in tools/dev/poetrypkg.py [0] poetry will call out to pip as a subprocess per package to control the exact versions installed, as pip does not have a public api.","title":"Usage"},{"location":"developer/packaging.rst/#caveats","text":"To maintain dependencies between packages within our repository, we specify all intra repo dependencies as dev dependencies with relative directory source paths. When when we generate setup.py files we do so sans any dev dependencies, which we resolve during generation to the latest version, frozen or semver compatible, per source directory development dependency. One interesting consequence of source directory dependencies in poetry is that they break any attempts to distribute/publish a package, even if they are [dev]{.title-ref} dependencies. This is because, per the pyproject.toml spec, poetry will be invoked during install by the build system. The invocation/installation of poetry as a build sys is transparently handled by pip, but simple resolution/parse of pyproject.toml dev dependencies will cause a poetry failure for a source distribution install, as installation of an sdist, is actually a wheel compilation. As a result of this publishing limitation we only publish wheels instead of sdists, which avoids the build system entirely, because a wheel is an extractable installation container/format file.","title":"Caveats"},{"location":"developer/tests/","text":"Testing for Developers {#developer-tests} Running tests Unit tests can be run with: $ tox Linting can be run with: $ make lint To run tests directly with pytest, or to integrate into your IDE, you can reference tox.ini for the appropriate commands and environment variable configuration. Testing done without C7N_TEST_RUN and C7N_VALIDATE may not match tox results. Operating System Compatibility Tests are currently executed on both Ubuntu 1804 and Windows Server 2019 and must pass on both operating systems. Both Windows and Linux sample dockerfiles are provided for running Tox which may help you. You can find these in [tools/dev]{.title-ref}. In Docker for Windows you can run both of these containers, even simultaneously . If you need access to Windows you can download a virtual machine directly from Microsoft for any hypervisor. Writing Tests for Cloud Controlled Resources Cloud Custodian makes use of flight recording to allow for both functional and unit testing. Each of the custodian providers uses a separate technique that integrates with the provider sdk to handle flight recording of custodian\\'s api calls, we provide a common abstraction over that in our testing framework via record_flight_data/replay_flight_data/ For setting up infrastructure to execute/test policies against we use the pytest-terraform library. Pytest Terraform a Pytest Plugin leveraging terraform to setup test environments Creating Cloud Resources with Terraform {#Creating Tests} If a test requires pre-existing cloud resources in order to operate, [pytest-terraform]{.title-ref} is the preferred method for creating those resources. Pytest Terraform uses Terraform to repeatably & reliably stand up cloud resources. Make sure you have installed terraform and the terraform command is available in your shell\\'s PATH. $ terraform version In addition to a working terraform installation, credentials and configuration for the target cloud will need to be completed. Getting started with Terraform Pytest Terraform looks for matching modules in the tests/terraform directory. So for a test named test_file_example the terraform files for that test will be in tests/terraform/file_example . Here\\'s an example terraform file for the upcoming example. It is placed in tests/terraform/file_example/main.tf . {.terraform} resource \"local_file\" \"bar\" { content = \"bar!\" filename = \"${path.module}/bar.txt\" } When invoked, this terraform module will create a file bar.txt with the contents bar! . In order to access this terraform module, import and wrap a test method with the @terraform decorator. The decorator takes one required positional argument, the name of the module, which in the above example is file_example . In addition to the single positional argument, there are several keyword arguments to control how the decorator operates. The following code example demonstrates how to run and interact with the previous terraform file. {.python} @terraform('file_example', replay=False) def test_file_example(file_example): assert file_example['local_file.bar.content'] == 'bar!' When first creating a test, explicitly set the replay parameter to False . This will force terraform to run on each invocation of the test and perform the flight recording function. The outputs and results of the terraform run are available via the fixture passed into the test method. The fixture will always be named after the terraform module supplied in the first parameter to the decorator, in this case file_example . Pytest Terraform uses JMSEPath lookups, so in order to get the content of the bar resource local_file.bar.content is supplied as the item for lookup. Run this test using the following command, which will also generate flight recordings for terraform: {.shell} $ pytest tests/path/to/test.py -s -v -k 'test_file_example' This may take a little while as tests are typically interacting with the cloud. All terraform state is recorded in the same directory of the terraform module as a tf_resources.json file. {.shell} $ ls tests/terraform/file_example/ main.tf tf_resources.json Each invocation of the test where replay is False , the tf_resources.json contents are replaced and updated with that runs output. When the test is completed, remove replay=False in order to switch to replay mode by default. ``` {.python} @terraform('file_example') def test_file_example(file_example): assert file_example['local_file.bar.content'] == 'bar!' ``` Now when the test is run it will use the data previously recorded terraform resources and not run terraform directly. When committing your test, don\\'t forget to include the tests/terraform/file_example directory! If your test performs destructive actions against a cloud resource created by terraform, check out Controlling Resource Cleanup Recording Custodian Interactions Cloud Custodian tests provide a pytest fixture, test , that provides access to common unittest methods (such as assertEqual ) as well as the placebo based test methods. In order to write a placebo enabled test two helper methods are provided: record_flight_data - use this when creating the test replay_flight_data - use this when the test is completed When first creating a test, use the record_flight_data method. This will contact the cloud and store all responses as files in the placebo directory ( tests/data/placebo/ ). The method takes one parameter, which is the directory name to store placebo output in and it must be unique across all tests. For example: ``` {.python} def test_example(test): session_factory = test.record_flight_data('test_example') policy = { 'name': 'list-ec2-instances', 'resource': 'aws.ec2', } policy = test.load_policy( policy, session_factory=session_factory ) resources = policy.run() test.assertEqual(len(resources), 1) ``` Now run this test using the following command to generate the placebo data: {.shell} $ pytest tests/path/to/test.py -s -v This may take a little while as the test is contacting AWS. All responses are stored in the placebo directory, and can be viewed when the test is finished. It is not necessary to inspect these files, but they can be helpful if the test is not behaving how you expect. {.shell} $ ls tests/data/placebo/test_example/ ec2.DescribeInstances_1.json ec2.DescribeTags_1.json If it is necessary to run the test again - for example, if the test fails, or if it is not yet fully complete - you can run with record_flight_data as many times as necessary. The contents of the directory will be cleared each time the test is run while record_flight_data is in place. When the test is completed, change to using replay_flight_data : ``` {.python} def test_example(self, test): session_factory = test.replay_flight_data('test_example') ... ``` Now when the test is run it will use the data previously recorded and will not contact the cloud. When committing your test, don\\'t forget to include the tests/data/placebo/test_example directory! Note: If it\\'s necessary to delay CLI calls due to delays in the time it takes for an attribute on a resource to be reflected in an API call or any other reason, use test.recording to only sleep when recording json like so: ``` {.python} import time ... def test_example(self, test): ... if test.recording: time.sleep(10) ``` Controlling Resource Cleanup If terraform destroy command fails during cleanup, it will mark the test as failed. For tests that perform destructive actions against terraform managed resources there is an option to tune how pytest-terraform performs this cleanup operation. There are three options available for the teardown parameter: [terraform.TEARDOWN_ON]{.title-ref} - Always perform terraform cleanup, fail on error [terraform.TEARDOWN_OFF]{.title-ref} - Never perform the terraform cleanup [terraform.TEARDOWN_IGNORE]{.title-ref} - Always perform the terraform cleanup, ignore errors In general, [TEARDOWN_ON]{.title-ref} and [TEARDOWN_IGNORE]{.title-ref} are used for test teardown. For debugging purposes [TEARDOWN_OFF]{.title-ref} is provided allowing test authors to inspect cloud entities after each test run. In this example we create a new SQS and a policy to delete it then assert it is deleted. To avoid terraform erroring on teardown [TEARDOWN_IGNORE]{.title-ref} is used. ``` {.terraform} provider \"aws\" {} resource \"aws_sqs_queue\" \"test_sqs\" { name = \"delete-me\" } ``` The following test uses the above [sqs_delete]{.title-ref} terraform module: ``` {.python} from pytest_terraform import terraform @terraform('sqs_delete', teardown=terraform.TEARDOWN_IGNORE) def test_sqs_delete(test, sqs_delete): # Create a placebo record/replay session. session_factory = test.replay_flight_data(\"test_sqs_delete\") client = session_factory().client(\"sqs\") # Extract Queue ARN from terraform output queue_arn = sqs_delete[\"aws_sqs_queue.test_sqs.arn\"] # Create a policy that will delete any matched resources p = test.load_policy( { \"name\": \"sqs-delete\", \"resource\": \"sqs\", \"filters\": [{\"QueueArn\": queue_arn}], \"actions\": [{\"type\": \"delete\"}], }, session_factory=session_factory, ) resources = p.run() # Checks to make sure our single test queue was found test.assertEqual(len(resources), 1) # Extract the QueueURL from the filtered resource queue_url = resources[0]['QueueUrl'] # Attempt to delete the queue and expect AWS API to produce an error pytest.raises(ClientError, client.purge_queue, QueueUrl=queue_url) ``` Converting older functional tests {#Converting Tests} Before the introduction of pytest-terraform many functional tests were wrapped with @functional and used class-based tests which inherited BaseTest . To convert a previous functional testing to use the preferred pytest-terraform method outlined above, first move the method to either a base class which does not inherit BaseTest as pytest does not support fixtures with unittest derived classes, alternatively convert the test to a function. Once the test method has been relocated, replace any references to @functional with the appropriate @terraform decorator from Creating Cloud Resources with Terraform . Finally, replace all mentions of self with the test fixture outlined in Recording Custodian Interactions Before committing any changes, the tests should be run explicitly in record mode to capture all new changes in flight data. Below is an example, older, functional test class TestSqs(BaseTest): @functional def test_sqs_delete(self): session_factory = self.replay_flight_data(\"test_sqs_delete\") client = session_factory().client(\"sqs\") client.create_queue(QueueName=\"test-sqs\") queue_url = client.get_queue_url(QueueName=\"test-sqs\")[\"QueueUrl\"] p = self.load_policy( { \"name\": \"sqs-delete\", \"resource\": \"sqs\", \"filters\": [{\"QueueUrl\": queue_url}], \"actions\": [{\"type\": \"delete\"}], }, session_factory=session_factory, ) resources = p.run() self.assertEqual(len(resources), 1) self.assertRaises(ClientError, client.purge_queue, QueueUrl=queue_url) if self.recording: time.sleep(60) This can be replaced with a new sqs_delete terraform module and the following code: from pytest_terraform import terraform @terraform('sqs_delete', teardown=terraform.TEARDOWN_IGNORE) def test_sqs_delete(test, sqs_delete): session_factory = test.replay_flight_data(\"test_sqs_delete\") client = session_factory().client(\"sqs\") queue_arn = sqs_delete[\"aws_sqs_queue.test_sqs.arn\"] p = test.load_policy( { \"name\": \"sqs-delete\", \"resource\": \"sqs\", \"filters\": [{\"QueueArn\": queue_arn}], \"actions\": [{\"type\": \"delete\"}], }, session_factory=session_factory, ) resources = p.run() test.assertEqual(len(resources), 1) queue_url = resources[0]['QueueUrl'] pytest.raises(ClientError, client.purge_queue, QueueUrl=queue_url)","title":"Tests"},{"location":"developer/tests/#testing-for-developers-developer-tests","text":"","title":"Testing for Developers {#developer-tests}"},{"location":"developer/tests/#running-tests","text":"Unit tests can be run with: $ tox Linting can be run with: $ make lint To run tests directly with pytest, or to integrate into your IDE, you can reference tox.ini for the appropriate commands and environment variable configuration. Testing done without C7N_TEST_RUN and C7N_VALIDATE may not match tox results.","title":"Running tests"},{"location":"developer/tests/#operating-system-compatibility","text":"Tests are currently executed on both Ubuntu 1804 and Windows Server 2019 and must pass on both operating systems. Both Windows and Linux sample dockerfiles are provided for running Tox which may help you. You can find these in [tools/dev]{.title-ref}. In Docker for Windows you can run both of these containers, even simultaneously . If you need access to Windows you can download a virtual machine directly from Microsoft for any hypervisor.","title":"Operating System Compatibility"},{"location":"developer/tests/#writing-tests-for-cloud-controlled-resources","text":"Cloud Custodian makes use of flight recording to allow for both functional and unit testing. Each of the custodian providers uses a separate technique that integrates with the provider sdk to handle flight recording of custodian\\'s api calls, we provide a common abstraction over that in our testing framework via record_flight_data/replay_flight_data/ For setting up infrastructure to execute/test policies against we use the pytest-terraform library. Pytest Terraform a Pytest Plugin leveraging terraform to setup test environments","title":"Writing Tests for Cloud Controlled Resources"},{"location":"developer/tests/#creating-cloud-resources-with-terraform-creating-tests","text":"If a test requires pre-existing cloud resources in order to operate, [pytest-terraform]{.title-ref} is the preferred method for creating those resources. Pytest Terraform uses Terraform to repeatably & reliably stand up cloud resources. Make sure you have installed terraform and the terraform command is available in your shell\\'s PATH. $ terraform version In addition to a working terraform installation, credentials and configuration for the target cloud will need to be completed. Getting started with Terraform Pytest Terraform looks for matching modules in the tests/terraform directory. So for a test named test_file_example the terraform files for that test will be in tests/terraform/file_example . Here\\'s an example terraform file for the upcoming example. It is placed in tests/terraform/file_example/main.tf . {.terraform} resource \"local_file\" \"bar\" { content = \"bar!\" filename = \"${path.module}/bar.txt\" } When invoked, this terraform module will create a file bar.txt with the contents bar! . In order to access this terraform module, import and wrap a test method with the @terraform decorator. The decorator takes one required positional argument, the name of the module, which in the above example is file_example . In addition to the single positional argument, there are several keyword arguments to control how the decorator operates. The following code example demonstrates how to run and interact with the previous terraform file. {.python} @terraform('file_example', replay=False) def test_file_example(file_example): assert file_example['local_file.bar.content'] == 'bar!' When first creating a test, explicitly set the replay parameter to False . This will force terraform to run on each invocation of the test and perform the flight recording function. The outputs and results of the terraform run are available via the fixture passed into the test method. The fixture will always be named after the terraform module supplied in the first parameter to the decorator, in this case file_example . Pytest Terraform uses JMSEPath lookups, so in order to get the content of the bar resource local_file.bar.content is supplied as the item for lookup. Run this test using the following command, which will also generate flight recordings for terraform: {.shell} $ pytest tests/path/to/test.py -s -v -k 'test_file_example' This may take a little while as tests are typically interacting with the cloud. All terraform state is recorded in the same directory of the terraform module as a tf_resources.json file. {.shell} $ ls tests/terraform/file_example/ main.tf tf_resources.json Each invocation of the test where replay is False , the tf_resources.json contents are replaced and updated with that runs output. When the test is completed, remove replay=False in order to switch to replay mode by default. ``` {.python} @terraform('file_example') def test_file_example(file_example): assert file_example['local_file.bar.content'] == 'bar!' ``` Now when the test is run it will use the data previously recorded terraform resources and not run terraform directly. When committing your test, don\\'t forget to include the tests/terraform/file_example directory! If your test performs destructive actions against a cloud resource created by terraform, check out Controlling Resource Cleanup","title":"Creating Cloud Resources with Terraform {#Creating Tests}"},{"location":"developer/tests/#recording-custodian-interactions","text":"Cloud Custodian tests provide a pytest fixture, test , that provides access to common unittest methods (such as assertEqual ) as well as the placebo based test methods. In order to write a placebo enabled test two helper methods are provided: record_flight_data - use this when creating the test replay_flight_data - use this when the test is completed When first creating a test, use the record_flight_data method. This will contact the cloud and store all responses as files in the placebo directory ( tests/data/placebo/ ). The method takes one parameter, which is the directory name to store placebo output in and it must be unique across all tests. For example: ``` {.python} def test_example(test): session_factory = test.record_flight_data('test_example') policy = { 'name': 'list-ec2-instances', 'resource': 'aws.ec2', } policy = test.load_policy( policy, session_factory=session_factory ) resources = policy.run() test.assertEqual(len(resources), 1) ``` Now run this test using the following command to generate the placebo data: {.shell} $ pytest tests/path/to/test.py -s -v This may take a little while as the test is contacting AWS. All responses are stored in the placebo directory, and can be viewed when the test is finished. It is not necessary to inspect these files, but they can be helpful if the test is not behaving how you expect. {.shell} $ ls tests/data/placebo/test_example/ ec2.DescribeInstances_1.json ec2.DescribeTags_1.json If it is necessary to run the test again - for example, if the test fails, or if it is not yet fully complete - you can run with record_flight_data as many times as necessary. The contents of the directory will be cleared each time the test is run while record_flight_data is in place. When the test is completed, change to using replay_flight_data : ``` {.python} def test_example(self, test): session_factory = test.replay_flight_data('test_example') ... ``` Now when the test is run it will use the data previously recorded and will not contact the cloud. When committing your test, don\\'t forget to include the tests/data/placebo/test_example directory! Note: If it\\'s necessary to delay CLI calls due to delays in the time it takes for an attribute on a resource to be reflected in an API call or any other reason, use test.recording to only sleep when recording json like so: ``` {.python} import time ... def test_example(self, test): ... if test.recording: time.sleep(10) ```","title":"Recording Custodian Interactions"},{"location":"developer/tests/#controlling-resource-cleanup","text":"If terraform destroy command fails during cleanup, it will mark the test as failed. For tests that perform destructive actions against terraform managed resources there is an option to tune how pytest-terraform performs this cleanup operation. There are three options available for the teardown parameter: [terraform.TEARDOWN_ON]{.title-ref} - Always perform terraform cleanup, fail on error [terraform.TEARDOWN_OFF]{.title-ref} - Never perform the terraform cleanup [terraform.TEARDOWN_IGNORE]{.title-ref} - Always perform the terraform cleanup, ignore errors In general, [TEARDOWN_ON]{.title-ref} and [TEARDOWN_IGNORE]{.title-ref} are used for test teardown. For debugging purposes [TEARDOWN_OFF]{.title-ref} is provided allowing test authors to inspect cloud entities after each test run. In this example we create a new SQS and a policy to delete it then assert it is deleted. To avoid terraform erroring on teardown [TEARDOWN_IGNORE]{.title-ref} is used. ``` {.terraform} provider \"aws\" {} resource \"aws_sqs_queue\" \"test_sqs\" { name = \"delete-me\" } ``` The following test uses the above [sqs_delete]{.title-ref} terraform module: ``` {.python} from pytest_terraform import terraform @terraform('sqs_delete', teardown=terraform.TEARDOWN_IGNORE) def test_sqs_delete(test, sqs_delete): # Create a placebo record/replay session. session_factory = test.replay_flight_data(\"test_sqs_delete\") client = session_factory().client(\"sqs\") # Extract Queue ARN from terraform output queue_arn = sqs_delete[\"aws_sqs_queue.test_sqs.arn\"] # Create a policy that will delete any matched resources p = test.load_policy( { \"name\": \"sqs-delete\", \"resource\": \"sqs\", \"filters\": [{\"QueueArn\": queue_arn}], \"actions\": [{\"type\": \"delete\"}], }, session_factory=session_factory, ) resources = p.run() # Checks to make sure our single test queue was found test.assertEqual(len(resources), 1) # Extract the QueueURL from the filtered resource queue_url = resources[0]['QueueUrl'] # Attempt to delete the queue and expect AWS API to produce an error pytest.raises(ClientError, client.purge_queue, QueueUrl=queue_url) ```","title":"Controlling Resource Cleanup"},{"location":"developer/tests/#converting-older-functional-tests-converting-tests","text":"Before the introduction of pytest-terraform many functional tests were wrapped with @functional and used class-based tests which inherited BaseTest . To convert a previous functional testing to use the preferred pytest-terraform method outlined above, first move the method to either a base class which does not inherit BaseTest as pytest does not support fixtures with unittest derived classes, alternatively convert the test to a function. Once the test method has been relocated, replace any references to @functional with the appropriate @terraform decorator from Creating Cloud Resources with Terraform . Finally, replace all mentions of self with the test fixture outlined in Recording Custodian Interactions Before committing any changes, the tests should be run explicitly in record mode to capture all new changes in flight data. Below is an example, older, functional test class TestSqs(BaseTest): @functional def test_sqs_delete(self): session_factory = self.replay_flight_data(\"test_sqs_delete\") client = session_factory().client(\"sqs\") client.create_queue(QueueName=\"test-sqs\") queue_url = client.get_queue_url(QueueName=\"test-sqs\")[\"QueueUrl\"] p = self.load_policy( { \"name\": \"sqs-delete\", \"resource\": \"sqs\", \"filters\": [{\"QueueUrl\": queue_url}], \"actions\": [{\"type\": \"delete\"}], }, session_factory=session_factory, ) resources = p.run() self.assertEqual(len(resources), 1) self.assertRaises(ClientError, client.purge_queue, QueueUrl=queue_url) if self.recording: time.sleep(60) This can be replaced with a new sqs_delete terraform module and the following code: from pytest_terraform import terraform @terraform('sqs_delete', teardown=terraform.TEARDOWN_IGNORE) def test_sqs_delete(test, sqs_delete): session_factory = test.replay_flight_data(\"test_sqs_delete\") client = session_factory().client(\"sqs\") queue_arn = sqs_delete[\"aws_sqs_queue.test_sqs.arn\"] p = test.load_policy( { \"name\": \"sqs-delete\", \"resource\": \"sqs\", \"filters\": [{\"QueueArn\": queue_arn}], \"actions\": [{\"type\": \"delete\"}], }, session_factory=session_factory, ) resources = p.run() test.assertEqual(len(resources), 1) queue_url = resources[0]['QueueUrl'] pytest.raises(ClientError, client.purge_queue, QueueUrl=queue_url)","title":"Converting older functional tests {#Converting Tests}"},{"location":"developer/tests.rst/","text":"Testing for Developers {#developer-tests} Running tests Unit tests can be run with: $ tox Linting can be run with: $ make lint To run tests directly with pytest, or to integrate into your IDE, you can reference tox.ini for the appropriate commands and environment variable configuration. Testing done without C7N_TEST_RUN and C7N_VALIDATE may not match tox results. Operating System Compatibility Tests are currently executed on both Ubuntu 1804 and Windows Server 2019 and must pass on both operating systems. Both Windows and Linux sample dockerfiles are provided for running Tox which may help you. You can find these in [tools/dev]{.title-ref}. In Docker for Windows you can run both of these containers, even simultaneously . If you need access to Windows you can download a virtual machine directly from Microsoft for any hypervisor. Writing Tests for Cloud Controlled Resources Cloud Custodian makes use of flight recording to allow for both functional and unit testing. Each of the custodian providers uses a separate technique that integrates with the provider sdk to handle flight recording of custodian\\'s api calls, we provide a common abstraction over that in our testing framework via record_flight_data/replay_flight_data/ For setting up infrastructure to execute/test policies against we use the pytest-terraform library. Pytest Terraform a Pytest Plugin leveraging terraform to setup test environments Creating Cloud Resources with Terraform {#Creating Tests} If a test requires pre-existing cloud resources in order to operate, [pytest-terraform]{.title-ref} is the preferred method for creating those resources. Pytest Terraform uses Terraform to repeatably & reliably stand up cloud resources. Make sure you have installed terraform and the terraform command is available in your shell\\'s PATH. $ terraform version In addition to a working terraform installation, credentials and configuration for the target cloud will need to be completed. Getting started with Terraform Pytest Terraform looks for matching modules in the tests/terraform directory. So for a test named test_file_example the terraform files for that test will be in tests/terraform/file_example . Here\\'s an example terraform file for the upcoming example. It is placed in tests/terraform/file_example/main.tf . {.terraform} resource \"local_file\" \"bar\" { content = \"bar!\" filename = \"${path.module}/bar.txt\" } When invoked, this terraform module will create a file bar.txt with the contents bar! . In order to access this terraform module, import and wrap a test method with the @terraform decorator. The decorator takes one required positional argument, the name of the module, which in the above example is file_example . In addition to the single positional argument, there are several keyword arguments to control how the decorator operates. The following code example demonstrates how to run and interact with the previous terraform file. {.python} @terraform('file_example', replay=False) def test_file_example(file_example): assert file_example['local_file.bar.content'] == 'bar!' When first creating a test, explicitly set the replay parameter to False . This will force terraform to run on each invocation of the test and perform the flight recording function. The outputs and results of the terraform run are available via the fixture passed into the test method. The fixture will always be named after the terraform module supplied in the first parameter to the decorator, in this case file_example . Pytest Terraform uses JMSEPath lookups, so in order to get the content of the bar resource local_file.bar.content is supplied as the item for lookup. Run this test using the following command, which will also generate flight recordings for terraform: {.shell} $ pytest tests/path/to/test.py -s -v -k 'test_file_example' This may take a little while as tests are typically interacting with the cloud. All terraform state is recorded in the same directory of the terraform module as a tf_resources.json file. {.shell} $ ls tests/terraform/file_example/ main.tf tf_resources.json Each invocation of the test where replay is False , the tf_resources.json contents are replaced and updated with that runs output. When the test is completed, remove replay=False in order to switch to replay mode by default. ``` {.python} @terraform('file_example') def test_file_example(file_example): assert file_example['local_file.bar.content'] == 'bar!' ``` Now when the test is run it will use the data previously recorded terraform resources and not run terraform directly. When committing your test, don\\'t forget to include the tests/terraform/file_example directory! If your test performs destructive actions against a cloud resource created by terraform, check out Controlling Resource Cleanup Recording Custodian Interactions Cloud Custodian tests provide a pytest fixture, test , that provides access to common unittest methods (such as assertEqual ) as well as the placebo based test methods. In order to write a placebo enabled test two helper methods are provided: record_flight_data - use this when creating the test replay_flight_data - use this when the test is completed When first creating a test, use the record_flight_data method. This will contact the cloud and store all responses as files in the placebo directory ( tests/data/placebo/ ). The method takes one parameter, which is the directory name to store placebo output in and it must be unique across all tests. For example: ``` {.python} def test_example(test): session_factory = test.record_flight_data('test_example') policy = { 'name': 'list-ec2-instances', 'resource': 'aws.ec2', } policy = test.load_policy( policy, session_factory=session_factory ) resources = policy.run() test.assertEqual(len(resources), 1) ``` Now run this test using the following command to generate the placebo data: {.shell} $ pytest tests/path/to/test.py -s -v This may take a little while as the test is contacting AWS. All responses are stored in the placebo directory, and can be viewed when the test is finished. It is not necessary to inspect these files, but they can be helpful if the test is not behaving how you expect. {.shell} $ ls tests/data/placebo/test_example/ ec2.DescribeInstances_1.json ec2.DescribeTags_1.json If it is necessary to run the test again - for example, if the test fails, or if it is not yet fully complete - you can run with record_flight_data as many times as necessary. The contents of the directory will be cleared each time the test is run while record_flight_data is in place. When the test is completed, change to using replay_flight_data : ``` {.python} def test_example(self, test): session_factory = test.replay_flight_data('test_example') ... ``` Now when the test is run it will use the data previously recorded and will not contact the cloud. When committing your test, don\\'t forget to include the tests/data/placebo/test_example directory! Note: If it\\'s necessary to delay CLI calls due to delays in the time it takes for an attribute on a resource to be reflected in an API call or any other reason, use test.recording to only sleep when recording json like so: ``` {.python} import time ... def test_example(self, test): ... if test.recording: time.sleep(10) ``` Controlling Resource Cleanup If terraform destroy command fails during cleanup, it will mark the test as failed. For tests that perform destructive actions against terraform managed resources there is an option to tune how pytest-terraform performs this cleanup operation. There are three options available for the teardown parameter: [terraform.TEARDOWN_ON]{.title-ref} - Always perform terraform cleanup, fail on error [terraform.TEARDOWN_OFF]{.title-ref} - Never perform the terraform cleanup [terraform.TEARDOWN_IGNORE]{.title-ref} - Always perform the terraform cleanup, ignore errors In general, [TEARDOWN_ON]{.title-ref} and [TEARDOWN_IGNORE]{.title-ref} are used for test teardown. For debugging purposes [TEARDOWN_OFF]{.title-ref} is provided allowing test authors to inspect cloud entities after each test run. In this example we create a new SQS and a policy to delete it then assert it is deleted. To avoid terraform erroring on teardown [TEARDOWN_IGNORE]{.title-ref} is used. ``` {.terraform} provider \"aws\" {} resource \"aws_sqs_queue\" \"test_sqs\" { name = \"delete-me\" } ``` The following test uses the above [sqs_delete]{.title-ref} terraform module: ``` {.python} from pytest_terraform import terraform @terraform('sqs_delete', teardown=terraform.TEARDOWN_IGNORE) def test_sqs_delete(test, sqs_delete): # Create a placebo record/replay session. session_factory = test.replay_flight_data(\"test_sqs_delete\") client = session_factory().client(\"sqs\") # Extract Queue ARN from terraform output queue_arn = sqs_delete[\"aws_sqs_queue.test_sqs.arn\"] # Create a policy that will delete any matched resources p = test.load_policy( { \"name\": \"sqs-delete\", \"resource\": \"sqs\", \"filters\": [{\"QueueArn\": queue_arn}], \"actions\": [{\"type\": \"delete\"}], }, session_factory=session_factory, ) resources = p.run() # Checks to make sure our single test queue was found test.assertEqual(len(resources), 1) # Extract the QueueURL from the filtered resource queue_url = resources[0]['QueueUrl'] # Attempt to delete the queue and expect AWS API to produce an error pytest.raises(ClientError, client.purge_queue, QueueUrl=queue_url) ``` Converting older functional tests {#Converting Tests} Before the introduction of pytest-terraform many functional tests were wrapped with @functional and used class-based tests which inherited BaseTest . To convert a previous functional testing to use the preferred pytest-terraform method outlined above, first move the method to either a base class which does not inherit BaseTest as pytest does not support fixtures with unittest derived classes, alternatively convert the test to a function. Once the test method has been relocated, replace any references to @functional with the appropriate @terraform decorator from Creating Cloud Resources with Terraform . Finally, replace all mentions of self with the test fixture outlined in Recording Custodian Interactions Before committing any changes, the tests should be run explicitly in record mode to capture all new changes in flight data. Below is an example, older, functional test class TestSqs(BaseTest): @functional def test_sqs_delete(self): session_factory = self.replay_flight_data(\"test_sqs_delete\") client = session_factory().client(\"sqs\") client.create_queue(QueueName=\"test-sqs\") queue_url = client.get_queue_url(QueueName=\"test-sqs\")[\"QueueUrl\"] p = self.load_policy( { \"name\": \"sqs-delete\", \"resource\": \"sqs\", \"filters\": [{\"QueueUrl\": queue_url}], \"actions\": [{\"type\": \"delete\"}], }, session_factory=session_factory, ) resources = p.run() self.assertEqual(len(resources), 1) self.assertRaises(ClientError, client.purge_queue, QueueUrl=queue_url) if self.recording: time.sleep(60) This can be replaced with a new sqs_delete terraform module and the following code: from pytest_terraform import terraform @terraform('sqs_delete', teardown=terraform.TEARDOWN_IGNORE) def test_sqs_delete(test, sqs_delete): session_factory = test.replay_flight_data(\"test_sqs_delete\") client = session_factory().client(\"sqs\") queue_arn = sqs_delete[\"aws_sqs_queue.test_sqs.arn\"] p = test.load_policy( { \"name\": \"sqs-delete\", \"resource\": \"sqs\", \"filters\": [{\"QueueArn\": queue_arn}], \"actions\": [{\"type\": \"delete\"}], }, session_factory=session_factory, ) resources = p.run() test.assertEqual(len(resources), 1) queue_url = resources[0]['QueueUrl'] pytest.raises(ClientError, client.purge_queue, QueueUrl=queue_url)","title":"Tests.rst"},{"location":"developer/tests.rst/#testing-for-developers-developer-tests","text":"","title":"Testing for Developers {#developer-tests}"},{"location":"developer/tests.rst/#running-tests","text":"Unit tests can be run with: $ tox Linting can be run with: $ make lint To run tests directly with pytest, or to integrate into your IDE, you can reference tox.ini for the appropriate commands and environment variable configuration. Testing done without C7N_TEST_RUN and C7N_VALIDATE may not match tox results.","title":"Running tests"},{"location":"developer/tests.rst/#operating-system-compatibility","text":"Tests are currently executed on both Ubuntu 1804 and Windows Server 2019 and must pass on both operating systems. Both Windows and Linux sample dockerfiles are provided for running Tox which may help you. You can find these in [tools/dev]{.title-ref}. In Docker for Windows you can run both of these containers, even simultaneously . If you need access to Windows you can download a virtual machine directly from Microsoft for any hypervisor.","title":"Operating System Compatibility"},{"location":"developer/tests.rst/#writing-tests-for-cloud-controlled-resources","text":"Cloud Custodian makes use of flight recording to allow for both functional and unit testing. Each of the custodian providers uses a separate technique that integrates with the provider sdk to handle flight recording of custodian\\'s api calls, we provide a common abstraction over that in our testing framework via record_flight_data/replay_flight_data/ For setting up infrastructure to execute/test policies against we use the pytest-terraform library. Pytest Terraform a Pytest Plugin leveraging terraform to setup test environments","title":"Writing Tests for Cloud Controlled Resources"},{"location":"developer/tests.rst/#creating-cloud-resources-with-terraform-creating-tests","text":"If a test requires pre-existing cloud resources in order to operate, [pytest-terraform]{.title-ref} is the preferred method for creating those resources. Pytest Terraform uses Terraform to repeatably & reliably stand up cloud resources. Make sure you have installed terraform and the terraform command is available in your shell\\'s PATH. $ terraform version In addition to a working terraform installation, credentials and configuration for the target cloud will need to be completed. Getting started with Terraform Pytest Terraform looks for matching modules in the tests/terraform directory. So for a test named test_file_example the terraform files for that test will be in tests/terraform/file_example . Here\\'s an example terraform file for the upcoming example. It is placed in tests/terraform/file_example/main.tf . {.terraform} resource \"local_file\" \"bar\" { content = \"bar!\" filename = \"${path.module}/bar.txt\" } When invoked, this terraform module will create a file bar.txt with the contents bar! . In order to access this terraform module, import and wrap a test method with the @terraform decorator. The decorator takes one required positional argument, the name of the module, which in the above example is file_example . In addition to the single positional argument, there are several keyword arguments to control how the decorator operates. The following code example demonstrates how to run and interact with the previous terraform file. {.python} @terraform('file_example', replay=False) def test_file_example(file_example): assert file_example['local_file.bar.content'] == 'bar!' When first creating a test, explicitly set the replay parameter to False . This will force terraform to run on each invocation of the test and perform the flight recording function. The outputs and results of the terraform run are available via the fixture passed into the test method. The fixture will always be named after the terraform module supplied in the first parameter to the decorator, in this case file_example . Pytest Terraform uses JMSEPath lookups, so in order to get the content of the bar resource local_file.bar.content is supplied as the item for lookup. Run this test using the following command, which will also generate flight recordings for terraform: {.shell} $ pytest tests/path/to/test.py -s -v -k 'test_file_example' This may take a little while as tests are typically interacting with the cloud. All terraform state is recorded in the same directory of the terraform module as a tf_resources.json file. {.shell} $ ls tests/terraform/file_example/ main.tf tf_resources.json Each invocation of the test where replay is False , the tf_resources.json contents are replaced and updated with that runs output. When the test is completed, remove replay=False in order to switch to replay mode by default. ``` {.python} @terraform('file_example') def test_file_example(file_example): assert file_example['local_file.bar.content'] == 'bar!' ``` Now when the test is run it will use the data previously recorded terraform resources and not run terraform directly. When committing your test, don\\'t forget to include the tests/terraform/file_example directory! If your test performs destructive actions against a cloud resource created by terraform, check out Controlling Resource Cleanup","title":"Creating Cloud Resources with Terraform {#Creating Tests}"},{"location":"developer/tests.rst/#recording-custodian-interactions","text":"Cloud Custodian tests provide a pytest fixture, test , that provides access to common unittest methods (such as assertEqual ) as well as the placebo based test methods. In order to write a placebo enabled test two helper methods are provided: record_flight_data - use this when creating the test replay_flight_data - use this when the test is completed When first creating a test, use the record_flight_data method. This will contact the cloud and store all responses as files in the placebo directory ( tests/data/placebo/ ). The method takes one parameter, which is the directory name to store placebo output in and it must be unique across all tests. For example: ``` {.python} def test_example(test): session_factory = test.record_flight_data('test_example') policy = { 'name': 'list-ec2-instances', 'resource': 'aws.ec2', } policy = test.load_policy( policy, session_factory=session_factory ) resources = policy.run() test.assertEqual(len(resources), 1) ``` Now run this test using the following command to generate the placebo data: {.shell} $ pytest tests/path/to/test.py -s -v This may take a little while as the test is contacting AWS. All responses are stored in the placebo directory, and can be viewed when the test is finished. It is not necessary to inspect these files, but they can be helpful if the test is not behaving how you expect. {.shell} $ ls tests/data/placebo/test_example/ ec2.DescribeInstances_1.json ec2.DescribeTags_1.json If it is necessary to run the test again - for example, if the test fails, or if it is not yet fully complete - you can run with record_flight_data as many times as necessary. The contents of the directory will be cleared each time the test is run while record_flight_data is in place. When the test is completed, change to using replay_flight_data : ``` {.python} def test_example(self, test): session_factory = test.replay_flight_data('test_example') ... ``` Now when the test is run it will use the data previously recorded and will not contact the cloud. When committing your test, don\\'t forget to include the tests/data/placebo/test_example directory! Note: If it\\'s necessary to delay CLI calls due to delays in the time it takes for an attribute on a resource to be reflected in an API call or any other reason, use test.recording to only sleep when recording json like so: ``` {.python} import time ... def test_example(self, test): ... if test.recording: time.sleep(10) ```","title":"Recording Custodian Interactions"},{"location":"developer/tests.rst/#controlling-resource-cleanup","text":"If terraform destroy command fails during cleanup, it will mark the test as failed. For tests that perform destructive actions against terraform managed resources there is an option to tune how pytest-terraform performs this cleanup operation. There are three options available for the teardown parameter: [terraform.TEARDOWN_ON]{.title-ref} - Always perform terraform cleanup, fail on error [terraform.TEARDOWN_OFF]{.title-ref} - Never perform the terraform cleanup [terraform.TEARDOWN_IGNORE]{.title-ref} - Always perform the terraform cleanup, ignore errors In general, [TEARDOWN_ON]{.title-ref} and [TEARDOWN_IGNORE]{.title-ref} are used for test teardown. For debugging purposes [TEARDOWN_OFF]{.title-ref} is provided allowing test authors to inspect cloud entities after each test run. In this example we create a new SQS and a policy to delete it then assert it is deleted. To avoid terraform erroring on teardown [TEARDOWN_IGNORE]{.title-ref} is used. ``` {.terraform} provider \"aws\" {} resource \"aws_sqs_queue\" \"test_sqs\" { name = \"delete-me\" } ``` The following test uses the above [sqs_delete]{.title-ref} terraform module: ``` {.python} from pytest_terraform import terraform @terraform('sqs_delete', teardown=terraform.TEARDOWN_IGNORE) def test_sqs_delete(test, sqs_delete): # Create a placebo record/replay session. session_factory = test.replay_flight_data(\"test_sqs_delete\") client = session_factory().client(\"sqs\") # Extract Queue ARN from terraform output queue_arn = sqs_delete[\"aws_sqs_queue.test_sqs.arn\"] # Create a policy that will delete any matched resources p = test.load_policy( { \"name\": \"sqs-delete\", \"resource\": \"sqs\", \"filters\": [{\"QueueArn\": queue_arn}], \"actions\": [{\"type\": \"delete\"}], }, session_factory=session_factory, ) resources = p.run() # Checks to make sure our single test queue was found test.assertEqual(len(resources), 1) # Extract the QueueURL from the filtered resource queue_url = resources[0]['QueueUrl'] # Attempt to delete the queue and expect AWS API to produce an error pytest.raises(ClientError, client.purge_queue, QueueUrl=queue_url) ```","title":"Controlling Resource Cleanup"},{"location":"developer/tests.rst/#converting-older-functional-tests-converting-tests","text":"Before the introduction of pytest-terraform many functional tests were wrapped with @functional and used class-based tests which inherited BaseTest . To convert a previous functional testing to use the preferred pytest-terraform method outlined above, first move the method to either a base class which does not inherit BaseTest as pytest does not support fixtures with unittest derived classes, alternatively convert the test to a function. Once the test method has been relocated, replace any references to @functional with the appropriate @terraform decorator from Creating Cloud Resources with Terraform . Finally, replace all mentions of self with the test fixture outlined in Recording Custodian Interactions Before committing any changes, the tests should be run explicitly in record mode to capture all new changes in flight data. Below is an example, older, functional test class TestSqs(BaseTest): @functional def test_sqs_delete(self): session_factory = self.replay_flight_data(\"test_sqs_delete\") client = session_factory().client(\"sqs\") client.create_queue(QueueName=\"test-sqs\") queue_url = client.get_queue_url(QueueName=\"test-sqs\")[\"QueueUrl\"] p = self.load_policy( { \"name\": \"sqs-delete\", \"resource\": \"sqs\", \"filters\": [{\"QueueUrl\": queue_url}], \"actions\": [{\"type\": \"delete\"}], }, session_factory=session_factory, ) resources = p.run() self.assertEqual(len(resources), 1) self.assertRaises(ClientError, client.purge_queue, QueueUrl=queue_url) if self.recording: time.sleep(60) This can be replaced with a new sqs_delete terraform module and the following code: from pytest_terraform import terraform @terraform('sqs_delete', teardown=terraform.TEARDOWN_IGNORE) def test_sqs_delete(test, sqs_delete): session_factory = test.replay_flight_data(\"test_sqs_delete\") client = session_factory().client(\"sqs\") queue_arn = sqs_delete[\"aws_sqs_queue.test_sqs.arn\"] p = test.load_policy( { \"name\": \"sqs-delete\", \"resource\": \"sqs\", \"filters\": [{\"QueueArn\": queue_arn}], \"actions\": [{\"type\": \"delete\"}], }, session_factory=session_factory, ) resources = p.run() test.assertEqual(len(resources), 1) queue_url = resources[0]['QueueUrl'] pytest.raises(ClientError, client.purge_queue, QueueUrl=queue_url)","title":"Converting older functional tests {#Converting Tests}"},{"location":"gcp/contribute/","text":"Developer Guide {#gcp_contribute} The c7n developer install includes c7n_gcp. A shortcut for creating a virtual env for development is available in the makefile: make install source bin/activate This creates a virtual env in your enlistment and installs all packages as editable. Instead, you can do [pip install -r tools/c7n_gcp/requirements.txt]{.title-ref} to install dependencies. Adding New GCP Resources Create New GCP Resource Most resources extend the QueryResourceManager class. Each class definition will use the \\@resources.register(\\'\\<resource_name>\\') decorator to register that class as a Custodian resource substituting \\<resource_name> with the new resource name. The name specified in the decorator is how the resource will be referenced within policies. Each resource also contains an internal class called [resource_type]{.title-ref}, which contains metadata about the resource definition, and has the following attributes: service is required field, part of the request to GCP resource, : The name of GCP service. component is required field, part of the request to GCP resource, : The name of GCP resource, version is required field, part of the request to GCP resource, : It is the [version]{.title-ref} of used resource API, enum_spec is a required field, : It has a tuple of ([enum_operation]{.title-ref}, [list_operation]{.title-ref}, [extra_args]{.title-ref}). - \\`enum\\_operation\\`: the name of the GCP resource method used to retrieve the list of resources, - \\`list\\_operation\\`: the JMESPath of the field name which contains the resource list in the JSON response body, - \\`extra\\_args\\`: can be used to set up additional params for a request to GCP. id is required field, : It\\'s a field name of the response field that have to be used as resource identifier. The [id]{.title-ref} value is used for filtering. scope is optional field, default is None, : The scope of the Custodian resource. There are available 2 options: [project]{.title-ref} or [None]{.title-ref}. If the [scope]{.title-ref} has a value [project]{.title-ref} the GOOGLE_CLOUD_PROJECT variable will be used for building request to GCP resource. If the scope is [None]{.title-ref} the request to GCP is built ignoring the GOOGLE_CLOUD_PROJECT variable. parent_spec is an optional field that allows to build additional requests to parent resources, default is None. : The field is used when the request to GCP resource should be created with extra parameters that can be loaded from parent resources. The resource should extend ChildResourceManager instead of QueryResourceManager and use ChildTypeInfo instead of TypeInfo to use the field. The [parent_spec]{.title-ref} has following fields: [resource]{.title-ref}, [child_enum_params]{.title-ref}, [parent_get_params]{.title-ref}, [use_child_query]{.title-ref}. - The field [resource]{.title-ref} has value of the [resource\\_name]{.title-ref} from \\@resources.register(\\'\\<resource\\_name\\>\\') that is used for the target parent resource. - The field [child\\_enum\\_params]{.title-ref} is an array of tuples each of which maps parent instance field (first tuple element) to child\\'s list argument (second tuple element). The mappings are used for building [list]{.title-ref} requests to parent resources. It works by the next scenario. First of all it loads a list of instances from parent resource. Further it loads instances for original resources using GCP resource field values from the loaded parent resources. It uses mappings for GCP resource fields from [child\\_enum\\_params]{.title-ref}. The first field in a tuple is a field from parent resource, the second one is the mapped original resource field name. - The field [parent\\_get\\_params]{.title-ref} is an array of tuples each of which maps child instance field (first tuple element) to parent\\'s [resource\\_info]{.title-ref} field. The [resource\\_info]{.title-ref} object has fields like Stackdriver log has. There are 2 options for the fields set: either [resource.labels]{.title-ref} and [protoPayload.resourceName]{.title-ref} or a log of the full event. The mappings are used for building [get]{.title-ref} requests to parent resources. - The field [use\\_child\\_query]{.title-ref} controls whether the [query]{.title-ref} block of the current resource should be copied to its parent. An example that uses [parent_spec]{.title-ref} is available below. # the class extends ChildResourceManager @resources.register('bq-table') class BigQueryTable(ChildResourceManager): # the class extends ChildTypeInfo class resource_type(ChildTypeInfo): service = 'bigquery' # the name of the GCP service version = 'v2' # the version of the GCP service component = 'tables' # the component of the GCP service # The `list` method in the resource. https://cloud.google.com/bigquery/docs/reference/rest/v2/tables/list # It requires 2 request params: `projectId` and `datasetId`. Since the resource has a parent the params will # be extracted from parent's instance enum_spec = ('list', 'tables[]', None) scope_key = 'projectId' id = 'id' parent_spec = { # the name of Custodian parent resource. 'resource': 'bq-dataset', 'child_enum_params': [ # Extract the `datasetReference.datasetId` field from a parent instance and use its value # as the `datasetId` argument for child's `list` method (see `resource_type.enum_spec`) ('datasetReference.datasetId', 'datasetId'), ], 'parent_get_params': [ # Extract the `tableReference.projectId` field from a child instance and use its value # as the `projectId` event's field for parent's `get` method (see `resource_type.get`) ('tableReference.projectId', 'projectId'), # Similar to above ('tableReference.datasetId', 'datasetId'), ] } @staticmethod def get(client, event): return client.execute_query('get', { 'projectId': event['project_id'], 'datasetId': event['dataset_id'], 'tableId': event['resourceName'].rsplit('/', 1)[-1] }) Most resources have get methods that are created based on the corresponding [get]{.title-ref} method of the actual GCP resource. As a rule the Custodian [get]{.title-ref} method has [resource_info]{.title-ref} param. The param has fields that can be found in Stackdriver logs in [protoPayload.resourceName]{.title-ref} and [resource]{.title-ref} fields. Examples of the Stackdriver logs are available in tools/c7n_gcp/tests/data/events folder. There is an example of the resource below. from c7n_gcp.provider import resources from c7n_gcp.query import QueryResourceManager, TypeInfo @resources.register('loadbalancer-address') class LoadBalancingAddress(QueryResourceManager): class resource_type(TypeInfo): service = 'compute' component = 'addresses' version = 'v1' enum_spec = ('aggregatedList', 'items.*.addresses[]', None) scope = 'project' id = 'name' @staticmethod def get(client, resource_info): return client.execute_command('get', { 'project': resource_info['project_id'], 'region': resource_info['location'], 'address': resource_info[ 'resourceName'].rsplit('/', 1)[-1]}) Load New GCP Resource If you created a new module for a GCP service (i.e. this was the first resource implemented for this service in Custodian), then import the new service module in entry.py: entry.py . import c7n_gcp.resources.<name of a file with created resources> Each resource has to have test cases. There are implemented test cases for resources list methods and get methods. Testing c7n_gcp follows the same guideance for test authoring as the core c7n code base<Creating Tests> {.interpreted-text role=\"ref\"}. Examples with relative directories (for example, tests/foo/bar ) should be applied to tools/c7n_gcp and not the root of the cloud-custodian repository. In order to execute live tests you will need to set two additional environment variables. These can be either set in your IDE or exported on the command line. The first is GOOGLE_CLOUD_PROJECT - this should point to a valid Google Cloud Project you have access to. The second, GOOGLE_APPLICATION_CREDENTIALS should be the corresponding credentials json with access to the aforementioned project. For example: export GOOGLE_CLOUD_PROJECT=cloud-custodian export GOOGLE_APPLICATION_CREDENTIALS=data/credentials.json pytest tools/c7n_gcp/tests -n auto Updating Existing Tests Many of tests in this project were created prior to the current functional testing practices. To convert an existing test see how to convert existing tests<Converting Tests> {.interpreted-text role=\"ref\"}.","title":"Contribute"},{"location":"gcp/contribute/#developer-guide-gcp_contribute","text":"The c7n developer install includes c7n_gcp. A shortcut for creating a virtual env for development is available in the makefile: make install source bin/activate This creates a virtual env in your enlistment and installs all packages as editable. Instead, you can do [pip install -r tools/c7n_gcp/requirements.txt]{.title-ref} to install dependencies.","title":"Developer Guide {#gcp_contribute}"},{"location":"gcp/contribute/#adding-new-gcp-resources","text":"","title":"Adding New GCP Resources"},{"location":"gcp/contribute/#create-new-gcp-resource","text":"Most resources extend the QueryResourceManager class. Each class definition will use the \\@resources.register(\\'\\<resource_name>\\') decorator to register that class as a Custodian resource substituting \\<resource_name> with the new resource name. The name specified in the decorator is how the resource will be referenced within policies. Each resource also contains an internal class called [resource_type]{.title-ref}, which contains metadata about the resource definition, and has the following attributes: service is required field, part of the request to GCP resource, : The name of GCP service. component is required field, part of the request to GCP resource, : The name of GCP resource, version is required field, part of the request to GCP resource, : It is the [version]{.title-ref} of used resource API, enum_spec is a required field, : It has a tuple of ([enum_operation]{.title-ref}, [list_operation]{.title-ref}, [extra_args]{.title-ref}). - \\`enum\\_operation\\`: the name of the GCP resource method used to retrieve the list of resources, - \\`list\\_operation\\`: the JMESPath of the field name which contains the resource list in the JSON response body, - \\`extra\\_args\\`: can be used to set up additional params for a request to GCP. id is required field, : It\\'s a field name of the response field that have to be used as resource identifier. The [id]{.title-ref} value is used for filtering. scope is optional field, default is None, : The scope of the Custodian resource. There are available 2 options: [project]{.title-ref} or [None]{.title-ref}. If the [scope]{.title-ref} has a value [project]{.title-ref} the GOOGLE_CLOUD_PROJECT variable will be used for building request to GCP resource. If the scope is [None]{.title-ref} the request to GCP is built ignoring the GOOGLE_CLOUD_PROJECT variable. parent_spec is an optional field that allows to build additional requests to parent resources, default is None. : The field is used when the request to GCP resource should be created with extra parameters that can be loaded from parent resources. The resource should extend ChildResourceManager instead of QueryResourceManager and use ChildTypeInfo instead of TypeInfo to use the field. The [parent_spec]{.title-ref} has following fields: [resource]{.title-ref}, [child_enum_params]{.title-ref}, [parent_get_params]{.title-ref}, [use_child_query]{.title-ref}. - The field [resource]{.title-ref} has value of the [resource\\_name]{.title-ref} from \\@resources.register(\\'\\<resource\\_name\\>\\') that is used for the target parent resource. - The field [child\\_enum\\_params]{.title-ref} is an array of tuples each of which maps parent instance field (first tuple element) to child\\'s list argument (second tuple element). The mappings are used for building [list]{.title-ref} requests to parent resources. It works by the next scenario. First of all it loads a list of instances from parent resource. Further it loads instances for original resources using GCP resource field values from the loaded parent resources. It uses mappings for GCP resource fields from [child\\_enum\\_params]{.title-ref}. The first field in a tuple is a field from parent resource, the second one is the mapped original resource field name. - The field [parent\\_get\\_params]{.title-ref} is an array of tuples each of which maps child instance field (first tuple element) to parent\\'s [resource\\_info]{.title-ref} field. The [resource\\_info]{.title-ref} object has fields like Stackdriver log has. There are 2 options for the fields set: either [resource.labels]{.title-ref} and [protoPayload.resourceName]{.title-ref} or a log of the full event. The mappings are used for building [get]{.title-ref} requests to parent resources. - The field [use\\_child\\_query]{.title-ref} controls whether the [query]{.title-ref} block of the current resource should be copied to its parent. An example that uses [parent_spec]{.title-ref} is available below. # the class extends ChildResourceManager @resources.register('bq-table') class BigQueryTable(ChildResourceManager): # the class extends ChildTypeInfo class resource_type(ChildTypeInfo): service = 'bigquery' # the name of the GCP service version = 'v2' # the version of the GCP service component = 'tables' # the component of the GCP service # The `list` method in the resource. https://cloud.google.com/bigquery/docs/reference/rest/v2/tables/list # It requires 2 request params: `projectId` and `datasetId`. Since the resource has a parent the params will # be extracted from parent's instance enum_spec = ('list', 'tables[]', None) scope_key = 'projectId' id = 'id' parent_spec = { # the name of Custodian parent resource. 'resource': 'bq-dataset', 'child_enum_params': [ # Extract the `datasetReference.datasetId` field from a parent instance and use its value # as the `datasetId` argument for child's `list` method (see `resource_type.enum_spec`) ('datasetReference.datasetId', 'datasetId'), ], 'parent_get_params': [ # Extract the `tableReference.projectId` field from a child instance and use its value # as the `projectId` event's field for parent's `get` method (see `resource_type.get`) ('tableReference.projectId', 'projectId'), # Similar to above ('tableReference.datasetId', 'datasetId'), ] } @staticmethod def get(client, event): return client.execute_query('get', { 'projectId': event['project_id'], 'datasetId': event['dataset_id'], 'tableId': event['resourceName'].rsplit('/', 1)[-1] }) Most resources have get methods that are created based on the corresponding [get]{.title-ref} method of the actual GCP resource. As a rule the Custodian [get]{.title-ref} method has [resource_info]{.title-ref} param. The param has fields that can be found in Stackdriver logs in [protoPayload.resourceName]{.title-ref} and [resource]{.title-ref} fields. Examples of the Stackdriver logs are available in tools/c7n_gcp/tests/data/events folder. There is an example of the resource below. from c7n_gcp.provider import resources from c7n_gcp.query import QueryResourceManager, TypeInfo @resources.register('loadbalancer-address') class LoadBalancingAddress(QueryResourceManager): class resource_type(TypeInfo): service = 'compute' component = 'addresses' version = 'v1' enum_spec = ('aggregatedList', 'items.*.addresses[]', None) scope = 'project' id = 'name' @staticmethod def get(client, resource_info): return client.execute_command('get', { 'project': resource_info['project_id'], 'region': resource_info['location'], 'address': resource_info[ 'resourceName'].rsplit('/', 1)[-1]})","title":"Create New GCP Resource"},{"location":"gcp/contribute/#load-new-gcp-resource","text":"If you created a new module for a GCP service (i.e. this was the first resource implemented for this service in Custodian), then import the new service module in entry.py: entry.py . import c7n_gcp.resources.<name of a file with created resources> Each resource has to have test cases. There are implemented test cases for resources list methods and get methods.","title":"Load New GCP Resource"},{"location":"gcp/contribute/#testing","text":"c7n_gcp follows the same guideance for test authoring as the core c7n code base<Creating Tests> {.interpreted-text role=\"ref\"}. Examples with relative directories (for example, tests/foo/bar ) should be applied to tools/c7n_gcp and not the root of the cloud-custodian repository. In order to execute live tests you will need to set two additional environment variables. These can be either set in your IDE or exported on the command line. The first is GOOGLE_CLOUD_PROJECT - this should point to a valid Google Cloud Project you have access to. The second, GOOGLE_APPLICATION_CREDENTIALS should be the corresponding credentials json with access to the aforementioned project. For example: export GOOGLE_CLOUD_PROJECT=cloud-custodian export GOOGLE_APPLICATION_CREDENTIALS=data/credentials.json pytest tools/c7n_gcp/tests -n auto","title":"Testing"},{"location":"gcp/contribute/#updating-existing-tests","text":"Many of tests in this project were created prior to the current functional testing practices. To convert an existing test see how to convert existing tests<Converting Tests> {.interpreted-text role=\"ref\"}.","title":"Updating Existing Tests"},{"location":"gcp/contribute.rst/","text":"Developer Guide {#gcp_contribute} The c7n developer install includes c7n_gcp. A shortcut for creating a virtual env for development is available in the makefile: make install source bin/activate This creates a virtual env in your enlistment and installs all packages as editable. Instead, you can do [pip install -r tools/c7n_gcp/requirements.txt]{.title-ref} to install dependencies. Adding New GCP Resources Create New GCP Resource Most resources extend the QueryResourceManager class. Each class definition will use the \\@resources.register(\\'\\<resource_name>\\') decorator to register that class as a Custodian resource substituting \\<resource_name> with the new resource name. The name specified in the decorator is how the resource will be referenced within policies. Each resource also contains an internal class called [resource_type]{.title-ref}, which contains metadata about the resource definition, and has the following attributes: service is required field, part of the request to GCP resource, : The name of GCP service. component is required field, part of the request to GCP resource, : The name of GCP resource, version is required field, part of the request to GCP resource, : It is the [version]{.title-ref} of used resource API, enum_spec is a required field, : It has a tuple of ([enum_operation]{.title-ref}, [list_operation]{.title-ref}, [extra_args]{.title-ref}). - \\`enum\\_operation\\`: the name of the GCP resource method used to retrieve the list of resources, - \\`list\\_operation\\`: the JMESPath of the field name which contains the resource list in the JSON response body, - \\`extra\\_args\\`: can be used to set up additional params for a request to GCP. id is required field, : It\\'s a field name of the response field that have to be used as resource identifier. The [id]{.title-ref} value is used for filtering. scope is optional field, default is None, : The scope of the Custodian resource. There are available 2 options: [project]{.title-ref} or [None]{.title-ref}. If the [scope]{.title-ref} has a value [project]{.title-ref} the GOOGLE_CLOUD_PROJECT variable will be used for building request to GCP resource. If the scope is [None]{.title-ref} the request to GCP is built ignoring the GOOGLE_CLOUD_PROJECT variable. parent_spec is an optional field that allows to build additional requests to parent resources, default is None. : The field is used when the request to GCP resource should be created with extra parameters that can be loaded from parent resources. The resource should extend ChildResourceManager instead of QueryResourceManager and use ChildTypeInfo instead of TypeInfo to use the field. The [parent_spec]{.title-ref} has following fields: [resource]{.title-ref}, [child_enum_params]{.title-ref}, [parent_get_params]{.title-ref}, [use_child_query]{.title-ref}. - The field [resource]{.title-ref} has value of the [resource\\_name]{.title-ref} from \\@resources.register(\\'\\<resource\\_name\\>\\') that is used for the target parent resource. - The field [child\\_enum\\_params]{.title-ref} is an array of tuples each of which maps parent instance field (first tuple element) to child\\'s list argument (second tuple element). The mappings are used for building [list]{.title-ref} requests to parent resources. It works by the next scenario. First of all it loads a list of instances from parent resource. Further it loads instances for original resources using GCP resource field values from the loaded parent resources. It uses mappings for GCP resource fields from [child\\_enum\\_params]{.title-ref}. The first field in a tuple is a field from parent resource, the second one is the mapped original resource field name. - The field [parent\\_get\\_params]{.title-ref} is an array of tuples each of which maps child instance field (first tuple element) to parent\\'s [resource\\_info]{.title-ref} field. The [resource\\_info]{.title-ref} object has fields like Stackdriver log has. There are 2 options for the fields set: either [resource.labels]{.title-ref} and [protoPayload.resourceName]{.title-ref} or a log of the full event. The mappings are used for building [get]{.title-ref} requests to parent resources. - The field [use\\_child\\_query]{.title-ref} controls whether the [query]{.title-ref} block of the current resource should be copied to its parent. An example that uses [parent_spec]{.title-ref} is available below. # the class extends ChildResourceManager @resources.register('bq-table') class BigQueryTable(ChildResourceManager): # the class extends ChildTypeInfo class resource_type(ChildTypeInfo): service = 'bigquery' # the name of the GCP service version = 'v2' # the version of the GCP service component = 'tables' # the component of the GCP service # The `list` method in the resource. https://cloud.google.com/bigquery/docs/reference/rest/v2/tables/list # It requires 2 request params: `projectId` and `datasetId`. Since the resource has a parent the params will # be extracted from parent's instance enum_spec = ('list', 'tables[]', None) scope_key = 'projectId' id = 'id' parent_spec = { # the name of Custodian parent resource. 'resource': 'bq-dataset', 'child_enum_params': [ # Extract the `datasetReference.datasetId` field from a parent instance and use its value # as the `datasetId` argument for child's `list` method (see `resource_type.enum_spec`) ('datasetReference.datasetId', 'datasetId'), ], 'parent_get_params': [ # Extract the `tableReference.projectId` field from a child instance and use its value # as the `projectId` event's field for parent's `get` method (see `resource_type.get`) ('tableReference.projectId', 'projectId'), # Similar to above ('tableReference.datasetId', 'datasetId'), ] } @staticmethod def get(client, event): return client.execute_query('get', { 'projectId': event['project_id'], 'datasetId': event['dataset_id'], 'tableId': event['resourceName'].rsplit('/', 1)[-1] }) Most resources have get methods that are created based on the corresponding [get]{.title-ref} method of the actual GCP resource. As a rule the Custodian [get]{.title-ref} method has [resource_info]{.title-ref} param. The param has fields that can be found in Stackdriver logs in [protoPayload.resourceName]{.title-ref} and [resource]{.title-ref} fields. Examples of the Stackdriver logs are available in tools/c7n_gcp/tests/data/events folder. There is an example of the resource below. from c7n_gcp.provider import resources from c7n_gcp.query import QueryResourceManager, TypeInfo @resources.register('loadbalancer-address') class LoadBalancingAddress(QueryResourceManager): class resource_type(TypeInfo): service = 'compute' component = 'addresses' version = 'v1' enum_spec = ('aggregatedList', 'items.*.addresses[]', None) scope = 'project' id = 'name' @staticmethod def get(client, resource_info): return client.execute_command('get', { 'project': resource_info['project_id'], 'region': resource_info['location'], 'address': resource_info[ 'resourceName'].rsplit('/', 1)[-1]}) Load New GCP Resource If you created a new module for a GCP service (i.e. this was the first resource implemented for this service in Custodian), then import the new service module in entry.py: entry.py . import c7n_gcp.resources.<name of a file with created resources> Each resource has to have test cases. There are implemented test cases for resources list methods and get methods. Testing c7n_gcp follows the same guideance for test authoring as the core c7n code base<Creating Tests> {.interpreted-text role=\"ref\"}. Examples with relative directories (for example, tests/foo/bar ) should be applied to tools/c7n_gcp and not the root of the cloud-custodian repository. In order to execute live tests you will need to set two additional environment variables. These can be either set in your IDE or exported on the command line. The first is GOOGLE_CLOUD_PROJECT - this should point to a valid Google Cloud Project you have access to. The second, GOOGLE_APPLICATION_CREDENTIALS should be the corresponding credentials json with access to the aforementioned project. For example: export GOOGLE_CLOUD_PROJECT=cloud-custodian export GOOGLE_APPLICATION_CREDENTIALS=data/credentials.json pytest tools/c7n_gcp/tests -n auto Updating Existing Tests Many of tests in this project were created prior to the current functional testing practices. To convert an existing test see how to convert existing tests<Converting Tests> {.interpreted-text role=\"ref\"}.","title":"Contribute.rst"},{"location":"gcp/contribute.rst/#developer-guide-gcp_contribute","text":"The c7n developer install includes c7n_gcp. A shortcut for creating a virtual env for development is available in the makefile: make install source bin/activate This creates a virtual env in your enlistment and installs all packages as editable. Instead, you can do [pip install -r tools/c7n_gcp/requirements.txt]{.title-ref} to install dependencies.","title":"Developer Guide {#gcp_contribute}"},{"location":"gcp/contribute.rst/#adding-new-gcp-resources","text":"","title":"Adding New GCP Resources"},{"location":"gcp/contribute.rst/#create-new-gcp-resource","text":"Most resources extend the QueryResourceManager class. Each class definition will use the \\@resources.register(\\'\\<resource_name>\\') decorator to register that class as a Custodian resource substituting \\<resource_name> with the new resource name. The name specified in the decorator is how the resource will be referenced within policies. Each resource also contains an internal class called [resource_type]{.title-ref}, which contains metadata about the resource definition, and has the following attributes: service is required field, part of the request to GCP resource, : The name of GCP service. component is required field, part of the request to GCP resource, : The name of GCP resource, version is required field, part of the request to GCP resource, : It is the [version]{.title-ref} of used resource API, enum_spec is a required field, : It has a tuple of ([enum_operation]{.title-ref}, [list_operation]{.title-ref}, [extra_args]{.title-ref}). - \\`enum\\_operation\\`: the name of the GCP resource method used to retrieve the list of resources, - \\`list\\_operation\\`: the JMESPath of the field name which contains the resource list in the JSON response body, - \\`extra\\_args\\`: can be used to set up additional params for a request to GCP. id is required field, : It\\'s a field name of the response field that have to be used as resource identifier. The [id]{.title-ref} value is used for filtering. scope is optional field, default is None, : The scope of the Custodian resource. There are available 2 options: [project]{.title-ref} or [None]{.title-ref}. If the [scope]{.title-ref} has a value [project]{.title-ref} the GOOGLE_CLOUD_PROJECT variable will be used for building request to GCP resource. If the scope is [None]{.title-ref} the request to GCP is built ignoring the GOOGLE_CLOUD_PROJECT variable. parent_spec is an optional field that allows to build additional requests to parent resources, default is None. : The field is used when the request to GCP resource should be created with extra parameters that can be loaded from parent resources. The resource should extend ChildResourceManager instead of QueryResourceManager and use ChildTypeInfo instead of TypeInfo to use the field. The [parent_spec]{.title-ref} has following fields: [resource]{.title-ref}, [child_enum_params]{.title-ref}, [parent_get_params]{.title-ref}, [use_child_query]{.title-ref}. - The field [resource]{.title-ref} has value of the [resource\\_name]{.title-ref} from \\@resources.register(\\'\\<resource\\_name\\>\\') that is used for the target parent resource. - The field [child\\_enum\\_params]{.title-ref} is an array of tuples each of which maps parent instance field (first tuple element) to child\\'s list argument (second tuple element). The mappings are used for building [list]{.title-ref} requests to parent resources. It works by the next scenario. First of all it loads a list of instances from parent resource. Further it loads instances for original resources using GCP resource field values from the loaded parent resources. It uses mappings for GCP resource fields from [child\\_enum\\_params]{.title-ref}. The first field in a tuple is a field from parent resource, the second one is the mapped original resource field name. - The field [parent\\_get\\_params]{.title-ref} is an array of tuples each of which maps child instance field (first tuple element) to parent\\'s [resource\\_info]{.title-ref} field. The [resource\\_info]{.title-ref} object has fields like Stackdriver log has. There are 2 options for the fields set: either [resource.labels]{.title-ref} and [protoPayload.resourceName]{.title-ref} or a log of the full event. The mappings are used for building [get]{.title-ref} requests to parent resources. - The field [use\\_child\\_query]{.title-ref} controls whether the [query]{.title-ref} block of the current resource should be copied to its parent. An example that uses [parent_spec]{.title-ref} is available below. # the class extends ChildResourceManager @resources.register('bq-table') class BigQueryTable(ChildResourceManager): # the class extends ChildTypeInfo class resource_type(ChildTypeInfo): service = 'bigquery' # the name of the GCP service version = 'v2' # the version of the GCP service component = 'tables' # the component of the GCP service # The `list` method in the resource. https://cloud.google.com/bigquery/docs/reference/rest/v2/tables/list # It requires 2 request params: `projectId` and `datasetId`. Since the resource has a parent the params will # be extracted from parent's instance enum_spec = ('list', 'tables[]', None) scope_key = 'projectId' id = 'id' parent_spec = { # the name of Custodian parent resource. 'resource': 'bq-dataset', 'child_enum_params': [ # Extract the `datasetReference.datasetId` field from a parent instance and use its value # as the `datasetId` argument for child's `list` method (see `resource_type.enum_spec`) ('datasetReference.datasetId', 'datasetId'), ], 'parent_get_params': [ # Extract the `tableReference.projectId` field from a child instance and use its value # as the `projectId` event's field for parent's `get` method (see `resource_type.get`) ('tableReference.projectId', 'projectId'), # Similar to above ('tableReference.datasetId', 'datasetId'), ] } @staticmethod def get(client, event): return client.execute_query('get', { 'projectId': event['project_id'], 'datasetId': event['dataset_id'], 'tableId': event['resourceName'].rsplit('/', 1)[-1] }) Most resources have get methods that are created based on the corresponding [get]{.title-ref} method of the actual GCP resource. As a rule the Custodian [get]{.title-ref} method has [resource_info]{.title-ref} param. The param has fields that can be found in Stackdriver logs in [protoPayload.resourceName]{.title-ref} and [resource]{.title-ref} fields. Examples of the Stackdriver logs are available in tools/c7n_gcp/tests/data/events folder. There is an example of the resource below. from c7n_gcp.provider import resources from c7n_gcp.query import QueryResourceManager, TypeInfo @resources.register('loadbalancer-address') class LoadBalancingAddress(QueryResourceManager): class resource_type(TypeInfo): service = 'compute' component = 'addresses' version = 'v1' enum_spec = ('aggregatedList', 'items.*.addresses[]', None) scope = 'project' id = 'name' @staticmethod def get(client, resource_info): return client.execute_command('get', { 'project': resource_info['project_id'], 'region': resource_info['location'], 'address': resource_info[ 'resourceName'].rsplit('/', 1)[-1]})","title":"Create New GCP Resource"},{"location":"gcp/contribute.rst/#load-new-gcp-resource","text":"If you created a new module for a GCP service (i.e. this was the first resource implemented for this service in Custodian), then import the new service module in entry.py: entry.py . import c7n_gcp.resources.<name of a file with created resources> Each resource has to have test cases. There are implemented test cases for resources list methods and get methods.","title":"Load New GCP Resource"},{"location":"gcp/contribute.rst/#testing","text":"c7n_gcp follows the same guideance for test authoring as the core c7n code base<Creating Tests> {.interpreted-text role=\"ref\"}. Examples with relative directories (for example, tests/foo/bar ) should be applied to tools/c7n_gcp and not the root of the cloud-custodian repository. In order to execute live tests you will need to set two additional environment variables. These can be either set in your IDE or exported on the command line. The first is GOOGLE_CLOUD_PROJECT - this should point to a valid Google Cloud Project you have access to. The second, GOOGLE_APPLICATION_CREDENTIALS should be the corresponding credentials json with access to the aforementioned project. For example: export GOOGLE_CLOUD_PROJECT=cloud-custodian export GOOGLE_APPLICATION_CREDENTIALS=data/credentials.json pytest tools/c7n_gcp/tests -n auto","title":"Testing"},{"location":"gcp/contribute.rst/#updating-existing-tests","text":"Many of tests in this project were created prior to the current functional testing practices. To convert an existing test see how to convert existing tests<Converting Tests> {.interpreted-text role=\"ref\"}.","title":"Updating Existing Tests"},{"location":"gcp/gettingstarted/","text":"Getting Started (Beta) {#gcp_gettingstarted} The GCP provider (Beta) is an optional package which can be installed to enable writing policies which interact with GCP related resources. Install GCP Plugin {#gcp_install-cc} First, ensure you have installed the base Cloud Custodian application <install-cc> {.interpreted-text role=\"ref\"}. Cloud Custodian is a Python application that supports Python 2 and 3 on Linux and Windows. We recommend using Python 3.6 or higher. Once the base install is complete, you are now ready to install the GCP provider package using one of the following options: Option 1: Install released packages to local Python Environment pip install c7n pip install c7n_gcp Option 2: Install latest from the repository git clone https://github.com/cloud-custodian/cloud-custodian.git pip install -e ./cloud-custodian pip install -e ./cloud-custodian/tools/c7n_gcp Connect Your Authentication Credentials {#gcp_authenticate} In order for Custodian to be able to interact with your GCP resources, you will need to configure your GCP authentication credentials on your system in a way in which the application is able to retrieve them. Choose from one of the following methods to configure your credentials, depending on your use case. In either option, after the configuration is complete, Custodian will implicitly pick up your credentials when it runs. GCP CLI If you are a general user accessing a single account, then you can use the GCP CLI to configure your credentials. First, install gcloud (the GCP Command Line Interface). Then run the following command, substituting your username: gcloud auth application-default login Executing the command will open a browser window with prompts to finish configuring your credentials. For more information on this command, view its documentation . Environment Variables If you are planning to run Custodian using a service account, then configure your credentials using environment variables. Follow the steps outlined in the GCP documentation to configure credentials for service accounts. Write Your First Policy {#gcp_write-policy} A policy is the primary way that Custodian is configured to manage cloud resources. It is a YAML file that follows a predetermined schema to describe what you want Custodian to do. There are three main components to a policy: Resource: the type of resource to run the policy against Filters: criteria to produce a specific subset of resources Actions: directives to take on the filtered set of resources In the example below, we will write a policy that filters for compute engine resources, and then stops each resource. Filename: custodian.yml policies: - name: my-first-policy description: | Stops all compute instances that are named \"test\" resource: gcp.instance filters: - type: value key: name value: test actions: - type: stop Run Your Policy {#gcp_run-policy} First, ensure you have configured one of the supported authentication mechanisms <gcp_authenticate> {.interpreted-text role=\"ref\"}. Next, run the following command to execute the policy with Custodian: GOOGLE_CLOUD_PROJECT=\"project-id\" custodian run --output-dir=. custodian.yml If successful, you should see output similar to the following on the command line: 2016-12-20 08:35:06,133: custodian.policy:INFO Running policy my-first-policy resource: gcp.instance 2016-12-20 08:35:07,514: custodian.policy:INFO policy: my-first-policy resource: gcp.instance has count:3 time:1.38 2016-12-20 08:35:08,188: custodian.policy:INFO policy: my-first-policy action: stop: 3 execution_time: 0.67 You should also find a new my-first-policy directory with a log and other files (subsequent runs will append to the log by default, rather than overwriting it). See filters {.interpreted-text role=\"ref\"} for more information on the features of the Value filter used in this sample.","title":"Gettingstarted"},{"location":"gcp/gettingstarted/#getting-started-beta-gcp_gettingstarted","text":"The GCP provider (Beta) is an optional package which can be installed to enable writing policies which interact with GCP related resources.","title":"Getting Started (Beta) {#gcp_gettingstarted}"},{"location":"gcp/gettingstarted/#install-gcp-plugin-gcp_install-cc","text":"First, ensure you have installed the base Cloud Custodian application <install-cc> {.interpreted-text role=\"ref\"}. Cloud Custodian is a Python application that supports Python 2 and 3 on Linux and Windows. We recommend using Python 3.6 or higher. Once the base install is complete, you are now ready to install the GCP provider package using one of the following options:","title":"Install GCP Plugin {#gcp_install-cc}"},{"location":"gcp/gettingstarted/#option-1-install-released-packages-to-local-python-environment","text":"pip install c7n pip install c7n_gcp","title":"Option 1: Install released packages to local Python Environment"},{"location":"gcp/gettingstarted/#option-2-install-latest-from-the-repository","text":"git clone https://github.com/cloud-custodian/cloud-custodian.git pip install -e ./cloud-custodian pip install -e ./cloud-custodian/tools/c7n_gcp","title":"Option 2: Install latest from the repository"},{"location":"gcp/gettingstarted/#connect-your-authentication-credentials-gcp_authenticate","text":"In order for Custodian to be able to interact with your GCP resources, you will need to configure your GCP authentication credentials on your system in a way in which the application is able to retrieve them. Choose from one of the following methods to configure your credentials, depending on your use case. In either option, after the configuration is complete, Custodian will implicitly pick up your credentials when it runs.","title":"Connect Your Authentication Credentials {#gcp_authenticate}"},{"location":"gcp/gettingstarted/#gcp-cli","text":"If you are a general user accessing a single account, then you can use the GCP CLI to configure your credentials. First, install gcloud (the GCP Command Line Interface). Then run the following command, substituting your username: gcloud auth application-default login Executing the command will open a browser window with prompts to finish configuring your credentials. For more information on this command, view its documentation .","title":"GCP CLI"},{"location":"gcp/gettingstarted/#environment-variables","text":"If you are planning to run Custodian using a service account, then configure your credentials using environment variables. Follow the steps outlined in the GCP documentation to configure credentials for service accounts.","title":"Environment Variables"},{"location":"gcp/gettingstarted/#write-your-first-policy-gcp_write-policy","text":"A policy is the primary way that Custodian is configured to manage cloud resources. It is a YAML file that follows a predetermined schema to describe what you want Custodian to do. There are three main components to a policy: Resource: the type of resource to run the policy against Filters: criteria to produce a specific subset of resources Actions: directives to take on the filtered set of resources In the example below, we will write a policy that filters for compute engine resources, and then stops each resource. Filename: custodian.yml policies: - name: my-first-policy description: | Stops all compute instances that are named \"test\" resource: gcp.instance filters: - type: value key: name value: test actions: - type: stop","title":"Write Your First Policy {#gcp_write-policy}"},{"location":"gcp/gettingstarted/#run-your-policy-gcp_run-policy","text":"First, ensure you have configured one of the supported authentication mechanisms <gcp_authenticate> {.interpreted-text role=\"ref\"}. Next, run the following command to execute the policy with Custodian: GOOGLE_CLOUD_PROJECT=\"project-id\" custodian run --output-dir=. custodian.yml If successful, you should see output similar to the following on the command line: 2016-12-20 08:35:06,133: custodian.policy:INFO Running policy my-first-policy resource: gcp.instance 2016-12-20 08:35:07,514: custodian.policy:INFO policy: my-first-policy resource: gcp.instance has count:3 time:1.38 2016-12-20 08:35:08,188: custodian.policy:INFO policy: my-first-policy action: stop: 3 execution_time: 0.67 You should also find a new my-first-policy directory with a log and other files (subsequent runs will append to the log by default, rather than overwriting it). See filters {.interpreted-text role=\"ref\"} for more information on the features of the Value filter used in this sample.","title":"Run Your Policy {#gcp_run-policy}"},{"location":"gcp/gettingstarted.rst/","text":"Getting Started (Beta) {#gcp_gettingstarted} The GCP provider (Beta) is an optional package which can be installed to enable writing policies which interact with GCP related resources. Install GCP Plugin {#gcp_install-cc} First, ensure you have installed the base Cloud Custodian application <install-cc> {.interpreted-text role=\"ref\"}. Cloud Custodian is a Python application that supports Python 2 and 3 on Linux and Windows. We recommend using Python 3.6 or higher. Once the base install is complete, you are now ready to install the GCP provider package using one of the following options: Option 1: Install released packages to local Python Environment pip install c7n pip install c7n_gcp Option 2: Install latest from the repository git clone https://github.com/cloud-custodian/cloud-custodian.git pip install -e ./cloud-custodian pip install -e ./cloud-custodian/tools/c7n_gcp Connect Your Authentication Credentials {#gcp_authenticate} In order for Custodian to be able to interact with your GCP resources, you will need to configure your GCP authentication credentials on your system in a way in which the application is able to retrieve them. Choose from one of the following methods to configure your credentials, depending on your use case. In either option, after the configuration is complete, Custodian will implicitly pick up your credentials when it runs. GCP CLI If you are a general user accessing a single account, then you can use the GCP CLI to configure your credentials. First, install gcloud (the GCP Command Line Interface). Then run the following command, substituting your username: gcloud auth application-default login Executing the command will open a browser window with prompts to finish configuring your credentials. For more information on this command, view its documentation . Environment Variables If you are planning to run Custodian using a service account, then configure your credentials using environment variables. Follow the steps outlined in the GCP documentation to configure credentials for service accounts. Write Your First Policy {#gcp_write-policy} A policy is the primary way that Custodian is configured to manage cloud resources. It is a YAML file that follows a predetermined schema to describe what you want Custodian to do. There are three main components to a policy: Resource: the type of resource to run the policy against Filters: criteria to produce a specific subset of resources Actions: directives to take on the filtered set of resources In the example below, we will write a policy that filters for compute engine resources, and then stops each resource. Filename: custodian.yml policies: - name: my-first-policy description: | Stops all compute instances that are named \"test\" resource: gcp.instance filters: - type: value key: name value: test actions: - type: stop Run Your Policy {#gcp_run-policy} First, ensure you have configured one of the supported authentication mechanisms <gcp_authenticate> {.interpreted-text role=\"ref\"}. Next, run the following command to execute the policy with Custodian: GOOGLE_CLOUD_PROJECT=\"project-id\" custodian run --output-dir=. custodian.yml If successful, you should see output similar to the following on the command line: 2016-12-20 08:35:06,133: custodian.policy:INFO Running policy my-first-policy resource: gcp.instance 2016-12-20 08:35:07,514: custodian.policy:INFO policy: my-first-policy resource: gcp.instance has count:3 time:1.38 2016-12-20 08:35:08,188: custodian.policy:INFO policy: my-first-policy action: stop: 3 execution_time: 0.67 You should also find a new my-first-policy directory with a log and other files (subsequent runs will append to the log by default, rather than overwriting it). See filters {.interpreted-text role=\"ref\"} for more information on the features of the Value filter used in this sample.","title":"Gettingstarted.rst"},{"location":"gcp/gettingstarted.rst/#getting-started-beta-gcp_gettingstarted","text":"The GCP provider (Beta) is an optional package which can be installed to enable writing policies which interact with GCP related resources.","title":"Getting Started (Beta) {#gcp_gettingstarted}"},{"location":"gcp/gettingstarted.rst/#install-gcp-plugin-gcp_install-cc","text":"First, ensure you have installed the base Cloud Custodian application <install-cc> {.interpreted-text role=\"ref\"}. Cloud Custodian is a Python application that supports Python 2 and 3 on Linux and Windows. We recommend using Python 3.6 or higher. Once the base install is complete, you are now ready to install the GCP provider package using one of the following options:","title":"Install GCP Plugin {#gcp_install-cc}"},{"location":"gcp/gettingstarted.rst/#option-1-install-released-packages-to-local-python-environment","text":"pip install c7n pip install c7n_gcp","title":"Option 1: Install released packages to local Python Environment"},{"location":"gcp/gettingstarted.rst/#option-2-install-latest-from-the-repository","text":"git clone https://github.com/cloud-custodian/cloud-custodian.git pip install -e ./cloud-custodian pip install -e ./cloud-custodian/tools/c7n_gcp","title":"Option 2: Install latest from the repository"},{"location":"gcp/gettingstarted.rst/#connect-your-authentication-credentials-gcp_authenticate","text":"In order for Custodian to be able to interact with your GCP resources, you will need to configure your GCP authentication credentials on your system in a way in which the application is able to retrieve them. Choose from one of the following methods to configure your credentials, depending on your use case. In either option, after the configuration is complete, Custodian will implicitly pick up your credentials when it runs.","title":"Connect Your Authentication Credentials {#gcp_authenticate}"},{"location":"gcp/gettingstarted.rst/#gcp-cli","text":"If you are a general user accessing a single account, then you can use the GCP CLI to configure your credentials. First, install gcloud (the GCP Command Line Interface). Then run the following command, substituting your username: gcloud auth application-default login Executing the command will open a browser window with prompts to finish configuring your credentials. For more information on this command, view its documentation .","title":"GCP CLI"},{"location":"gcp/gettingstarted.rst/#environment-variables","text":"If you are planning to run Custodian using a service account, then configure your credentials using environment variables. Follow the steps outlined in the GCP documentation to configure credentials for service accounts.","title":"Environment Variables"},{"location":"gcp/gettingstarted.rst/#write-your-first-policy-gcp_write-policy","text":"A policy is the primary way that Custodian is configured to manage cloud resources. It is a YAML file that follows a predetermined schema to describe what you want Custodian to do. There are three main components to a policy: Resource: the type of resource to run the policy against Filters: criteria to produce a specific subset of resources Actions: directives to take on the filtered set of resources In the example below, we will write a policy that filters for compute engine resources, and then stops each resource. Filename: custodian.yml policies: - name: my-first-policy description: | Stops all compute instances that are named \"test\" resource: gcp.instance filters: - type: value key: name value: test actions: - type: stop","title":"Write Your First Policy {#gcp_write-policy}"},{"location":"gcp/gettingstarted.rst/#run-your-policy-gcp_run-policy","text":"First, ensure you have configured one of the supported authentication mechanisms <gcp_authenticate> {.interpreted-text role=\"ref\"}. Next, run the following command to execute the policy with Custodian: GOOGLE_CLOUD_PROJECT=\"project-id\" custodian run --output-dir=. custodian.yml If successful, you should see output similar to the following on the command line: 2016-12-20 08:35:06,133: custodian.policy:INFO Running policy my-first-policy resource: gcp.instance 2016-12-20 08:35:07,514: custodian.policy:INFO policy: my-first-policy resource: gcp.instance has count:3 time:1.38 2016-12-20 08:35:08,188: custodian.policy:INFO policy: my-first-policy action: stop: 3 execution_time: 0.67 You should also find a new my-first-policy directory with a log and other files (subsequent runs will append to the log by default, rather than overwriting it). See filters {.interpreted-text role=\"ref\"} for more information on the features of the Value filter used in this sample.","title":"Run Your Policy {#gcp_run-policy}"},{"location":"gcp/examples/","text":"Examples {#gcp_examples} These use cases provide examples of specific policies. ::: {.toctree titlesonly=\"\" glob=\"\"} ./* :::","title":"Index"},{"location":"gcp/examples/#examples-gcp_examples","text":"These use cases provide examples of specific policies. ::: {.toctree titlesonly=\"\" glob=\"\"} ./* :::","title":"Examples {#gcp_examples}"},{"location":"gcp/examples/appengine-certificates/","text":"App Engine - Check if an SSL Certificate is About to Expire Custodian can check and notify if an SSL certificate is about to expire. Note that the notify action requires a Pub/Sub topic to be configured. In the example below, the policy is set to filter certificates which expire in 60 days or less. policies: - name: appengine-certificate-age description: | Check existing certificate resource: gcp.app-engine-certificate filters: - type: value key: expireTime op: less-than value_type: expiration value: 60 actions: - type: notify subject: Certificates expiring in 60 days to: - email@address format: txt transport: type: pubsub topic: projects/my-gcp-project/topics/my-topic","title":"Appengine certificates"},{"location":"gcp/examples/appengine-certificates/#app-engine-check-if-an-ssl-certificate-is-about-to-expire","text":"Custodian can check and notify if an SSL certificate is about to expire. Note that the notify action requires a Pub/Sub topic to be configured. In the example below, the policy is set to filter certificates which expire in 60 days or less. policies: - name: appengine-certificate-age description: | Check existing certificate resource: gcp.app-engine-certificate filters: - type: value key: expireTime op: less-than value_type: expiration value: 60 actions: - type: notify subject: Certificates expiring in 60 days to: - email@address format: txt transport: type: pubsub topic: projects/my-gcp-project/topics/my-topic","title":"App Engine - Check if an SSL Certificate is About to Expire"},{"location":"gcp/examples/appengine-domain/","text":"App Engine - Check if a blacklisted domain is still in use Custodian can check and notify if there are user-defined blacklisted domains in use. Note that the notify action requires a Pub/Sub topic to be configured. In the example below, the policy checks if there are any domains contained in the &blacklisted-domains variable. vars: blacklisted-domains-in-use: &blacklisted-domains - appengine-de.mo - gcp-li.ga - whatever.com policies: - name: gcp-app-engine-domain-notify-if-blacklisted-in-use resource: gcp.app-engine-domain filters: - type: value key: id op: in value: *blacklisted-domains actions: - type: notify subject: Blacklisted domains still in use to: - email@address subject: Domains no longer in use format: txt transport: type: pubsub topic: projects/my-gcp-project/topics/my-topic","title":"Appengine domain"},{"location":"gcp/examples/appengine-domain/#app-engine-check-if-a-blacklisted-domain-is-still-in-use","text":"Custodian can check and notify if there are user-defined blacklisted domains in use. Note that the notify action requires a Pub/Sub topic to be configured. In the example below, the policy checks if there are any domains contained in the &blacklisted-domains variable. vars: blacklisted-domains-in-use: &blacklisted-domains - appengine-de.mo - gcp-li.ga - whatever.com policies: - name: gcp-app-engine-domain-notify-if-blacklisted-in-use resource: gcp.app-engine-domain filters: - type: value key: id op: in value: *blacklisted-domains actions: - type: notify subject: Blacklisted domains still in use to: - email@address subject: Domains no longer in use format: txt transport: type: pubsub topic: projects/my-gcp-project/topics/my-topic","title":"App Engine - Check if a blacklisted domain is still in use"},{"location":"gcp/examples/appengine-firewall-ingress-rule/","text":"App Engine - Check if a Firewall Rule is in Place Custodian can check and notify if App Engine firewall ingress rules have been misconfigured. Note that the notify action requires a Pub/Sub topic to be configured. In the example below, the policy checks that there is only one rule allowing all connections. policies: - name: gcp-app-engine-firewall-ingress-rule-notify-if-default-unrestricted-access resource: gcp.app-engine-firewall-ingress-rule filters: - and: - type: value value_type: resource_count op: eq value: 1 - type: value key: sourceRange value: '*' - type: value key: action value: ALLOW actions: - type: notify to: - email@address subject: App Engine has default unrestricted access format: txt transport: type: pubsub topic: projects/my-gcp-project/topics/my-topic In this variant, the policy checks if there are any firewall rules with sourceRange violating min-network-prefix-size . vars: min-network-prefix-size: &min-network-prefix-size 24 policies: - name: appengine-firewall-rules description: | Check if firewall rule network prefix size is long enough resource: gcp.app-engine-firewall-ingress-rule filters: - not: - type: value key: sourceRange op: regex # filtering out the * special character and IP addresses without network prefix length value: \"^([0-9]{1,3}\\\\.){3}[0-9]{1,3}(\\\\/([0-9]|[1-2][0-9]|3[0-2]))?$\" - type: value key: sourceRange value_type: cidr_size op: ge value: *min-network-prefix-size actions: - type: notify to: - email@address subject: A required firewall rule is missing format: txt transport: type: pubsub topic: projects/my-gcp-project/topics/my-topic","title":"Appengine firewall ingress rule"},{"location":"gcp/examples/appengine-firewall-ingress-rule/#app-engine-check-if-a-firewall-rule-is-in-place","text":"Custodian can check and notify if App Engine firewall ingress rules have been misconfigured. Note that the notify action requires a Pub/Sub topic to be configured. In the example below, the policy checks that there is only one rule allowing all connections. policies: - name: gcp-app-engine-firewall-ingress-rule-notify-if-default-unrestricted-access resource: gcp.app-engine-firewall-ingress-rule filters: - and: - type: value value_type: resource_count op: eq value: 1 - type: value key: sourceRange value: '*' - type: value key: action value: ALLOW actions: - type: notify to: - email@address subject: App Engine has default unrestricted access format: txt transport: type: pubsub topic: projects/my-gcp-project/topics/my-topic In this variant, the policy checks if there are any firewall rules with sourceRange violating min-network-prefix-size . vars: min-network-prefix-size: &min-network-prefix-size 24 policies: - name: appengine-firewall-rules description: | Check if firewall rule network prefix size is long enough resource: gcp.app-engine-firewall-ingress-rule filters: - not: - type: value key: sourceRange op: regex # filtering out the * special character and IP addresses without network prefix length value: \"^([0-9]{1,3}\\\\.){3}[0-9]{1,3}(\\\\/([0-9]|[1-2][0-9]|3[0-2]))?$\" - type: value key: sourceRange value_type: cidr_size op: ge value: *min-network-prefix-size actions: - type: notify to: - email@address subject: A required firewall rule is missing format: txt transport: type: pubsub topic: projects/my-gcp-project/topics/my-topic","title":"App Engine - Check if a Firewall Rule is in Place"},{"location":"gcp/examples/dataflow-job/","text":"Dataflow - Check for Hanged Jobs Once started, a job in the Cloud Dataflow service transits from state to state and normally enters a terminal state. Custodian can check if there are any jobs hanging in temporary statuses abnormally long. Note that the notify action requires a Pub/Sub topic to be configured. To configure Cloud Pub/Sub messaging please take a look at the gcp_genericgcpactions {.interpreted-text role=\"ref\"} page. In the example below, the policy checks if there are any jobs which started over 1 day ago (configurable period) but not yet transitioned to a certain stable state for some reason (remains in JOB_STATE_RUNNING , JOB_STATE_DRAINING , JOB_STATE_CANCELLING statuses) and therefore may need administrator\\'s attention. policies: - name: gcp-dataflow-jobs-update resource: gcp.dataflow-job filters: - type: value key: startTime op: greater-than value_type: age value: 1 - type: value key: currentState value: [JOB_STATE_RUNNING, JOB_STATE_DRAINING, JOB_STATE_CANCELLING] actions: - type: notify to: - email@address format: json transport: type: pubsub topic: projects/cloud-custodian/topics/dataflow","title":"Dataflow job"},{"location":"gcp/examples/dataflow-job/#dataflow-check-for-hanged-jobs","text":"Once started, a job in the Cloud Dataflow service transits from state to state and normally enters a terminal state. Custodian can check if there are any jobs hanging in temporary statuses abnormally long. Note that the notify action requires a Pub/Sub topic to be configured. To configure Cloud Pub/Sub messaging please take a look at the gcp_genericgcpactions {.interpreted-text role=\"ref\"} page. In the example below, the policy checks if there are any jobs which started over 1 day ago (configurable period) but not yet transitioned to a certain stable state for some reason (remains in JOB_STATE_RUNNING , JOB_STATE_DRAINING , JOB_STATE_CANCELLING statuses) and therefore may need administrator\\'s attention. policies: - name: gcp-dataflow-jobs-update resource: gcp.dataflow-job filters: - type: value key: startTime op: greater-than value_type: age value: 1 - type: value key: currentState value: [JOB_STATE_RUNNING, JOB_STATE_DRAINING, JOB_STATE_CANCELLING] actions: - type: notify to: - email@address format: json transport: type: pubsub topic: projects/cloud-custodian/topics/dataflow","title":"Dataflow - Check for Hanged Jobs"},{"location":"gcp/examples/dm-deployments/","text":"Deployment Manager - Find expired deployments Custodian can check and delete deployments that have reached their expiration date which is in turn determined by your governance rules. In the example below, the policy is set to delete deployments which were created more than 6 days ago. policies: - name: expired-deployments description: Finds expired deployments resource: gcp.dm-deployment filters: - type: value key: insertTime value_type: expiration op: gte value: 7 actions: - delete","title":"Dm deployments"},{"location":"gcp/examples/dm-deployments/#deployment-manager-find-expired-deployments","text":"Custodian can check and delete deployments that have reached their expiration date which is in turn determined by your governance rules. In the example below, the policy is set to delete deployments which were created more than 6 days ago. policies: - name: expired-deployments description: Finds expired deployments resource: gcp.dm-deployment filters: - type: value key: insertTime value_type: expiration op: gte value: 7 actions: - delete","title":"Deployment Manager - Find expired deployments"},{"location":"gcp/examples/dns-managed-zone/","text":"DNS - Notify if DNS Managed Zone has no DNSSEC A ManagedZone is a resource that represents a DNS zone hosted by the Cloud DNS service. Custodian can check if DNSSEC is disabled in DNS Managed Zone which may violate security policy of an organization. Note that the notify action requires a Pub/Sub topic to be configured. To configure Cloud Pub/Sub messaging please take a look at the gcp_genericgcpactions {.interpreted-text role=\"ref\"} page. policies: - name: gcp-dns-managed-zones-notify-if-no-dnssec resource: gcp.dns-managed-zone filters: - type: value key: dnssecConfig.state # off without quotes is treated as bool False value: \"off\" actions: - type: notify to: - email@email format: json transport: type: pubsub topic: projects/cloud-custodian/topics/dns","title":"Dns managed zone"},{"location":"gcp/examples/dns-managed-zone/#dns-notify-if-dns-managed-zone-has-no-dnssec","text":"A ManagedZone is a resource that represents a DNS zone hosted by the Cloud DNS service. Custodian can check if DNSSEC is disabled in DNS Managed Zone which may violate security policy of an organization. Note that the notify action requires a Pub/Sub topic to be configured. To configure Cloud Pub/Sub messaging please take a look at the gcp_genericgcpactions {.interpreted-text role=\"ref\"} page. policies: - name: gcp-dns-managed-zones-notify-if-no-dnssec resource: gcp.dns-managed-zone filters: - type: value key: dnssecConfig.state # off without quotes is treated as bool False value: \"off\" actions: - type: notify to: - email@email format: json transport: type: pubsub topic: projects/cloud-custodian/topics/dns","title":"DNS - Notify if DNS Managed Zone has no DNSSEC"},{"location":"gcp/examples/dns-policy/","text":"DNS - Notify if Logging is Disabled in DNS Policy A policy is a collection of DNS rules applied to one or more Virtual Private Cloud resources. Custodian can check logging state in DNS policies and report those which violate an established logging convention. Note that the notify action requires a Pub/Sub topic to be configured. To configure Cloud Pub/Sub messaging please take a look at the gcp_genericgcpactions {.interpreted-text role=\"ref\"} page. policies: - name: gcp-dns-policies-notify-if-logging-disabled resource: gcp.dns-policy filters: - type: value key: enableLogging value: false actions: - type: notify to: - email@email format: json transport: type: pubsub topic: projects/cloud-custodian/topics/dns","title":"Dns policy"},{"location":"gcp/examples/dns-policy/#dns-notify-if-logging-is-disabled-in-dns-policy","text":"A policy is a collection of DNS rules applied to one or more Virtual Private Cloud resources. Custodian can check logging state in DNS policies and report those which violate an established logging convention. Note that the notify action requires a Pub/Sub topic to be configured. To configure Cloud Pub/Sub messaging please take a look at the gcp_genericgcpactions {.interpreted-text role=\"ref\"} page. policies: - name: gcp-dns-policies-notify-if-logging-disabled resource: gcp.dns-policy filters: - type: value key: enableLogging value: false actions: - type: notify to: - email@email format: json transport: type: pubsub topic: projects/cloud-custodian/topics/dns","title":"DNS - Notify if Logging is Disabled in DNS Policy"},{"location":"gcp/examples/gce-autoscaler/","text":"Compute Engine - Enforce minimal CPU utilization target for autoscalers In the example below, the policy checks the CPU utilization target of newly added autoscalers and modifies it when needed guaranteeing the target won\\'t be less than 80%. vars: min-utilization-target: &min-utilization-target 0.8 policies: - name: gcp-autoscalers-enforced resource: gcp.autoscaler mode: type: gcp-audit methods: - v1.compute.autoscalers.insert filters: - type: value key: autoscalingPolicy.cpuUtilization.utilizationTarget op: less-than value: *min-utilization-target actions: - type: set cpuUtilization: utilizationTarget: 0.8","title":"Gce autoscaler"},{"location":"gcp/examples/gce-autoscaler/#compute-engine-enforce-minimal-cpu-utilization-target-for-autoscalers","text":"In the example below, the policy checks the CPU utilization target of newly added autoscalers and modifies it when needed guaranteeing the target won\\'t be less than 80%. vars: min-utilization-target: &min-utilization-target 0.8 policies: - name: gcp-autoscalers-enforced resource: gcp.autoscaler mode: type: gcp-audit methods: - v1.compute.autoscalers.insert filters: - type: value key: autoscalingPolicy.cpuUtilization.utilizationTarget op: less-than value: *min-utilization-target actions: - type: set cpuUtilization: utilizationTarget: 0.8","title":"Compute Engine - Enforce minimal CPU utilization target for autoscalers"},{"location":"gcp/examples/gce-instance-template/","text":"Compute Engine - Delete Instance Templates with Wrong Settings Custodian can delete Instance Templates whose settings do not match the requirements. In the example below, the policy checks if there are instance templates whose machineType setting is among the disallowed-machine-types . vars: # See https://cloud.google.com/compute/docs/machine-types disallowed-machine-types: &disallowed-machine-types - \"f1-micro\" - \"g1-small\" - \"n1-highcpu-32\" - \"n1-highcpu-64\" - \"n1-highcpu-96\" policies: - name: gcp-instance-template-delete-disallowed-machine-types resource: gcp.instance-template filters: - type: value key: properties.machineType op: in value: *disallowed-machine-types actions: - type: delete","title":"Gce instance template"},{"location":"gcp/examples/gce-instance-template/#compute-engine-delete-instance-templates-with-wrong-settings","text":"Custodian can delete Instance Templates whose settings do not match the requirements. In the example below, the policy checks if there are instance templates whose machineType setting is among the disallowed-machine-types . vars: # See https://cloud.google.com/compute/docs/machine-types disallowed-machine-types: &disallowed-machine-types - \"f1-micro\" - \"g1-small\" - \"n1-highcpu-32\" - \"n1-highcpu-64\" - \"n1-highcpu-96\" policies: - name: gcp-instance-template-delete-disallowed-machine-types resource: gcp.instance-template filters: - type: value key: properties.machineType op: in value: *disallowed-machine-types actions: - type: delete","title":"Compute Engine - Delete Instance Templates with Wrong Settings"},{"location":"gcp/examples/kms-cryptokey/","text":"Key Management System - Audit Crypto Key protection level Cloud KMS allows to create and manage cryptographic keys in one central cloud service. Custodian can audit and notify if any of KMS cryptographic keys have been created using the wrong settings. Note that the notify action requires a Pub/Sub topic to be configured. To configure Cloud Pub/Sub messaging please take a look at the gcp_genericgcpactions {.interpreted-text role=\"ref\"} page. In the example below, the policy filters and reports keys with protection level other than Hardware Security Module (HSM). policies: - name: gcp-kms-cryptokey-audit-creation resource: gcp.kms-cryptokey mode: type: gcp-audit methods: - CreateCryptoKey filters: - type: value key: primary.protectionLevel op: not-in value: - HSM actions: - type: notify to: - email@email format: json transport: type: pubsub topic: projects/my-gcp-project/topics/my-topic","title":"Kms cryptokey"},{"location":"gcp/examples/kms-cryptokey/#key-management-system-audit-crypto-key-protection-level","text":"Cloud KMS allows to create and manage cryptographic keys in one central cloud service. Custodian can audit and notify if any of KMS cryptographic keys have been created using the wrong settings. Note that the notify action requires a Pub/Sub topic to be configured. To configure Cloud Pub/Sub messaging please take a look at the gcp_genericgcpactions {.interpreted-text role=\"ref\"} page. In the example below, the policy filters and reports keys with protection level other than Hardware Security Module (HSM). policies: - name: gcp-kms-cryptokey-audit-creation resource: gcp.kms-cryptokey mode: type: gcp-audit methods: - CreateCryptoKey filters: - type: value key: primary.protectionLevel op: not-in value: - HSM actions: - type: notify to: - email@email format: json transport: type: pubsub topic: projects/my-gcp-project/topics/my-topic","title":"Key Management System - Audit Crypto Key protection level"},{"location":"gcp/examples/loadbalancer-delete-backend-buckets/","text":"Load Balancer - Delete backend buckets If a bucket was deleted it doesn\\'t mean that appropriate backend buckets were deleted. The policy allow to delete backend buckets by the name of non-existing bucket. policies: - name: gcp-loadbalancer-backend-buckets-delete resource: gcp.loadbalancer-backend-bucket filters: - type: value key: bucketName op: eq value: custodian-bucket-0 actions: - type: delete","title":"Loadbalancer delete backend buckets"},{"location":"gcp/examples/loadbalancer-delete-backend-buckets/#load-balancer-delete-backend-buckets","text":"If a bucket was deleted it doesn\\'t mean that appropriate backend buckets were deleted. The policy allow to delete backend buckets by the name of non-existing bucket. policies: - name: gcp-loadbalancer-backend-buckets-delete resource: gcp.loadbalancer-backend-bucket filters: - type: value key: bucketName op: eq value: custodian-bucket-0 actions: - type: delete","title":"Load Balancer - Delete backend buckets"},{"location":"gcp/examples/loadbalancer-network-tiers/","text":"Load Balancer - Network Tiers These examples allow to work with GCP loadbalancer-address resource. It described below how to notify to Cloud Pub/Sub information about the addresses in standard and premium tiers. To configure Cloud Pub/Sub messaging please take a look at the gcp_genericgcpactions {.interpreted-text role=\"ref\"} page. policies: - name: load-balancers-addresses-in-standard-network-tier description: | List of Load Balancers' Addresses in standard network tier resource: gcp.loadbalancer-address filters: - type: value key: networkTier value: STANDARD actions: - type: notify to: - email@email format: json transport: type: pubsub topic: projects/cloud-custodian/topics/load-balancer-resources - name: load-balancers-addresses-in-premium-network-tier description: | List of Load Balancers' Addresses in premium network tier resource: gcp.loadbalancer-address filters: - type: value key: networkTier value: PREMIUM actions: - type: notify to: - email@email format: json transport: type: pubsub topic: projects/cloud-custodian/topics/load-balancer-resources","title":"Loadbalancer network tiers"},{"location":"gcp/examples/loadbalancer-network-tiers/#load-balancer-network-tiers","text":"These examples allow to work with GCP loadbalancer-address resource. It described below how to notify to Cloud Pub/Sub information about the addresses in standard and premium tiers. To configure Cloud Pub/Sub messaging please take a look at the gcp_genericgcpactions {.interpreted-text role=\"ref\"} page. policies: - name: load-balancers-addresses-in-standard-network-tier description: | List of Load Balancers' Addresses in standard network tier resource: gcp.loadbalancer-address filters: - type: value key: networkTier value: STANDARD actions: - type: notify to: - email@email format: json transport: type: pubsub topic: projects/cloud-custodian/topics/load-balancer-resources - name: load-balancers-addresses-in-premium-network-tier description: | List of Load Balancers' Addresses in premium network tier resource: gcp.loadbalancer-address filters: - type: value key: networkTier value: PREMIUM actions: - type: notify to: - email@email format: json transport: type: pubsub topic: projects/cloud-custodian/topics/load-balancer-resources","title":"Load Balancer - Network Tiers"},{"location":"gcp/examples/loadbalancer-ssl-policy-delete-old-versions/","text":"Load Balancer - SSL Policies - Delete policies by TLS version It\\'s possible to delete all SSL Policies that don\\'t have TLS version 1.2. policies: - name: gcp-load-balancing-ssl-policies-delete resource: gcp.loadbalancer-ssl-policy filters: - type: value key: minTlsVersion op: ne value: TLS_1_2 actions: - type: delete","title":"Loadbalancer ssl policy delete old versions"},{"location":"gcp/examples/loadbalancer-ssl-policy-delete-old-versions/#load-balancer-ssl-policies-delete-policies-by-tls-version","text":"It\\'s possible to delete all SSL Policies that don\\'t have TLS version 1.2. policies: - name: gcp-load-balancing-ssl-policies-delete resource: gcp.loadbalancer-ssl-policy filters: - type: value key: minTlsVersion op: ne value: TLS_1_2 actions: - type: delete","title":"Load Balancer - SSL Policies - Delete policies by TLS version"},{"location":"gcp/examples/pubsub-snapshots/","text":"Pub/Sub - Early Detection of Obsolete Snapshots In Cloud Pub/Sub, the snapshot feature allows users to capture the message acknowledgment state of a subscription to a topic. Once a snapshot is created, it retains all messages that were unacknowledged in the source subscription (at the time of the snapshot\\'s creation). All pubsub snapshots are deleted in a week or less of their creation. For some use cases it maybe useful to delete them earlier. Note that the notify action requires a Pub/Sub topic to be configured. To configure Cloud Pub/Sub messaging please take a look at the gcp_genericgcpactions {.interpreted-text role=\"ref\"} page. In the example below, the policy reports existing snapshots whose topics have been deleted, therefore snapshots may need prompt deletion as well. policies: - name: gcp-pub-sub-snapshots-notify-if-topic-deleted resource: gcp.pubsub-snapshot filters: - type: value key: topic value: _deleted-topic_ actions: - type: notify to: - email@address format: txt transport: type: pubsub topic: projects/my-gcp-project/topics/my-topic","title":"Pubsub snapshots"},{"location":"gcp/examples/pubsub-snapshots/#pubsub-early-detection-of-obsolete-snapshots","text":"In Cloud Pub/Sub, the snapshot feature allows users to capture the message acknowledgment state of a subscription to a topic. Once a snapshot is created, it retains all messages that were unacknowledged in the source subscription (at the time of the snapshot\\'s creation). All pubsub snapshots are deleted in a week or less of their creation. For some use cases it maybe useful to delete them earlier. Note that the notify action requires a Pub/Sub topic to be configured. To configure Cloud Pub/Sub messaging please take a look at the gcp_genericgcpactions {.interpreted-text role=\"ref\"} page. In the example below, the policy reports existing snapshots whose topics have been deleted, therefore snapshots may need prompt deletion as well. policies: - name: gcp-pub-sub-snapshots-notify-if-topic-deleted resource: gcp.pubsub-snapshot filters: - type: value key: topic value: _deleted-topic_ actions: - type: notify to: - email@address format: txt transport: type: pubsub topic: projects/my-gcp-project/topics/my-topic","title":"Pub/Sub - Early Detection of Obsolete Snapshots"},{"location":"gcp/examples/pubsub-subscriptions/","text":"Pub/Sub - Audit Subscriptions to Match Requirements In Cloud Pub/Sub, subscriptions connect a topic to a subscriber application that receives and processes messages published to the topic. Custodian can find Pub/Sub subscriptions whose settings do not match the required ones. Note that the notify action requires a Pub/Sub topic to be configured. To configure Cloud Pub/Sub messaging please take a look at the gcp_genericgcpactions {.interpreted-text role=\"ref\"} page. In the example below, users are notified if the resources appearing in the logs with CreateSubscription or UpdateSubscription action have expiration policy unset. policies: - name: gcp-pub-sub-subscription-audit resource: gcp.pubsub-subscription mode: type: gcp-audit methods: - \"google.pubsub.v1.Subscriber.CreateSubscription\" - \"google.pubsub.v1.Subscriber.UpdateSubscription\" filters: - type: value key: expirationPolicy.ttl value: actions: - type: notify to: - email@address format: txt transport: type: pubsub topic: projects/my-gcp-project/topics/my-topic","title":"Pubsub subscriptions"},{"location":"gcp/examples/pubsub-subscriptions/#pubsub-audit-subscriptions-to-match-requirements","text":"In Cloud Pub/Sub, subscriptions connect a topic to a subscriber application that receives and processes messages published to the topic. Custodian can find Pub/Sub subscriptions whose settings do not match the required ones. Note that the notify action requires a Pub/Sub topic to be configured. To configure Cloud Pub/Sub messaging please take a look at the gcp_genericgcpactions {.interpreted-text role=\"ref\"} page. In the example below, users are notified if the resources appearing in the logs with CreateSubscription or UpdateSubscription action have expiration policy unset. policies: - name: gcp-pub-sub-subscription-audit resource: gcp.pubsub-subscription mode: type: gcp-audit methods: - \"google.pubsub.v1.Subscriber.CreateSubscription\" - \"google.pubsub.v1.Subscriber.UpdateSubscription\" filters: - type: value key: expirationPolicy.ttl value: actions: - type: notify to: - email@address format: txt transport: type: pubsub topic: projects/my-gcp-project/topics/my-topic","title":"Pub/Sub - Audit Subscriptions to Match Requirements"},{"location":"gcp/examples/spanner-drop-databases/","text":"Spanner - Drop Databases This policy drops (deletes) databases that have [dev]{.title-ref} in the name and then notifies about the action taken via an email. To configure Cloud Pub/Sub messaging please take a look at the gcp_genericgcpactions {.interpreted-text role=\"ref\"} page. policies: - name: gcp-spanner-instance-databases-delete-and-notify resource: gcp.spanner-database-instance filters: - type: value key: name op: contains value: dev actions: - type: delete - type: notify subject: The following databases were dropped to: - email@address transport: type: pubsub topic: projects/cloud-custodian/topics/demo-notifications","title":"Spanner drop databases"},{"location":"gcp/examples/spanner-drop-databases/#spanner-drop-databases","text":"This policy drops (deletes) databases that have [dev]{.title-ref} in the name and then notifies about the action taken via an email. To configure Cloud Pub/Sub messaging please take a look at the gcp_genericgcpactions {.interpreted-text role=\"ref\"} page. policies: - name: gcp-spanner-instance-databases-delete-and-notify resource: gcp.spanner-database-instance filters: - type: value key: name op: contains value: dev actions: - type: delete - type: notify subject: The following databases were dropped to: - email@address transport: type: pubsub topic: projects/cloud-custodian/topics/demo-notifications","title":"Spanner - Drop Databases"},{"location":"gcp/examples/spanner-reduce-instances-count/","text":"Spanner - Reduce Count of Instance Nodes This policy reduces the node count to 1 node for a spanner instance and then notifies about the action taken via an email. To configure Cloud Pub/Sub messaging please take a look at the gcp_genericgcpactions {.interpreted-text role=\"ref\"} page. policies: - name: gcp-spanner-instances-change-node-count resource: gcp.spanner-instance filters: - type: value key: nodeCount op: gte value: 2 actions: - type: set nodeCount: 1 - type: notify subject: The node count for spanner instances was updated to: - email@address transport: type: pubsub topic: projects/cloud-custodian/topics/demo-notifications","title":"Spanner reduce instances count"},{"location":"gcp/examples/spanner-reduce-instances-count/#spanner-reduce-count-of-instance-nodes","text":"This policy reduces the node count to 1 node for a spanner instance and then notifies about the action taken via an email. To configure Cloud Pub/Sub messaging please take a look at the gcp_genericgcpactions {.interpreted-text role=\"ref\"} page. policies: - name: gcp-spanner-instances-change-node-count resource: gcp.spanner-instance filters: - type: value key: nodeCount op: gte value: 2 actions: - type: set nodeCount: 1 - type: notify subject: The node count for spanner instances was updated to: - email@address transport: type: pubsub topic: projects/cloud-custodian/topics/demo-notifications","title":"Spanner - Reduce Count of Instance Nodes"},{"location":"gcp/examples/spanner-set-iam-policy/","text":"Spanner - Set IAM Policies These policies update the IAM policy for spanner instances ([add-bindings]{.title-ref}) and databases ([remove-bindings]{.title-ref}), respectively. policies: - name: gcp-spanner-instances-set-iam-policy resource: gcp.spanner-instance actions: - type: set-iam-policy add-bindings: - members: - user:user1@test.com - user:user2@test.com role: roles/owner - members: - user:user3@gmail.com role: roles/viewer - name: gcp-spanner-database-instances-set-iam-policy resource: gcp.spanner-database-instance actions: - type: set-iam-policy remove-bindings: - members: \"*\" role: roles/owner - members: - user:user3@gmail.com role: roles/viewer","title":"Spanner set iam policy"},{"location":"gcp/examples/spanner-set-iam-policy/#spanner-set-iam-policies","text":"These policies update the IAM policy for spanner instances ([add-bindings]{.title-ref}) and databases ([remove-bindings]{.title-ref}), respectively. policies: - name: gcp-spanner-instances-set-iam-policy resource: gcp.spanner-instance actions: - type: set-iam-policy add-bindings: - members: - user:user1@test.com - user:user2@test.com role: roles/owner - members: - user:user3@gmail.com role: roles/viewer - name: gcp-spanner-database-instances-set-iam-policy resource: gcp.spanner-database-instance actions: - type: set-iam-policy remove-bindings: - members: \"*\" role: roles/owner - members: - user:user3@gmail.com role: roles/viewer","title":"Spanner - Set IAM Policies"},{"location":"gcp/examples/sql-backupruns/","text":"Cloud SQL - List Unsucessful Backups Older Than N Days The following example demonstrates ability of Cloud Custodian to track backup runs of Cloud SQL instances and list unsuccessful backups (if any) older than 5 days. policies: - name: sql-backup-run description: | check basic work of Cloud SQL filter on backup runs: lists unsucessful backups older than 5 days resource: gcp.sql-backup-run filters: - type: value key: status op: not-equal value: SUCCESSFUL - type: value key: endTime op: greater-than value_type: age value: 5 actions: - type: notify to: - email@address # address doesnt matter format: txt transport: type: pubsub topic: projects/river-oxygen-233508/topics/first","title":"Sql backupruns"},{"location":"gcp/examples/sql-backupruns/#cloud-sql-list-unsucessful-backups-older-than-n-days","text":"The following example demonstrates ability of Cloud Custodian to track backup runs of Cloud SQL instances and list unsuccessful backups (if any) older than 5 days. policies: - name: sql-backup-run description: | check basic work of Cloud SQL filter on backup runs: lists unsucessful backups older than 5 days resource: gcp.sql-backup-run filters: - type: value key: status op: not-equal value: SUCCESSFUL - type: value key: endTime op: greater-than value_type: age value: 5 actions: - type: notify to: - email@address # address doesnt matter format: txt transport: type: pubsub topic: projects/river-oxygen-233508/topics/first","title":"Cloud SQL - List Unsucessful Backups Older Than N Days"},{"location":"gcp/examples/sql-instance/","text":"Cloud SQL - Check Regions of Instances and Their State Execution of the following policy returns instances which are not in an approved set of regions AND not in runnable state. You may use more complex logic to combine any condition you need. policies: - name: sql-instance description: | check basic work of Cloud SQL filter on instances: returns instances which are not in an approved set of regions AND not in runnable state resource: gcp.sql-instance filters: - type: value key: region op: not-in value: [europe-west1, europe-west2] - type: value key: state op: not-equal value: RUNNABLE actions: - type: notify to: - email@address # address doesnt matter format: txt transport: type: pubsub topic: projects/river-oxygen-233508/topics/first","title":"Sql instance"},{"location":"gcp/examples/sql-instance/#cloud-sql-check-regions-of-instances-and-their-state","text":"Execution of the following policy returns instances which are not in an approved set of regions AND not in runnable state. You may use more complex logic to combine any condition you need. policies: - name: sql-instance description: | check basic work of Cloud SQL filter on instances: returns instances which are not in an approved set of regions AND not in runnable state resource: gcp.sql-instance filters: - type: value key: region op: not-in value: [europe-west1, europe-west2] - type: value key: state op: not-equal value: RUNNABLE actions: - type: notify to: - email@address # address doesnt matter format: txt transport: type: pubsub topic: projects/river-oxygen-233508/topics/first","title":"Cloud SQL - Check Regions of Instances and Their State"},{"location":"gcp/examples/sql-sslcerts/","text":"Cloud SQL - Notify on Certificates Which Are About to Expire In the example below, Custodian will track SSL certificates which are in use by your Cloud SQL instances and notify about the ones which are going to expire in 60 days or less. policies: - name: sql-ssl-cert description: | check basic work of Cloud SQL filter on SSL certificates: returns certs which are about to expire in 60 days or less resource: gcp.sql-ssl-cert filters: - type: value key: expirationTime op: less-than value_type: expiration value: 60 actions: - type: notify to: - email@address # address doesnt matter format: txt transport: type: pubsub topic: projects/river-oxygen-233508/topics/first","title":"Sql sslcerts"},{"location":"gcp/examples/sql-sslcerts/#cloud-sql-notify-on-certificates-which-are-about-to-expire","text":"In the example below, Custodian will track SSL certificates which are in use by your Cloud SQL instances and notify about the ones which are going to expire in 60 days or less. policies: - name: sql-ssl-cert description: | check basic work of Cloud SQL filter on SSL certificates: returns certs which are about to expire in 60 days or less resource: gcp.sql-ssl-cert filters: - type: value key: expirationTime op: less-than value_type: expiration value: 60 actions: - type: notify to: - email@address # address doesnt matter format: txt transport: type: pubsub topic: projects/river-oxygen-233508/topics/first","title":"Cloud SQL - Notify on Certificates Which Are About to Expire"},{"location":"gcp/examples/sql-users/","text":"Cloud SQL - Check Users One of security best practices is to control list of your users with extended permissions (e.g. \\'postgresql\\', \\'root\\', etc). In the example below, Custodian lists existing users which are not included into an approved set. policies: - name: sql-user description: | check basic work of Cloud SQL filter on users: lists instance superusers which are not included into a standard user set resource: gcp.sql-user filters: - type: value key: name op: not-in value: [postgres, jamesbond] actions: - type: notify to: - email@address # address doesnt matter format: txt transport: type: pubsub topic: projects/river-oxygen-233508/topics/first","title":"Sql users"},{"location":"gcp/examples/sql-users/#cloud-sql-check-users","text":"One of security best practices is to control list of your users with extended permissions (e.g. \\'postgresql\\', \\'root\\', etc). In the example below, Custodian lists existing users which are not included into an approved set. policies: - name: sql-user description: | check basic work of Cloud SQL filter on users: lists instance superusers which are not included into a standard user set resource: gcp.sql-user filters: - type: value key: name op: not-in value: [postgres, jamesbond] actions: - type: notify to: - email@address # address doesnt matter format: txt transport: type: pubsub topic: projects/river-oxygen-233508/topics/first","title":"Cloud SQL - Check Users"},{"location":"gcp/policy/","text":"Policies {#gcp_policies} Here are some more specific example policies for these resources: ::: {.toctree maxdepth=\"2\" titlesonly=\"\" glob=\"\"} genericgcpactions resources/* :::","title":"Index"},{"location":"gcp/policy/#policies-gcp_policies","text":"Here are some more specific example policies for these resources: ::: {.toctree maxdepth=\"2\" titlesonly=\"\" glob=\"\"} genericgcpactions resources/* :::","title":"Policies {#gcp_policies}"},{"location":"gcp/policy/genericgcpactions/","text":"Generic Actions {#gcp_genericgcpactions} This action can be applied to any GCP resource type. Notify Notify : Add notify message into Cloud Pub/Sub. To check below example please use created GCP project and configured Pub/Sub topic. ``` {.yaml} policies: - name: gcp-notify-first-example description: | Example of notify action resource: gcp.loadbalancer-address actions: - type: notify to: - email@email format: json transport: type: pubsub topic: projects/<name of a project>/topics/<name of a topic> ``` To run the example please make sure the project is configured, Pub/Sub topic is created and configured, Cloud function for messages decoding is created. Pub/Sub system is able to provide information about message in encoded format. One of the ways how to make the message readable is using a decoding cloud function. If the messaging system is not configured please follow next steps: create a project in GCP or use created one (name of the project use instead of \\<name of a project> in the policy), open Pub/Sub page , create a topic (name of the topic use instead of \\<name of a topic> in the policy), open Cloud Function page create a function with following params: Name: name of the function, Topic: \\<name of the topic>, Memory allocated: 128 MB, Runtime: Python 3.7, Function to execute: print_decoded, main.py: ``` {.python} import base64 import zlib def print_decoded(event, context): decoded_compressed = base64.b64decode(event['data']) decompressed = zlib.decompress(decoded_compressed) print(decompressed) ``` After the steps are executed please open Log Viewer . Select created function in the resource combobox. It should be available in Cloud Function -> name of the function section. If the above policy is run new decoded message will appear in logs.","title":"Genericgcpactions"},{"location":"gcp/policy/genericgcpactions/#generic-actions-gcp_genericgcpactions","text":"This action can be applied to any GCP resource type.","title":"Generic Actions {#gcp_genericgcpactions}"},{"location":"gcp/policy/genericgcpactions/#notify","text":"Notify : Add notify message into Cloud Pub/Sub. To check below example please use created GCP project and configured Pub/Sub topic. ``` {.yaml} policies: - name: gcp-notify-first-example description: | Example of notify action resource: gcp.loadbalancer-address actions: - type: notify to: - email@email format: json transport: type: pubsub topic: projects/<name of a project>/topics/<name of a topic> ``` To run the example please make sure the project is configured, Pub/Sub topic is created and configured, Cloud function for messages decoding is created. Pub/Sub system is able to provide information about message in encoded format. One of the ways how to make the message readable is using a decoding cloud function. If the messaging system is not configured please follow next steps: create a project in GCP or use created one (name of the project use instead of \\<name of a project> in the policy), open Pub/Sub page , create a topic (name of the topic use instead of \\<name of a topic> in the policy), open Cloud Function page create a function with following params: Name: name of the function, Topic: \\<name of the topic>, Memory allocated: 128 MB, Runtime: Python 3.7, Function to execute: print_decoded, main.py: ``` {.python} import base64 import zlib def print_decoded(event, context): decoded_compressed = base64.b64decode(event['data']) decompressed = zlib.decompress(decoded_compressed) print(decompressed) ``` After the steps are executed please open Log Viewer . Select created function in the resource combobox. It should be available in Cloud Function -> name of the function section. If the above policy is run new decoded message will appear in logs.","title":"Notify"},{"location":"gcp/policy/resources/loadbalancer/","text":"Load Balancer {#gcp_loadbalancer}","title":"Loadbalancer"},{"location":"gcp/policy/resources/loadbalancer/#load-balancer-gcp_loadbalancer","text":"","title":"Load Balancer {#gcp_loadbalancer}"},{"location":"quickstart/","text":"Getting Started {#quickstart} See also the readme in the GitHub repository. install-cc {.interpreted-text role=\"ref\"} explore-cc {.interpreted-text role=\"ref\"} cloud-providers {.interpreted-text role=\"ref\"} monitor-cc {.interpreted-text role=\"ref\"} tab-completion {.interpreted-text role=\"ref\"} community {.interpreted-text role=\"ref\"} Install Cloud Custodian {#install-cc} These instructions will install Cloud Custodian. Cloud Custodian is a Python application that supports Python 3 on Linux, MacOS and Windows. We recommend using Python 3.6 or higher. NOTE: Ensure you install the correct follow-on package depending on the cloud you are deploying to, otherwise you won\\'t have the right modules for that specific cloud. Linux and Mac OS To install Cloud Custodian : python3 -m venv custodian source custodian/bin/activate pip install c7n # This includes AWS support To install Cloud Custodian for Azure, you will also need to run: pip install c7n_azure # Install Azure package To install Cloud Custodian for GCP, you will also need to run: pip install c7n_gcp # Install GCP Package Windows (CMD/PowerShell) To install Cloud Custodian run: python3 -m venv custodian ./custodian/bin/activate pip install c7n # This includes AWS support To install Cloud Custodian for Azure, you will also need to run: pip install c7n_azure To install Cloud Custodian for GCP, you will also need to run: pip install c7n_gcp Docker To install via docker, run: docker pull cloudcustodian/c7n You\\'ll need to export cloud provider credentials to the container when executing. One example, if you\\'re using environment variables for provider credentials: docker run -it \\ -v $(pwd)/output:/home/custodian/output \\ -v $(pwd)/policy.yml:/home/custodian/policy.yml \\ --env-file <(env | grep \"^AWS\\|^AZURE\\|^GOOGLE\") \\ cloudcustodian/c7n run -v -s /home/custodian/output /home/custodian/policy.yml Explore Cloud Custodian {#explore-cc} Run custodian -h to see a list of available commands. Run custodian schema to see the complete list of cloud resources against which you can run policies. To invoke command-line help with more information about policy schema details, run custodian schema -h . Run custodian schema <cloud-provider> to see the available resources for a specific cloud provider: custodian schema aws Run custodian schema <cloud-provider>.<resource> to see the available filters and actions for each resource. Drill down to get more information about available policy settings for each resource, where the model for the command is: custodian schema <cloud>.<resource>.<category>.<item> For example: custodian schema aws.s3.filters.is-log-target provides the following information: Help ---- Filter and return buckets are log destinations. Not suitable for use in lambda on large accounts, This is a api heavy process to detect scan all possible log sources. Sources: - elb (Access Log) - s3 (Access Log) - cfn (Template writes) - cloudtrail :example: .. code-block: yaml policies: - name: s3-log-bucket resource: s3 filters: - type: is-log-target Schema ------ { 'additionalProperties': False, 'properties': { 'type': { 'enum': ['is-log-target']}, 'value': { 'type': 'boolean'}}, 'required': ['type'], 'type': 'object'} Additionally, you can use the schema command to view information on the different supported modes in Cloud Custodian: custodian schema mode Cloud Provider Specific Help {#cloud-providers} For specific setup isntructions for AWS, Azure, and GCP, visit the relevant getting started page. AWS <aws-gettingstarted> {.interpreted-text role=\"ref\"} Azure <azure_gettingstarted> {.interpreted-text role=\"ref\"} GCP <gcp_gettingstarted> {.interpreted-text role=\"ref\"} Troubleshooting & Tinkering The policy is validated automatically when you run it, but you can also validate it separately: custodian validate custodian.yml You can also check which resources are identified by the policy, without running any actions on the resources: custodian run --dryrun -s . custodian.yml Monitor resources {#monitor-cc} Additional commands let you monitor your services in detail. You can generate metrics, log outputs, and output to blob storage in each of the different providers (AWS, Azure, Google Cloud Platform). For detailed instructions on how to add metrics, logging, and blob storage output for the different clouds, check out the cloud provider specific pages: AWS <aws-gettingstarted> {.interpreted-text role=\"ref\"} Azure <azure_gettingstarted> {.interpreted-text role=\"ref\"} GCP <gcp_gettingstarted> {.interpreted-text role=\"ref\"} For details, see usage {.interpreted-text role=\"ref\"}. Editor Integration If your preferred editor supports language servers, you can configure it to provide completion and validation while authoring policies. First generate use custodian to generate a json schema file: custodian schema --json > schema.json Next install a YAML plug-in for your editor, like YAML for Visual Studio Code or coc-yaml for coc.nvim . Both plug-ins use the yaml-language-server under the hood. You\\'ll then need to configure your plug-in to use the generated [schema.json]{.title-ref} as the schema for your policy files. For example in Visual Studio Code, navigate to the settings for the YAML plug-in and under Schemas, edit configuration file and add the following schema configuration: \"yaml.schemas\": { \"./schema.json\": \"*yml\" }, Note the path to schema.json can either be either relative or the full path. You\\'ll now have completion and validation while authoring policies. Note if you\\'re authoring policies in json you can also configure the json-language-server for the same. Also, if you\\'re seeing errors like 'Request textDocument/hover failed with message: Cannot read property '$ref' of null' try re-creating your schema.json file. Tab Completion To enable command-line tab completion for [custodian]{.title-ref} on bash do the following one-time steps: Run: activate-global-python-argcomplete Now launch a new shell (or refresh your bash environment by sourcing the appropriate file). Community Resources We have a regular community meeting that is open to all users and developers of every skill level. Joining the mailing list https://groups.google.com/forum/#!forum/cloud-custodian will automatically send you a meeting invite. See the notes below for more technical information on joining the meeting. Community Meeting Videos Community Meeting Notes Archive Troubleshooting If you get an error about \\\"complete -D\\\" not being supported, you need to update bash. See the \\\"Base Version Compatability\\\" note in the argcomplete docs : If you have other errors, or for tcsh support, see the argcomplete docs . If you are invoking [custodian]{.title-ref} via the [python]{.title-ref} executable tab completion will not work. You must invoke [custodian]{.title-ref} directly.","title":"Index"},{"location":"quickstart/#getting-started-quickstart","text":"See also the readme in the GitHub repository. install-cc {.interpreted-text role=\"ref\"} explore-cc {.interpreted-text role=\"ref\"} cloud-providers {.interpreted-text role=\"ref\"} monitor-cc {.interpreted-text role=\"ref\"} tab-completion {.interpreted-text role=\"ref\"} community {.interpreted-text role=\"ref\"}","title":"Getting Started {#quickstart}"},{"location":"quickstart/#install-cloud-custodian-install-cc","text":"These instructions will install Cloud Custodian. Cloud Custodian is a Python application that supports Python 3 on Linux, MacOS and Windows. We recommend using Python 3.6 or higher. NOTE: Ensure you install the correct follow-on package depending on the cloud you are deploying to, otherwise you won\\'t have the right modules for that specific cloud.","title":"Install Cloud Custodian {#install-cc}"},{"location":"quickstart/#linux-and-mac-os","text":"To install Cloud Custodian : python3 -m venv custodian source custodian/bin/activate pip install c7n # This includes AWS support To install Cloud Custodian for Azure, you will also need to run: pip install c7n_azure # Install Azure package To install Cloud Custodian for GCP, you will also need to run: pip install c7n_gcp # Install GCP Package","title":"Linux and Mac OS"},{"location":"quickstart/#windows-cmdpowershell","text":"To install Cloud Custodian run: python3 -m venv custodian ./custodian/bin/activate pip install c7n # This includes AWS support To install Cloud Custodian for Azure, you will also need to run: pip install c7n_azure To install Cloud Custodian for GCP, you will also need to run: pip install c7n_gcp","title":"Windows (CMD/PowerShell)"},{"location":"quickstart/#docker","text":"To install via docker, run: docker pull cloudcustodian/c7n You\\'ll need to export cloud provider credentials to the container when executing. One example, if you\\'re using environment variables for provider credentials: docker run -it \\ -v $(pwd)/output:/home/custodian/output \\ -v $(pwd)/policy.yml:/home/custodian/policy.yml \\ --env-file <(env | grep \"^AWS\\|^AZURE\\|^GOOGLE\") \\ cloudcustodian/c7n run -v -s /home/custodian/output /home/custodian/policy.yml","title":"Docker"},{"location":"quickstart/#explore-cloud-custodian-explore-cc","text":"Run custodian -h to see a list of available commands. Run custodian schema to see the complete list of cloud resources against which you can run policies. To invoke command-line help with more information about policy schema details, run custodian schema -h . Run custodian schema <cloud-provider> to see the available resources for a specific cloud provider: custodian schema aws Run custodian schema <cloud-provider>.<resource> to see the available filters and actions for each resource. Drill down to get more information about available policy settings for each resource, where the model for the command is: custodian schema <cloud>.<resource>.<category>.<item> For example: custodian schema aws.s3.filters.is-log-target provides the following information: Help ---- Filter and return buckets are log destinations. Not suitable for use in lambda on large accounts, This is a api heavy process to detect scan all possible log sources. Sources: - elb (Access Log) - s3 (Access Log) - cfn (Template writes) - cloudtrail :example: .. code-block: yaml policies: - name: s3-log-bucket resource: s3 filters: - type: is-log-target Schema ------ { 'additionalProperties': False, 'properties': { 'type': { 'enum': ['is-log-target']}, 'value': { 'type': 'boolean'}}, 'required': ['type'], 'type': 'object'} Additionally, you can use the schema command to view information on the different supported modes in Cloud Custodian: custodian schema mode","title":"Explore Cloud Custodian {#explore-cc}"},{"location":"quickstart/#cloud-provider-specific-help-cloud-providers","text":"For specific setup isntructions for AWS, Azure, and GCP, visit the relevant getting started page. AWS <aws-gettingstarted> {.interpreted-text role=\"ref\"} Azure <azure_gettingstarted> {.interpreted-text role=\"ref\"} GCP <gcp_gettingstarted> {.interpreted-text role=\"ref\"}","title":"Cloud Provider Specific Help {#cloud-providers}"},{"location":"quickstart/#troubleshooting-tinkering","text":"The policy is validated automatically when you run it, but you can also validate it separately: custodian validate custodian.yml You can also check which resources are identified by the policy, without running any actions on the resources: custodian run --dryrun -s . custodian.yml","title":"Troubleshooting &amp; Tinkering"},{"location":"quickstart/#monitor-resources-monitor-cc","text":"Additional commands let you monitor your services in detail. You can generate metrics, log outputs, and output to blob storage in each of the different providers (AWS, Azure, Google Cloud Platform). For detailed instructions on how to add metrics, logging, and blob storage output for the different clouds, check out the cloud provider specific pages: AWS <aws-gettingstarted> {.interpreted-text role=\"ref\"} Azure <azure_gettingstarted> {.interpreted-text role=\"ref\"} GCP <gcp_gettingstarted> {.interpreted-text role=\"ref\"} For details, see usage {.interpreted-text role=\"ref\"}.","title":"Monitor resources {#monitor-cc}"},{"location":"quickstart/#editor-integration","text":"If your preferred editor supports language servers, you can configure it to provide completion and validation while authoring policies. First generate use custodian to generate a json schema file: custodian schema --json > schema.json Next install a YAML plug-in for your editor, like YAML for Visual Studio Code or coc-yaml for coc.nvim . Both plug-ins use the yaml-language-server under the hood. You\\'ll then need to configure your plug-in to use the generated [schema.json]{.title-ref} as the schema for your policy files. For example in Visual Studio Code, navigate to the settings for the YAML plug-in and under Schemas, edit configuration file and add the following schema configuration: \"yaml.schemas\": { \"./schema.json\": \"*yml\" }, Note the path to schema.json can either be either relative or the full path. You\\'ll now have completion and validation while authoring policies. Note if you\\'re authoring policies in json you can also configure the json-language-server for the same. Also, if you\\'re seeing errors like 'Request textDocument/hover failed with message: Cannot read property '$ref' of null' try re-creating your schema.json file.","title":"Editor Integration"},{"location":"quickstart/#tab-completion","text":"To enable command-line tab completion for [custodian]{.title-ref} on bash do the following one-time steps: Run: activate-global-python-argcomplete Now launch a new shell (or refresh your bash environment by sourcing the appropriate file).","title":"Tab Completion"},{"location":"quickstart/#community-resources","text":"We have a regular community meeting that is open to all users and developers of every skill level. Joining the mailing list https://groups.google.com/forum/#!forum/cloud-custodian will automatically send you a meeting invite. See the notes below for more technical information on joining the meeting. Community Meeting Videos Community Meeting Notes Archive","title":"Community Resources"},{"location":"quickstart/#troubleshooting","text":"If you get an error about \\\"complete -D\\\" not being supported, you need to update bash. See the \\\"Base Version Compatability\\\" note in the argcomplete docs : If you have other errors, or for tcsh support, see the argcomplete docs . If you are invoking [custodian]{.title-ref} via the [python]{.title-ref} executable tab completion will not work. You must invoke [custodian]{.title-ref} directly.","title":"Troubleshooting"},{"location":"quickstart/advanced/","text":"Advanced Usage {#advanced} run-multiple-regions {.interpreted-text role=\"ref\"} report-multiple-regions {.interpreted-text role=\"ref\"} report-custom-fields {.interpreted-text role=\"ref\"} policy_resource_limits {.interpreted-text role=\"ref\"} Running against multiple regions {#run-multiple-regions} By default Cloud Custodian determines the region to run against in the following order: the --region flag the AWS_DEFAULT_REGION environment variable the region set in the ~/.aws/config file It is possible to run policies against multiple regions by specifying the --region flag multiple times: custodian run -s out --region us-east-1 --region us-west-1 policy.yml If a supplied region does not support the resource for a given policy that region will be skipped. The special all keyword can be used in place of a region to specify the policy should run against all applicable regions for the policy\\'s resource: custodian run -s out --region all policy.yml Note: when running reports against multiple regions the output is placed in a different directory than when running against a single region. See the multi-region reporting section below. Reporting against multiple regions {#report-multiple-regions} When running against multiple regions the output files are placed in a different location that when running against a single region. When generating a report, specify multiple regions the same way as with the run command: custodian report -s out --region us-east-1 --region-us-west-1 policy.yml A region column will be added to reports generated that include multiple regions to indicate which region each row is from. Conditional Policy Execution {#scheduling-policy-execution} Cloud Custodian can skip policies that are included in a policy file when running if the policy specifies conditions that aren\\'t met by the current environment. The available environment keys are Key Description name Name of the policy region Region the policy is being evaluated in. resource The resource type of the policy. account_id The account id (subscription, project) the policy is being evaluated in. provider The name of the cloud provider (aws, azure, gcp, etc) policy The policy data as structure now The current time event In serverless, the event that triggered the policy account When running in c7n-org, current account info per account config file If a policy is executing in a serverless mode the triggering event is available. As an example, one can set up policy conditions to only execute between a given set of dates. policies: # other compliance related policies that # should always be running... - name: holiday-break-stop description: | This policy will stop all EC2 instances if the current date is between 12-15-2018 to 12-31-2018 when the policy is run. Use this in conjunction with a cron job to ensure that the environment is fully turned off during the break. resource: ec2 conditions: - type: value key: now op: greater-than value_type: date value: \"2018-12-15\" - type: value key: now op: less-than value_type: date value: \"2018-12-31\" filters: - \"tag:holiday-off-hours\": present actions: - stop - name: holiday-break-start description: | This policy will start up all EC2 instances and only run on 1-1-2019. resource: ec2 conditions: - type: value key: now value_type: date op: greater-than value: \"2009-1-1\" - type: value key: now value_type: date op: less-than value: \"2019-1-1 23:59:59\" filters: - \"tag:holiday-off-hours\": present actions: - start Limiting how many resources custodian affects {#policy_resource_limits} Custodian by default will operate on as many resources exist within an environment that match a policy\\'s filters. Custodian also allows policy authors to stop policy execution if a policy affects more resources than expected, either as a number of resources or as a percentage of total extant resources. policies: - name: log-delete description: | This policy will delete all log groups that haven't been written to in 5 days. As a safety belt, it will stop execution if the number of log groups that would be affected is more than 5% of the total log groups in the account's region. resource: aws.log-group max-resources-percent: 5 filters: - type: last-write days: 5 actions: - delete Max resources can also be specified as an absolute number using [max-resources]{.title-ref} specified on a policy. When executing if the limit is exceeded, policy execution is stopped before taking any actions: custodian run -s out policy.yml custodian.commands:ERROR policy: log-delete exceeded resource limit: 2.5% found: 1 total: 1 If metrics are being published (-m/--metrics) then an additional metric named [ResourceCount]{.title-ref} will be published with the number of resources that matched the policy. Max resources can also be specified as an object with an [or]{.title-ref} or [and]{.title-ref} operator if you would like both a resource percent and a resource amount enforced. policies: - name: log-delete description: | This policy will not execute if the resources affected are over 50% of the total resource type amount and that amount is over 20. resource: aws.log-group max-resources: percent: 50 amount: 20 op: and filters: - type: last-write days: 5 actions: - delete Adding custom fields to reports {#report-custom-fields} Reports use a default set of fields that are resource-specific. To add other fields use the --field flag, which can be supplied multiple times. The syntax is: --field KEY=VALUE where KEY is the header name (what will print at the top of the column) and the VALUE is a JMESPath expression accessing the desired data: custodian report -s out --field Image=ImageId policy.yml If hyphens or other special characters are present in the JMESPath it may require quoting, e.g.: custodian report -s . --field \"AccessKey1LastRotated\"='\"c7n:credential-report\".access_keys[0].last_rotated' policy.yml To remove the default fields and only add the desired ones, the --no-default-fields flag can be specified and then specific fields can be added in, e.g.: custodian report -s out --no-default-fields --field Image=ImageId policy.yml","title":"Advanced"},{"location":"quickstart/advanced/#advanced-usage-advanced","text":"run-multiple-regions {.interpreted-text role=\"ref\"} report-multiple-regions {.interpreted-text role=\"ref\"} report-custom-fields {.interpreted-text role=\"ref\"} policy_resource_limits {.interpreted-text role=\"ref\"}","title":"Advanced Usage {#advanced}"},{"location":"quickstart/advanced/#running-against-multiple-regions-run-multiple-regions","text":"By default Cloud Custodian determines the region to run against in the following order: the --region flag the AWS_DEFAULT_REGION environment variable the region set in the ~/.aws/config file It is possible to run policies against multiple regions by specifying the --region flag multiple times: custodian run -s out --region us-east-1 --region us-west-1 policy.yml If a supplied region does not support the resource for a given policy that region will be skipped. The special all keyword can be used in place of a region to specify the policy should run against all applicable regions for the policy\\'s resource: custodian run -s out --region all policy.yml Note: when running reports against multiple regions the output is placed in a different directory than when running against a single region. See the multi-region reporting section below.","title":"Running against multiple regions {#run-multiple-regions}"},{"location":"quickstart/advanced/#reporting-against-multiple-regions-report-multiple-regions","text":"When running against multiple regions the output files are placed in a different location that when running against a single region. When generating a report, specify multiple regions the same way as with the run command: custodian report -s out --region us-east-1 --region-us-west-1 policy.yml A region column will be added to reports generated that include multiple regions to indicate which region each row is from.","title":"Reporting against multiple regions {#report-multiple-regions}"},{"location":"quickstart/advanced/#conditional-policy-execution-scheduling-policy-execution","text":"Cloud Custodian can skip policies that are included in a policy file when running if the policy specifies conditions that aren\\'t met by the current environment. The available environment keys are Key Description name Name of the policy region Region the policy is being evaluated in. resource The resource type of the policy. account_id The account id (subscription, project) the policy is being evaluated in. provider The name of the cloud provider (aws, azure, gcp, etc) policy The policy data as structure now The current time event In serverless, the event that triggered the policy account When running in c7n-org, current account info per account config file If a policy is executing in a serverless mode the triggering event is available. As an example, one can set up policy conditions to only execute between a given set of dates. policies: # other compliance related policies that # should always be running... - name: holiday-break-stop description: | This policy will stop all EC2 instances if the current date is between 12-15-2018 to 12-31-2018 when the policy is run. Use this in conjunction with a cron job to ensure that the environment is fully turned off during the break. resource: ec2 conditions: - type: value key: now op: greater-than value_type: date value: \"2018-12-15\" - type: value key: now op: less-than value_type: date value: \"2018-12-31\" filters: - \"tag:holiday-off-hours\": present actions: - stop - name: holiday-break-start description: | This policy will start up all EC2 instances and only run on 1-1-2019. resource: ec2 conditions: - type: value key: now value_type: date op: greater-than value: \"2009-1-1\" - type: value key: now value_type: date op: less-than value: \"2019-1-1 23:59:59\" filters: - \"tag:holiday-off-hours\": present actions: - start","title":"Conditional Policy Execution {#scheduling-policy-execution}"},{"location":"quickstart/advanced/#limiting-how-many-resources-custodian-affects-policy_resource_limits","text":"Custodian by default will operate on as many resources exist within an environment that match a policy\\'s filters. Custodian also allows policy authors to stop policy execution if a policy affects more resources than expected, either as a number of resources or as a percentage of total extant resources. policies: - name: log-delete description: | This policy will delete all log groups that haven't been written to in 5 days. As a safety belt, it will stop execution if the number of log groups that would be affected is more than 5% of the total log groups in the account's region. resource: aws.log-group max-resources-percent: 5 filters: - type: last-write days: 5 actions: - delete Max resources can also be specified as an absolute number using [max-resources]{.title-ref} specified on a policy. When executing if the limit is exceeded, policy execution is stopped before taking any actions: custodian run -s out policy.yml custodian.commands:ERROR policy: log-delete exceeded resource limit: 2.5% found: 1 total: 1 If metrics are being published (-m/--metrics) then an additional metric named [ResourceCount]{.title-ref} will be published with the number of resources that matched the policy. Max resources can also be specified as an object with an [or]{.title-ref} or [and]{.title-ref} operator if you would like both a resource percent and a resource amount enforced. policies: - name: log-delete description: | This policy will not execute if the resources affected are over 50% of the total resource type amount and that amount is over 20. resource: aws.log-group max-resources: percent: 50 amount: 20 op: and filters: - type: last-write days: 5 actions: - delete","title":"Limiting how many resources custodian affects {#policy_resource_limits}"},{"location":"quickstart/advanced/#adding-custom-fields-to-reports-report-custom-fields","text":"Reports use a default set of fields that are resource-specific. To add other fields use the --field flag, which can be supplied multiple times. The syntax is: --field KEY=VALUE where KEY is the header name (what will print at the top of the column) and the VALUE is a JMESPath expression accessing the desired data: custodian report -s out --field Image=ImageId policy.yml If hyphens or other special characters are present in the JMESPath it may require quoting, e.g.: custodian report -s . --field \"AccessKey1LastRotated\"='\"c7n:credential-report\".access_keys[0].last_rotated' policy.yml To remove the default fields and only add the desired ones, the --no-default-fields flag can be specified and then specific fields can be added in, e.g.: custodian report -s out --no-default-fields --field Image=ImageId policy.yml","title":"Adding custom fields to reports {#report-custom-fields}"},{"location":"quickstart/advanced.rst/","text":"Advanced Usage {#advanced} run-multiple-regions {.interpreted-text role=\"ref\"} report-multiple-regions {.interpreted-text role=\"ref\"} report-custom-fields {.interpreted-text role=\"ref\"} policy_resource_limits {.interpreted-text role=\"ref\"} Running against multiple regions {#run-multiple-regions} By default Cloud Custodian determines the region to run against in the following order: the --region flag the AWS_DEFAULT_REGION environment variable the region set in the ~/.aws/config file It is possible to run policies against multiple regions by specifying the --region flag multiple times: custodian run -s out --region us-east-1 --region us-west-1 policy.yml If a supplied region does not support the resource for a given policy that region will be skipped. The special all keyword can be used in place of a region to specify the policy should run against all applicable regions for the policy\\'s resource: custodian run -s out --region all policy.yml Note: when running reports against multiple regions the output is placed in a different directory than when running against a single region. See the multi-region reporting section below. Reporting against multiple regions {#report-multiple-regions} When running against multiple regions the output files are placed in a different location that when running against a single region. When generating a report, specify multiple regions the same way as with the run command: custodian report -s out --region us-east-1 --region-us-west-1 policy.yml A region column will be added to reports generated that include multiple regions to indicate which region each row is from. Conditional Policy Execution {#scheduling-policy-execution} Cloud Custodian can skip policies that are included in a policy file when running if the policy specifies conditions that aren\\'t met by the current environment. The available environment keys are Key Description name Name of the policy region Region the policy is being evaluated in. resource The resource type of the policy. account_id The account id (subscription, project) the policy is being evaluated in. provider The name of the cloud provider (aws, azure, gcp, etc) policy The policy data as structure now The current time event In serverless, the event that triggered the policy account When running in c7n-org, current account info per account config file If a policy is executing in a serverless mode the triggering event is available. As an example, one can set up policy conditions to only execute between a given set of dates. policies: # other compliance related policies that # should always be running... - name: holiday-break-stop description: | This policy will stop all EC2 instances if the current date is between 12-15-2018 to 12-31-2018 when the policy is run. Use this in conjunction with a cron job to ensure that the environment is fully turned off during the break. resource: ec2 conditions: - type: value key: now op: greater-than value_type: date value: \"2018-12-15\" - type: value key: now op: less-than value_type: date value: \"2018-12-31\" filters: - \"tag:holiday-off-hours\": present actions: - stop - name: holiday-break-start description: | This policy will start up all EC2 instances and only run on 1-1-2019. resource: ec2 conditions: - type: value key: now value_type: date op: greater-than value: \"2009-1-1\" - type: value key: now value_type: date op: less-than value: \"2019-1-1 23:59:59\" filters: - \"tag:holiday-off-hours\": present actions: - start Limiting how many resources custodian affects {#policy_resource_limits} Custodian by default will operate on as many resources exist within an environment that match a policy\\'s filters. Custodian also allows policy authors to stop policy execution if a policy affects more resources than expected, either as a number of resources or as a percentage of total extant resources. policies: - name: log-delete description: | This policy will delete all log groups that haven't been written to in 5 days. As a safety belt, it will stop execution if the number of log groups that would be affected is more than 5% of the total log groups in the account's region. resource: aws.log-group max-resources-percent: 5 filters: - type: last-write days: 5 actions: - delete Max resources can also be specified as an absolute number using [max-resources]{.title-ref} specified on a policy. When executing if the limit is exceeded, policy execution is stopped before taking any actions: custodian run -s out policy.yml custodian.commands:ERROR policy: log-delete exceeded resource limit: 2.5% found: 1 total: 1 If metrics are being published (-m/--metrics) then an additional metric named [ResourceCount]{.title-ref} will be published with the number of resources that matched the policy. Max resources can also be specified as an object with an [or]{.title-ref} or [and]{.title-ref} operator if you would like both a resource percent and a resource amount enforced. policies: - name: log-delete description: | This policy will not execute if the resources affected are over 50% of the total resource type amount and that amount is over 20. resource: aws.log-group max-resources: percent: 50 amount: 20 op: and filters: - type: last-write days: 5 actions: - delete Adding custom fields to reports {#report-custom-fields} Reports use a default set of fields that are resource-specific. To add other fields use the --field flag, which can be supplied multiple times. The syntax is: --field KEY=VALUE where KEY is the header name (what will print at the top of the column) and the VALUE is a JMESPath expression accessing the desired data: custodian report -s out --field Image=ImageId policy.yml If hyphens or other special characters are present in the JMESPath it may require quoting, e.g.: custodian report -s . --field \"AccessKey1LastRotated\"='\"c7n:credential-report\".access_keys[0].last_rotated' policy.yml To remove the default fields and only add the desired ones, the --no-default-fields flag can be specified and then specific fields can be added in, e.g.: custodian report -s out --no-default-fields --field Image=ImageId policy.yml","title":"Advanced.rst"},{"location":"quickstart/advanced.rst/#advanced-usage-advanced","text":"run-multiple-regions {.interpreted-text role=\"ref\"} report-multiple-regions {.interpreted-text role=\"ref\"} report-custom-fields {.interpreted-text role=\"ref\"} policy_resource_limits {.interpreted-text role=\"ref\"}","title":"Advanced Usage {#advanced}"},{"location":"quickstart/advanced.rst/#running-against-multiple-regions-run-multiple-regions","text":"By default Cloud Custodian determines the region to run against in the following order: the --region flag the AWS_DEFAULT_REGION environment variable the region set in the ~/.aws/config file It is possible to run policies against multiple regions by specifying the --region flag multiple times: custodian run -s out --region us-east-1 --region us-west-1 policy.yml If a supplied region does not support the resource for a given policy that region will be skipped. The special all keyword can be used in place of a region to specify the policy should run against all applicable regions for the policy\\'s resource: custodian run -s out --region all policy.yml Note: when running reports against multiple regions the output is placed in a different directory than when running against a single region. See the multi-region reporting section below.","title":"Running against multiple regions {#run-multiple-regions}"},{"location":"quickstart/advanced.rst/#reporting-against-multiple-regions-report-multiple-regions","text":"When running against multiple regions the output files are placed in a different location that when running against a single region. When generating a report, specify multiple regions the same way as with the run command: custodian report -s out --region us-east-1 --region-us-west-1 policy.yml A region column will be added to reports generated that include multiple regions to indicate which region each row is from.","title":"Reporting against multiple regions {#report-multiple-regions}"},{"location":"quickstart/advanced.rst/#conditional-policy-execution-scheduling-policy-execution","text":"Cloud Custodian can skip policies that are included in a policy file when running if the policy specifies conditions that aren\\'t met by the current environment. The available environment keys are Key Description name Name of the policy region Region the policy is being evaluated in. resource The resource type of the policy. account_id The account id (subscription, project) the policy is being evaluated in. provider The name of the cloud provider (aws, azure, gcp, etc) policy The policy data as structure now The current time event In serverless, the event that triggered the policy account When running in c7n-org, current account info per account config file If a policy is executing in a serverless mode the triggering event is available. As an example, one can set up policy conditions to only execute between a given set of dates. policies: # other compliance related policies that # should always be running... - name: holiday-break-stop description: | This policy will stop all EC2 instances if the current date is between 12-15-2018 to 12-31-2018 when the policy is run. Use this in conjunction with a cron job to ensure that the environment is fully turned off during the break. resource: ec2 conditions: - type: value key: now op: greater-than value_type: date value: \"2018-12-15\" - type: value key: now op: less-than value_type: date value: \"2018-12-31\" filters: - \"tag:holiday-off-hours\": present actions: - stop - name: holiday-break-start description: | This policy will start up all EC2 instances and only run on 1-1-2019. resource: ec2 conditions: - type: value key: now value_type: date op: greater-than value: \"2009-1-1\" - type: value key: now value_type: date op: less-than value: \"2019-1-1 23:59:59\" filters: - \"tag:holiday-off-hours\": present actions: - start","title":"Conditional Policy Execution {#scheduling-policy-execution}"},{"location":"quickstart/advanced.rst/#limiting-how-many-resources-custodian-affects-policy_resource_limits","text":"Custodian by default will operate on as many resources exist within an environment that match a policy\\'s filters. Custodian also allows policy authors to stop policy execution if a policy affects more resources than expected, either as a number of resources or as a percentage of total extant resources. policies: - name: log-delete description: | This policy will delete all log groups that haven't been written to in 5 days. As a safety belt, it will stop execution if the number of log groups that would be affected is more than 5% of the total log groups in the account's region. resource: aws.log-group max-resources-percent: 5 filters: - type: last-write days: 5 actions: - delete Max resources can also be specified as an absolute number using [max-resources]{.title-ref} specified on a policy. When executing if the limit is exceeded, policy execution is stopped before taking any actions: custodian run -s out policy.yml custodian.commands:ERROR policy: log-delete exceeded resource limit: 2.5% found: 1 total: 1 If metrics are being published (-m/--metrics) then an additional metric named [ResourceCount]{.title-ref} will be published with the number of resources that matched the policy. Max resources can also be specified as an object with an [or]{.title-ref} or [and]{.title-ref} operator if you would like both a resource percent and a resource amount enforced. policies: - name: log-delete description: | This policy will not execute if the resources affected are over 50% of the total resource type amount and that amount is over 20. resource: aws.log-group max-resources: percent: 50 amount: 20 op: and filters: - type: last-write days: 5 actions: - delete","title":"Limiting how many resources custodian affects {#policy_resource_limits}"},{"location":"quickstart/advanced.rst/#adding-custom-fields-to-reports-report-custom-fields","text":"Reports use a default set of fields that are resource-specific. To add other fields use the --field flag, which can be supplied multiple times. The syntax is: --field KEY=VALUE where KEY is the header name (what will print at the top of the column) and the VALUE is a JMESPath expression accessing the desired data: custodian report -s out --field Image=ImageId policy.yml If hyphens or other special characters are present in the JMESPath it may require quoting, e.g.: custodian report -s . --field \"AccessKey1LastRotated\"='\"c7n:credential-report\".access_keys[0].last_rotated' policy.yml To remove the default fields and only add the desired ones, the --no-default-fields flag can be specified and then specific fields can be added in, e.g.: custodian report -s out --no-default-fields --field Image=ImageId policy.yml","title":"Adding custom fields to reports {#report-custom-fields}"},{"location":"quickstart/index.rst/","text":"Getting Started {#quickstart} See also the readme in the GitHub repository. install-cc {.interpreted-text role=\"ref\"} explore-cc {.interpreted-text role=\"ref\"} cloud-providers {.interpreted-text role=\"ref\"} monitor-cc {.interpreted-text role=\"ref\"} tab-completion {.interpreted-text role=\"ref\"} community {.interpreted-text role=\"ref\"} Install Cloud Custodian {#install-cc} These instructions will install Cloud Custodian. Cloud Custodian is a Python application that supports Python 3 on Linux, MacOS and Windows. We recommend using Python 3.6 or higher. NOTE: Ensure you install the correct follow-on package depending on the cloud you are deploying to, otherwise you won\\'t have the right modules for that specific cloud. Linux and Mac OS To install Cloud Custodian : python3 -m venv custodian source custodian/bin/activate pip install c7n # This includes AWS support To install Cloud Custodian for Azure, you will also need to run: pip install c7n_azure # Install Azure package To install Cloud Custodian for GCP, you will also need to run: pip install c7n_gcp # Install GCP Package Windows (CMD/PowerShell) To install Cloud Custodian run: python3 -m venv custodian ./custodian/bin/activate pip install c7n # This includes AWS support To install Cloud Custodian for Azure, you will also need to run: pip install c7n_azure To install Cloud Custodian for GCP, you will also need to run: pip install c7n_gcp Docker To install via docker, run: docker pull cloudcustodian/c7n You\\'ll need to export cloud provider credentials to the container when executing. One example, if you\\'re using environment variables for provider credentials: docker run -it \\ -v $(pwd)/output:/home/custodian/output \\ -v $(pwd)/policy.yml:/home/custodian/policy.yml \\ --env-file <(env | grep \"^AWS\\|^AZURE\\|^GOOGLE\") \\ cloudcustodian/c7n run -v -s /home/custodian/output /home/custodian/policy.yml Explore Cloud Custodian {#explore-cc} Run custodian -h to see a list of available commands. Run custodian schema to see the complete list of cloud resources against which you can run policies. To invoke command-line help with more information about policy schema details, run custodian schema -h . Run custodian schema <cloud-provider> to see the available resources for a specific cloud provider: custodian schema aws Run custodian schema <cloud-provider>.<resource> to see the available filters and actions for each resource. Drill down to get more information about available policy settings for each resource, where the model for the command is: custodian schema <cloud>.<resource>.<category>.<item> For example: custodian schema aws.s3.filters.is-log-target provides the following information: Help ---- Filter and return buckets are log destinations. Not suitable for use in lambda on large accounts, This is a api heavy process to detect scan all possible log sources. Sources: - elb (Access Log) - s3 (Access Log) - cfn (Template writes) - cloudtrail :example: .. code-block: yaml policies: - name: s3-log-bucket resource: s3 filters: - type: is-log-target Schema ------ { 'additionalProperties': False, 'properties': { 'type': { 'enum': ['is-log-target']}, 'value': { 'type': 'boolean'}}, 'required': ['type'], 'type': 'object'} Additionally, you can use the schema command to view information on the different supported modes in Cloud Custodian: custodian schema mode Cloud Provider Specific Help {#cloud-providers} For specific setup isntructions for AWS, Azure, and GCP, visit the relevant getting started page. AWS <aws-gettingstarted> {.interpreted-text role=\"ref\"} Azure <azure_gettingstarted> {.interpreted-text role=\"ref\"} GCP <gcp_gettingstarted> {.interpreted-text role=\"ref\"} Troubleshooting & Tinkering The policy is validated automatically when you run it, but you can also validate it separately: custodian validate custodian.yml You can also check which resources are identified by the policy, without running any actions on the resources: custodian run --dryrun -s . custodian.yml Monitor resources {#monitor-cc} Additional commands let you monitor your services in detail. You can generate metrics, log outputs, and output to blob storage in each of the different providers (AWS, Azure, Google Cloud Platform). For detailed instructions on how to add metrics, logging, and blob storage output for the different clouds, check out the cloud provider specific pages: AWS <aws-gettingstarted> {.interpreted-text role=\"ref\"} Azure <azure_gettingstarted> {.interpreted-text role=\"ref\"} GCP <gcp_gettingstarted> {.interpreted-text role=\"ref\"} For details, see usage {.interpreted-text role=\"ref\"}. Editor Integration If your preferred editor supports language servers, you can configure it to provide completion and validation while authoring policies. First generate use custodian to generate a json schema file: custodian schema --json > schema.json Next install a YAML plug-in for your editor, like YAML for Visual Studio Code or coc-yaml for coc.nvim . Both plug-ins use the yaml-language-server under the hood. You\\'ll then need to configure your plug-in to use the generated [schema.json]{.title-ref} as the schema for your policy files. For example in Visual Studio Code, navigate to the settings for the YAML plug-in and under Schemas, edit configuration file and add the following schema configuration: \"yaml.schemas\": { \"./schema.json\": \"*yml\" }, Note the path to schema.json can either be either relative or the full path. You\\'ll now have completion and validation while authoring policies. Note if you\\'re authoring policies in json you can also configure the json-language-server for the same. Also, if you\\'re seeing errors like 'Request textDocument/hover failed with message: Cannot read property '$ref' of null' try re-creating your schema.json file. Tab Completion To enable command-line tab completion for [custodian]{.title-ref} on bash do the following one-time steps: Run: activate-global-python-argcomplete Now launch a new shell (or refresh your bash environment by sourcing the appropriate file). Community Resources We have a regular community meeting that is open to all users and developers of every skill level. Joining the mailing list https://groups.google.com/forum/#!forum/cloud-custodian will automatically send you a meeting invite. See the notes below for more technical information on joining the meeting. Community Meeting Videos Community Meeting Notes Archive Troubleshooting If you get an error about \\\"complete -D\\\" not being supported, you need to update bash. See the \\\"Base Version Compatability\\\" note in the argcomplete docs : If you have other errors, or for tcsh support, see the argcomplete docs . If you are invoking [custodian]{.title-ref} via the [python]{.title-ref} executable tab completion will not work. You must invoke [custodian]{.title-ref} directly.","title":"Index.rst"},{"location":"quickstart/index.rst/#getting-started-quickstart","text":"See also the readme in the GitHub repository. install-cc {.interpreted-text role=\"ref\"} explore-cc {.interpreted-text role=\"ref\"} cloud-providers {.interpreted-text role=\"ref\"} monitor-cc {.interpreted-text role=\"ref\"} tab-completion {.interpreted-text role=\"ref\"} community {.interpreted-text role=\"ref\"}","title":"Getting Started {#quickstart}"},{"location":"quickstart/index.rst/#install-cloud-custodian-install-cc","text":"These instructions will install Cloud Custodian. Cloud Custodian is a Python application that supports Python 3 on Linux, MacOS and Windows. We recommend using Python 3.6 or higher. NOTE: Ensure you install the correct follow-on package depending on the cloud you are deploying to, otherwise you won\\'t have the right modules for that specific cloud.","title":"Install Cloud Custodian {#install-cc}"},{"location":"quickstart/index.rst/#linux-and-mac-os","text":"To install Cloud Custodian : python3 -m venv custodian source custodian/bin/activate pip install c7n # This includes AWS support To install Cloud Custodian for Azure, you will also need to run: pip install c7n_azure # Install Azure package To install Cloud Custodian for GCP, you will also need to run: pip install c7n_gcp # Install GCP Package","title":"Linux and Mac OS"},{"location":"quickstart/index.rst/#windows-cmdpowershell","text":"To install Cloud Custodian run: python3 -m venv custodian ./custodian/bin/activate pip install c7n # This includes AWS support To install Cloud Custodian for Azure, you will also need to run: pip install c7n_azure To install Cloud Custodian for GCP, you will also need to run: pip install c7n_gcp","title":"Windows (CMD/PowerShell)"},{"location":"quickstart/index.rst/#docker","text":"To install via docker, run: docker pull cloudcustodian/c7n You\\'ll need to export cloud provider credentials to the container when executing. One example, if you\\'re using environment variables for provider credentials: docker run -it \\ -v $(pwd)/output:/home/custodian/output \\ -v $(pwd)/policy.yml:/home/custodian/policy.yml \\ --env-file <(env | grep \"^AWS\\|^AZURE\\|^GOOGLE\") \\ cloudcustodian/c7n run -v -s /home/custodian/output /home/custodian/policy.yml","title":"Docker"},{"location":"quickstart/index.rst/#explore-cloud-custodian-explore-cc","text":"Run custodian -h to see a list of available commands. Run custodian schema to see the complete list of cloud resources against which you can run policies. To invoke command-line help with more information about policy schema details, run custodian schema -h . Run custodian schema <cloud-provider> to see the available resources for a specific cloud provider: custodian schema aws Run custodian schema <cloud-provider>.<resource> to see the available filters and actions for each resource. Drill down to get more information about available policy settings for each resource, where the model for the command is: custodian schema <cloud>.<resource>.<category>.<item> For example: custodian schema aws.s3.filters.is-log-target provides the following information: Help ---- Filter and return buckets are log destinations. Not suitable for use in lambda on large accounts, This is a api heavy process to detect scan all possible log sources. Sources: - elb (Access Log) - s3 (Access Log) - cfn (Template writes) - cloudtrail :example: .. code-block: yaml policies: - name: s3-log-bucket resource: s3 filters: - type: is-log-target Schema ------ { 'additionalProperties': False, 'properties': { 'type': { 'enum': ['is-log-target']}, 'value': { 'type': 'boolean'}}, 'required': ['type'], 'type': 'object'} Additionally, you can use the schema command to view information on the different supported modes in Cloud Custodian: custodian schema mode","title":"Explore Cloud Custodian {#explore-cc}"},{"location":"quickstart/index.rst/#cloud-provider-specific-help-cloud-providers","text":"For specific setup isntructions for AWS, Azure, and GCP, visit the relevant getting started page. AWS <aws-gettingstarted> {.interpreted-text role=\"ref\"} Azure <azure_gettingstarted> {.interpreted-text role=\"ref\"} GCP <gcp_gettingstarted> {.interpreted-text role=\"ref\"}","title":"Cloud Provider Specific Help {#cloud-providers}"},{"location":"quickstart/index.rst/#troubleshooting-tinkering","text":"The policy is validated automatically when you run it, but you can also validate it separately: custodian validate custodian.yml You can also check which resources are identified by the policy, without running any actions on the resources: custodian run --dryrun -s . custodian.yml","title":"Troubleshooting &amp; Tinkering"},{"location":"quickstart/index.rst/#monitor-resources-monitor-cc","text":"Additional commands let you monitor your services in detail. You can generate metrics, log outputs, and output to blob storage in each of the different providers (AWS, Azure, Google Cloud Platform). For detailed instructions on how to add metrics, logging, and blob storage output for the different clouds, check out the cloud provider specific pages: AWS <aws-gettingstarted> {.interpreted-text role=\"ref\"} Azure <azure_gettingstarted> {.interpreted-text role=\"ref\"} GCP <gcp_gettingstarted> {.interpreted-text role=\"ref\"} For details, see usage {.interpreted-text role=\"ref\"}.","title":"Monitor resources {#monitor-cc}"},{"location":"quickstart/index.rst/#editor-integration","text":"If your preferred editor supports language servers, you can configure it to provide completion and validation while authoring policies. First generate use custodian to generate a json schema file: custodian schema --json > schema.json Next install a YAML plug-in for your editor, like YAML for Visual Studio Code or coc-yaml for coc.nvim . Both plug-ins use the yaml-language-server under the hood. You\\'ll then need to configure your plug-in to use the generated [schema.json]{.title-ref} as the schema for your policy files. For example in Visual Studio Code, navigate to the settings for the YAML plug-in and under Schemas, edit configuration file and add the following schema configuration: \"yaml.schemas\": { \"./schema.json\": \"*yml\" }, Note the path to schema.json can either be either relative or the full path. You\\'ll now have completion and validation while authoring policies. Note if you\\'re authoring policies in json you can also configure the json-language-server for the same. Also, if you\\'re seeing errors like 'Request textDocument/hover failed with message: Cannot read property '$ref' of null' try re-creating your schema.json file.","title":"Editor Integration"},{"location":"quickstart/index.rst/#tab-completion","text":"To enable command-line tab completion for [custodian]{.title-ref} on bash do the following one-time steps: Run: activate-global-python-argcomplete Now launch a new shell (or refresh your bash environment by sourcing the appropriate file).","title":"Tab Completion"},{"location":"quickstart/index.rst/#community-resources","text":"We have a regular community meeting that is open to all users and developers of every skill level. Joining the mailing list https://groups.google.com/forum/#!forum/cloud-custodian will automatically send you a meeting invite. See the notes below for more technical information on joining the meeting. Community Meeting Videos Community Meeting Notes Archive","title":"Community Resources"},{"location":"quickstart/index.rst/#troubleshooting","text":"If you get an error about \\\"complete -D\\\" not being supported, you need to update bash. See the \\\"Base Version Compatability\\\" note in the argcomplete docs : If you have other errors, or for tcsh support, see the argcomplete docs . If you are invoking [custodian]{.title-ref} via the [python]{.title-ref} executable tab completion will not work. You must invoke [custodian]{.title-ref} directly.","title":"Troubleshooting"},{"location":"quickstart/policyStructure/","text":"Example tag compliance policy {#policyStructure} In this sample policy we are filtering for EC2 instances that are: running, not part of an Auto Scaling Group (ASG), not already marked for an operation, have less than 10 tags, and are missing one or more of the required tags. Once Custodian has filtered the list, it will mark all EC2 instances that match the above criteria with a tag. That tag specifies an action that will take place at a certain time. This policy is one of three that are needed to manage tag compliance. The other two policies in this set are, 1) checking to see if the tags have been corrected before the four day period is up, and 2) performing the operation of stopping all instances with the status to be stopped on that particular day. - name: ec2-tag-compliance-mark resource: ec2 comment: | Mark non-compliant, Non-ASG EC2 instances with stoppage in 4 days filters: \u25a3\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 - \"State.Name\": running \u2502 \u25a3\u2500\u2500\u2500\u2500\u2500\u2500\u2500 - \"tag:aws:autoscaling:groupName\": absent \u2502 \u2502 \u25a3\u2500\u2500\u2500\u2500\u2500 - \"tag:c7n_status\": absent \u2502 \u2502 \u2502 \u25a3\u2500\u2500\u2500 - type: tag-count \u2502 \u2502 \u2502 \u2502 - or: \u2500\u2510 \u2502 \u2502 \u2502 \u2502 - \"tag:Owner\": absent \u251c\u2500If any of these tags are \u2502 \u2502 \u2502 \u2502 - \"tag:CostCenter\": absent \u2502 missing, then select instance \u2502 \u2502 \u2502 \u2502 - \"tag:Project\": absent \u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 actions: \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6 For selected instances, run this action \u2502 \u2502 \u2502 \u2502 - type: mark-for-op \u2500\u2500\u2500\u2500\u25b6 Mark instance for operation \u2502 \u2502 \u2502 \u2502 op: stop \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6 Stop instance \u2502 \u2502 \u2502 \u2502 days: 4 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6 After 4 days \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u25a3\u2500\u2500\u2500\u2500\u25b6 If instance has 10 tags, skip \u2502 \u2502 \u25a3\u2500\u2500\u2500\u2500\u2500\u2500\u25b6 If instance already has a c7n_status, skip \u2502 \u25a3\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6 If instance is part of an ASG, skip \u25a3\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6 If instance is not running, skip","title":"policyStructure"},{"location":"quickstart/policyStructure/#example-tag-compliance-policy-policystructure","text":"In this sample policy we are filtering for EC2 instances that are: running, not part of an Auto Scaling Group (ASG), not already marked for an operation, have less than 10 tags, and are missing one or more of the required tags. Once Custodian has filtered the list, it will mark all EC2 instances that match the above criteria with a tag. That tag specifies an action that will take place at a certain time. This policy is one of three that are needed to manage tag compliance. The other two policies in this set are, 1) checking to see if the tags have been corrected before the four day period is up, and 2) performing the operation of stopping all instances with the status to be stopped on that particular day. - name: ec2-tag-compliance-mark resource: ec2 comment: | Mark non-compliant, Non-ASG EC2 instances with stoppage in 4 days filters: \u25a3\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 - \"State.Name\": running \u2502 \u25a3\u2500\u2500\u2500\u2500\u2500\u2500\u2500 - \"tag:aws:autoscaling:groupName\": absent \u2502 \u2502 \u25a3\u2500\u2500\u2500\u2500\u2500 - \"tag:c7n_status\": absent \u2502 \u2502 \u2502 \u25a3\u2500\u2500\u2500 - type: tag-count \u2502 \u2502 \u2502 \u2502 - or: \u2500\u2510 \u2502 \u2502 \u2502 \u2502 - \"tag:Owner\": absent \u251c\u2500If any of these tags are \u2502 \u2502 \u2502 \u2502 - \"tag:CostCenter\": absent \u2502 missing, then select instance \u2502 \u2502 \u2502 \u2502 - \"tag:Project\": absent \u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 actions: \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6 For selected instances, run this action \u2502 \u2502 \u2502 \u2502 - type: mark-for-op \u2500\u2500\u2500\u2500\u25b6 Mark instance for operation \u2502 \u2502 \u2502 \u2502 op: stop \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6 Stop instance \u2502 \u2502 \u2502 \u2502 days: 4 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6 After 4 days \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u25a3\u2500\u2500\u2500\u2500\u25b6 If instance has 10 tags, skip \u2502 \u2502 \u25a3\u2500\u2500\u2500\u2500\u2500\u2500\u25b6 If instance already has a c7n_status, skip \u2502 \u25a3\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6 If instance is part of an ASG, skip \u25a3\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6 If instance is not running, skip","title":"Example tag compliance policy {#policyStructure}"},{"location":"quickstart/policyStructure.rst/","text":"Example tag compliance policy {#policyStructure} In this sample policy we are filtering for EC2 instances that are: running, not part of an Auto Scaling Group (ASG), not already marked for an operation, have less than 10 tags, and are missing one or more of the required tags. Once Custodian has filtered the list, it will mark all EC2 instances that match the above criteria with a tag. That tag specifies an action that will take place at a certain time. This policy is one of three that are needed to manage tag compliance. The other two policies in this set are, 1) checking to see if the tags have been corrected before the four day period is up, and 2) performing the operation of stopping all instances with the status to be stopped on that particular day. - name: ec2-tag-compliance-mark resource: ec2 comment: | Mark non-compliant, Non-ASG EC2 instances with stoppage in 4 days filters: \u25a3\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 - \"State.Name\": running \u2502 \u25a3\u2500\u2500\u2500\u2500\u2500\u2500\u2500 - \"tag:aws:autoscaling:groupName\": absent \u2502 \u2502 \u25a3\u2500\u2500\u2500\u2500\u2500 - \"tag:c7n_status\": absent \u2502 \u2502 \u2502 \u25a3\u2500\u2500\u2500 - type: tag-count \u2502 \u2502 \u2502 \u2502 - or: \u2500\u2510 \u2502 \u2502 \u2502 \u2502 - \"tag:Owner\": absent \u251c\u2500If any of these tags are \u2502 \u2502 \u2502 \u2502 - \"tag:CostCenter\": absent \u2502 missing, then select instance \u2502 \u2502 \u2502 \u2502 - \"tag:Project\": absent \u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 actions: \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6 For selected instances, run this action \u2502 \u2502 \u2502 \u2502 - type: mark-for-op \u2500\u2500\u2500\u2500\u25b6 Mark instance for operation \u2502 \u2502 \u2502 \u2502 op: stop \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6 Stop instance \u2502 \u2502 \u2502 \u2502 days: 4 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6 After 4 days \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u25a3\u2500\u2500\u2500\u2500\u25b6 If instance has 10 tags, skip \u2502 \u2502 \u25a3\u2500\u2500\u2500\u2500\u2500\u2500\u25b6 If instance already has a c7n_status, skip \u2502 \u25a3\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6 If instance is part of an ASG, skip \u25a3\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6 If instance is not running, skip","title":"policyStructure.rst"},{"location":"quickstart/policyStructure.rst/#example-tag-compliance-policy-policystructure","text":"In this sample policy we are filtering for EC2 instances that are: running, not part of an Auto Scaling Group (ASG), not already marked for an operation, have less than 10 tags, and are missing one or more of the required tags. Once Custodian has filtered the list, it will mark all EC2 instances that match the above criteria with a tag. That tag specifies an action that will take place at a certain time. This policy is one of three that are needed to manage tag compliance. The other two policies in this set are, 1) checking to see if the tags have been corrected before the four day period is up, and 2) performing the operation of stopping all instances with the status to be stopped on that particular day. - name: ec2-tag-compliance-mark resource: ec2 comment: | Mark non-compliant, Non-ASG EC2 instances with stoppage in 4 days filters: \u25a3\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 - \"State.Name\": running \u2502 \u25a3\u2500\u2500\u2500\u2500\u2500\u2500\u2500 - \"tag:aws:autoscaling:groupName\": absent \u2502 \u2502 \u25a3\u2500\u2500\u2500\u2500\u2500 - \"tag:c7n_status\": absent \u2502 \u2502 \u2502 \u25a3\u2500\u2500\u2500 - type: tag-count \u2502 \u2502 \u2502 \u2502 - or: \u2500\u2510 \u2502 \u2502 \u2502 \u2502 - \"tag:Owner\": absent \u251c\u2500If any of these tags are \u2502 \u2502 \u2502 \u2502 - \"tag:CostCenter\": absent \u2502 missing, then select instance \u2502 \u2502 \u2502 \u2502 - \"tag:Project\": absent \u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 actions: \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6 For selected instances, run this action \u2502 \u2502 \u2502 \u2502 - type: mark-for-op \u2500\u2500\u2500\u2500\u25b6 Mark instance for operation \u2502 \u2502 \u2502 \u2502 op: stop \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6 Stop instance \u2502 \u2502 \u2502 \u2502 days: 4 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6 After 4 days \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u25a3\u2500\u2500\u2500\u2500\u25b6 If instance has 10 tags, skip \u2502 \u2502 \u25a3\u2500\u2500\u2500\u2500\u2500\u2500\u25b6 If instance already has a c7n_status, skip \u2502 \u25a3\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6 If instance is part of an ASG, skip \u25a3\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6 If instance is not running, skip","title":"Example tag compliance policy {#policyStructure}"}]}